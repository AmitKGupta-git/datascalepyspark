{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab03ec7ae0c04627a23a3d69323fd680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1603597063561_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-66-161.ec2.internal:20888/proxy/application_1603597063561_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-69-101.ec2.internal:8042/node/containerlogs/container_1603597063561_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833f59037b3747a58857a492c03461db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e7d04ce24143c2871911ce0e0adb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading https://files.pythonhosted.org/packages/cc/a8/b5037dc144e458b3574c085d891b85ab2035b63ab946b5c91c23f2dfc1c6/boto3-1.16.4-py2.py3-none-any.whl (129kB)\n",
      "Collecting botocore<1.20.0,>=1.19.4 (from boto3)\n",
      "  Downloading https://files.pythonhosted.org/packages/2b/55/9347e51769db0fe3487ed2ae5f438b3cc6aa2916e5e9d05e60a04855373e/botocore-1.19.4-py2.py3-none-any.whl (6.7MB)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0 (from boto3)\n",
      "  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
      "Collecting urllib3<1.26,>=1.25.4; python_version != \"3.4\" (from botocore<1.20.0,>=1.19.4->boto3)\n",
      "  Downloading https://files.pythonhosted.org/packages/56/aa/4ef5aa67a9a62505db124a5cb5262332d1d4153462eb8fd89c9fa41e5d92/urllib3-1.25.11-py2.py3-none-any.whl (127kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore<1.20.0,>=1.19.4->boto3)\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.4->boto3)\n",
      "Installing collected packages: urllib3, python-dateutil, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.16.4 botocore-1.19.4 python-dateutil-2.8.1 s3transfer-0.3.3 urllib3-1.25.11\n",
      "\n",
      "Collecting pandas\n",
      "  Downloading https://files.pythonhosted.org/packages/25/47/22fc373440e144e2111363adaa07abb09ec1f03fbc071b6d9fc0bbf65f68/pandas-1.1.3-cp37-cp37m-manylinux1_x86_64.whl (9.5MB)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib64/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /mnt/tmp/1603597215298-0/lib/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.1.3\n",
      "\n",
      "Collecting requests\n",
      "  Downloading https://files.pythonhosted.org/packages/45/1e/0c169c6a5381e241ba7404532c16a21d86ab872c9bed8bdcd4c423954103/requests-2.24.0-py2.py3-none-any.whl (61kB)\n",
      "Collecting chardet<4,>=3.0.2 (from requests)\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "Collecting idna<3,>=2.5 (from requests)\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/38/928ddce2273eaa564f6f50de919327bf3a00f091b5baba8dfa9460f3a8a8/idna-2.10-py2.py3-none-any.whl (58kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /mnt/tmp/1603597215298-0/lib/python3.7/site-packages (from requests)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/c4/6c4fe722df5343c33226f0b4e0bb042e4dc13483228b4718baf286f86d87/certifi-2020.6.20-py2.py3-none-any.whl (156kB)\n",
      "Installing collected packages: chardet, idna, certifi, requests\n",
      "Successfully installed certifi-2020.6.20 chardet-3.0.4 idna-2.10 requests-2.24.0\n",
      "\n",
      "Collecting s3fs\n",
      "  Downloading https://files.pythonhosted.org/packages/a7/58/732ea1c735d725b1cc4cf365ae6326c22569a5e88c8502d13844e91f08ef/s3fs-0.5.1-py3-none-any.whl\n",
      "Collecting aiobotocore>=1.0.1 (from s3fs)\n",
      "  Downloading https://files.pythonhosted.org/packages/73/25/a81b015035012131056a6b7a339eec052f86f33e35fd91f160e961ea2a5e/aiobotocore-1.1.2-py3-none-any.whl (45kB)\n",
      "Collecting fsspec>=0.8.0 (from s3fs)\n",
      "  Downloading https://files.pythonhosted.org/packages/a5/8b/1df260f860f17cb08698170153ef7db672c497c1840dcc8613ce26a8a005/fsspec-0.8.4-py3-none-any.whl (91kB)\n",
      "Collecting aioitertools>=0.5.1 (from aiobotocore>=1.0.1->s3fs)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/42/90df27c516ce54fa26964bc4a632ecaf352c7e99574b515255e48b4a7cc7/aioitertools-0.7.0-py3-none-any.whl\n",
      "Collecting botocore<1.17.45,>=1.17.44 (from aiobotocore>=1.0.1->s3fs)\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/6a/b6490235c01c941a24a86235e2a641e9505cf0ce4b4968d4987573d92bec/botocore-1.17.44-py2.py3-none-any.whl (6.5MB)\n",
      "Collecting wrapt>=1.10.10 (from aiobotocore>=1.0.1->s3fs)\n",
      "  Downloading https://files.pythonhosted.org/packages/82/f7/e43cefbe88c5fd371f4cf0cf5eb3feccd07515af9fd6cf7dbf1d1793a797/wrapt-1.12.1.tar.gz\n",
      "Collecting aiohttp>=3.3.1 (from aiobotocore>=1.0.1->s3fs)\n",
      "  Downloading https://files.pythonhosted.org/packages/de/14/0cac091b4dee22540f5c54a6ff31b56624e71f3a893b8b01485bf2ce2a4f/aiohttp-3.7.0.tar.gz (1.1MB)\n",
      "Collecting typing_extensions>=3.7 (from aioitertools>=0.5.1->aiobotocore>=1.0.1->s3fs)\n",
      "  Downloading https://files.pythonhosted.org/packages/60/7a/e881b5abb54db0e6e671ab088d079c57ce54e8a01a3ca443f561ccadb37e/typing_extensions-3.7.4.3-py3-none-any.whl\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /mnt/tmp/1603597215298-0/lib/python3.7/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /mnt/tmp/1603597215298-0/lib/python3.7/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs)\n",
      "Collecting docutils<0.16,>=0.10 (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs)\n",
      "  Downloading https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl (547kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "  Downloading https://files.pythonhosted.org/packages/14/df/479736ae1ef59842f512548bacefad1abed705e400212acba43f9b0fa556/attrs-20.2.0-py2.py3-none-any.whl (48kB)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /mnt/tmp/1603597215298-0/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "  Downloading https://files.pythonhosted.org/packages/d2/5a/e95b0f9ebacd42e094e229a9a0a9e44d02876abf64969d0cb07dadcf3c4a/multidict-5.0.0.tar.gz (53kB)\n",
      "Collecting async_timeout<4.0,>=3.0 (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "  Downloading https://files.pythonhosted.org/packages/6d/0b/44006eb5b3b7898e54df4b7481839d22d15cf3fc4f0420394ad9e3bb214e/yarl-1.6.2.tar.gz (177kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs)\n",
      "Requirement already satisfied: idna>=2.0 in /mnt/tmp/1603597215298-0/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "Building wheels for collected packages: wrapt, aiohttp, multidict, yarl\n",
      "  Running setup.py bdist_wheel for wrapt: started\n",
      "  Running setup.py bdist_wheel for wrapt: finished with status 'done'\n",
      "  Stored in directory: /var/lib/livy/.cache/pip/wheels/b1/c2/ed/d62208260edbd3fa7156545c00ef966f45f2063d0a84f8208a\n",
      "  Running setup.py bdist_wheel for aiohttp: started\n",
      "  Running setup.py bdist_wheel for aiohttp: finished with status 'done'\n",
      "  Stored in directory: /var/lib/livy/.cache/pip/wheels/91/27/92/c0c7d88b626c5497188729c9c0e5d17d21c2c38c5d7acca78b\n",
      "  Running setup.py bdist_wheel for multidict: started\n",
      "  Running setup.py bdist_wheel for multidict: finished with status 'error'\n",
      "  Complete output from command /tmp/1603597215298-0/bin/python -u -c \"import setuptools, tokenize;__file__='/mnt/tmp/pip-build-730hag_t/multidict/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /tmp/tmpybbztl_ypip-wheel- --python-tag cp37:\n",
      "  **********************\n",
      "  * Accellerated build *\n",
      "  **********************\n",
      "  /usr/lib64/python3.7/distutils/dist.py:274: UserWarning: Unknown distribution option: 'project_urls'\n",
      "    warnings.warn(msg)\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.linux-x86_64-3.7\n",
      "  creating build/lib.linux-x86_64-3.7/multidict\n",
      "  copying multidict/__init__.py -> build/lib.linux-x86_64-3.7/multidict\n",
      "  copying multidict/_abc.py -> build/lib.linux-x86_64-3.7/multidict\n",
      "  copying multidict/_compat.py -> build/lib.linux-x86_64-3.7/multidict\n",
      "  copying multidict/_multidict_base.py -> build/lib.linux-x86_64-3.7/multidict\n",
      "  copying multidict/_multidict_py.py -> build/lib.linux-x86_64-3.7/multidict\n",
      "  running egg_info\n",
      "  writing multidict.egg-info/PKG-INFO\n",
      "  writing dependency_links to multidict.egg-info/dependency_links.txt\n",
      "  writing top-level names to multidict.egg-info/top_level.txt\n",
      "  warning: manifest_maker: standard file '-c' not found\n",
      "  \n",
      "  reading manifest file 'multidict.egg-info/SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no previously-included files matching '*.pyc' found anywhere in distribution\n",
      "  warning: no previously-included files found matching 'multidict/_multidict.html'\n",
      "  warning: no previously-included files found matching 'multidict/*.so'\n",
      "  warning: no previously-included files found matching 'multidict/*.pyd'\n",
      "  warning: no previously-included files found matching 'multidict/*.pyd'\n",
      "  no previously-included directories found matching 'docs/_build'\n",
      "  writing manifest file 'multidict.egg-info/SOURCES.txt'\n",
      "  copying multidict/__init__.pyi -> build/lib.linux-x86_64-3.7/multidict\n",
      "  copying multidict/_multidict.c -> build/lib.linux-x86_64-3.7/multidict\n",
      "  copying multidict/py.typed -> build/lib.linux-x86_64-3.7/multidict\n",
      "  creating build/lib.linux-x86_64-3.7/multidict/_multilib\n",
      "  copying multidict/_multilib/defs.h -> build/lib.linux-x86_64-3.7/multidict/_multilib\n",
      "  copying multidict/_multilib/dict.h -> build/lib.linux-x86_64-3.7/multidict/_multilib\n",
      "  copying multidict/_multilib/istr.h -> build/lib.linux-x86_64-3.7/multidict/_multilib\n",
      "  copying multidict/_multilib/iter.h -> build/lib.linux-x86_64-3.7/multidict/_multilib\n",
      "  copying multidict/_multilib/pair_list.h -> build/lib.linux-x86_64-3.7/multidict/_multilib\n",
      "  copying multidict/_multilib/views.h -> build/lib.linux-x86_64-3.7/multidict/_multilib\n",
      "  running build_ext\n",
      "  building 'multidict._multidict' extension\n",
      "  creating build/temp.linux-x86_64-3.7\n",
      "  creating build/temp.linux-x86_64-3.7/multidict\n",
      "  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -I/usr/include/python3.7m -c multidict/_multidict.c -o build/temp.linux-x86_64-3.7/multidict/_multidict.o -O2 -std=c99 -Wall -Wsign-compare -Wconversion -fno-strict-aliasing -pedantic\n",
      "  multidict/_multidict.c:1:10: fatal error: Python.h: No such file or directory\n",
      "   #include \"Python.h\"\n",
      "            ^~~~~~~~~~\n",
      "  compilation terminated.\n",
      "  error: command 'gcc' failed with exit status 1\n",
      "  \n",
      "  ----------------------------------------\n",
      "  Running setup.py clean for multidict\n",
      "  Running setup.py bdist_wheel for yarl: started\n",
      "  Running setup.py bdist_wheel for yarl: finished with status 'error'\n",
      "  Complete output from command /tmp/1603597215298-0/bin/python -u -c \"import setuptools, tokenize;__file__='/mnt/tmp/pip-build-730hag_t/yarl/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /tmp/tmp36fmxeqjpip-wheel- --python-tag cp37:\n",
      "  **********************\n",
      "  * Accellerated build *\n",
      "  **********************\n",
      "  /usr/lib64/python3.7/distutils/dist.py:274: UserWarning: Unknown distribution option: 'long_description_content_type'\n",
      "    warnings.warn(msg)\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.linux-x86_64-3.7\n",
      "  creating build/lib.linux-x86_64-3.7/yarl\n",
      "  copying yarl/__init__.py -> build/lib.linux-x86_64-3.7/yarl\n",
      "  copying yarl/_quoting.py -> build/lib.linux-x86_64-3.7/yarl\n",
      "  copying yarl/_quoting_py.py -> build/lib.linux-x86_64-3.7/yarl\n",
      "  copying yarl/_url.py -> build/lib.linux-x86_64-3.7/yarl\n",
      "  running egg_info\n",
      "  writing yarl.egg-info/PKG-INFO\n",
      "  writing dependency_links to yarl.egg-info/dependency_links.txt\n",
      "  writing requirements to yarl.egg-info/requires.txt\n",
      "  writing top-level names to yarl.egg-info/top_level.txt\n",
      "  warning: manifest_maker: standard file '-c' not found\n",
      "  \n",
      "  reading manifest file 'yarl.egg-info/SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no previously-included files matching '*.pyc' found anywhere in distribution\n",
      "  warning: no previously-included files matching '*.cache' found anywhere in distribution\n",
      "  warning: no previously-included files found matching 'yarl/*.html'\n",
      "  warning: no previously-included files found matching 'yarl/*.so'\n",
      "  warning: no previously-included files found matching 'yarl/*.pyd'\n",
      "  no previously-included directories found matching 'docs/_build'\n",
      "  writing manifest file 'yarl.egg-info/SOURCES.txt'\n",
      "  copying yarl/__init__.pyi -> build/lib.linux-x86_64-3.7/yarl\n",
      "  copying yarl/_quoting_c.c -> build/lib.linux-x86_64-3.7/yarl\n",
      "  copying yarl/_quoting_c.pyi -> build/lib.linux-x86_64-3.7/yarl\n",
      "  copying yarl/_quoting_c.pyx -> build/lib.linux-x86_64-3.7/yarl\n",
      "  copying yarl/py.typed -> build/lib.linux-x86_64-3.7/yarl\n",
      "  running build_ext\n",
      "  building 'yarl._quoting_c' extension\n",
      "  creating build/temp.linux-x86_64-3.7\n",
      "  creating build/temp.linux-x86_64-3.7/yarl\n",
      "  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -I/usr/include/python3.7m -c yarl/_quoting_c.c -o build/temp.linux-x86_64-3.7/yarl/_quoting_c.o\n",
      "  yarl/_quoting_c.c:4:10: fatal error: Python.h: No such file or directory\n",
      "   #include \"Python.h\"\n",
      "            ^~~~~~~~~~\n",
      "  compilation terminated.\n",
      "  error: command 'gcc' failed with exit status 1\n",
      "  \n",
      "  ----------------------------------------\n",
      "  Running setup.py clean for yarl\n",
      "Successfully built wrapt aiohttp\n",
      "Failed to build multidict yarl\n",
      "Installing collected packages: typing-extensions, aioitertools, docutils, botocore, wrapt, attrs, multidict, async-timeout, yarl, aiohttp, aiobotocore, fsspec, s3fs\n",
      "  Found existing installation: botocore 1.19.4\n",
      "    Uninstalling botocore-1.19.4:\n",
      "      Successfully uninstalled botocore-1.19.4\n",
      "  Running setup.py install for multidict: started\n",
      "    Running setup.py install for multidict: finished with status 'error'\n",
      "    Complete output from command /tmp/1603597215298-0/bin/python -u -c \"import setuptools, tokenize;__file__='/mnt/tmp/pip-build-730hag_t/multidict/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-c5t6zlmz-record/install-record.txt --single-version-externally-managed --compile --install-headers /tmp/1603597215298-0/include/site/python3.7/multidict:\n",
      "    **********************\n",
      "    * Accellerated build *\n",
      "    **********************\n",
      "    /usr/lib64/python3.7/distutils/dist.py:274: UserWarning: Unknown distribution option: 'project_urls'\n",
      "      warnings.warn(msg)\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib.linux-x86_64-3.7\n",
      "    creating build/lib.linux-x86_64-3.7/multidict\n",
      "    copying multidict/__init__.py -> build/lib.linux-x86_64-3.7/multidict\n",
      "    copying multidict/_abc.py -> build/lib.linux-x86_64-3.7/multidict\n",
      "    copying multidict/_compat.py -> build/lib.linux-x86_64-3.7/multidict\n",
      "    copying multidict/_multidict_base.py -> build/lib.linux-x86_64-3.7/multidict\n",
      "    copying multidict/_multidict_py.py -> build/lib.linux-x86_64-3.7/multidict\n",
      "    running egg_info\n",
      "    writing multidict.egg-info/PKG-INFO\n",
      "    writing dependency_links to multidict.egg-info/dependency_links.txt\n",
      "    writing top-level names to multidict.egg-info/top_level.txt\n",
      "    warning: manifest_maker: standard file '-c' not found\n",
      "    \n",
      "    reading manifest file 'multidict.egg-info/SOURCES.txt'\n",
      "    reading manifest template 'MANIFEST.in'\n",
      "    warning: no previously-included files matching '*.pyc' found anywhere in distribution\n",
      "    warning: no previously-included files found matching 'multidict/_multidict.html'\n",
      "    warning: no previously-included files found matching 'multidict/*.so'\n",
      "    warning: no previously-included files found matching 'multidict/*.pyd'\n",
      "    warning: no previously-included files found matching 'multidict/*.pyd'\n",
      "    no previously-included directories found matching 'docs/_build'\n",
      "    writing manifest file 'multidict.egg-info/SOURCES.txt'\n",
      "    copying multidict/__init__.pyi -> build/lib.linux-x86_64-3.7/multidict\n",
      "    copying multidict/_multidict.c -> build/lib.linux-x86_64-3.7/multidict\n",
      "    copying multidict/py.typed -> build/lib.linux-x86_64-3.7/multidict\n",
      "    creating build/lib.linux-x86_64-3.7/multidict/_multilib\n",
      "    copying multidict/_multilib/defs.h -> build/lib.linux-x86_64-3.7/multidict/_multilib\n",
      "    copying multidict/_multilib/dict.h -> build/lib.linux-x86_64-3.7/multidict/_multilib\n",
      "    copying multidict/_multilib/istr.h -> build/lib.linux-x86_64-3.7/multidict/_multilib\n",
      "    copying multidict/_multilib/iter.h -> build/lib.linux-x86_64-3.7/multidict/_multilib\n",
      "    copying multidict/_multilib/pair_list.h -> build/lib.linux-x86_64-3.7/multidict/_multilib\n",
      "    copying multidict/_multilib/views.h -> build/lib.linux-x86_64-3.7/multidict/_multilib\n",
      "    running build_ext\n",
      "    building 'multidict._multidict' extension\n",
      "    creating build/temp.linux-x86_64-3.7\n",
      "    creating build/temp.linux-x86_64-3.7/multidict\n",
      "    gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -I/usr/include/python3.7m -c multidict/_multidict.c -o build/temp.linux-x86_64-3.7/multidict/_multidict.o -O2 -std=c99 -Wall -Wsign-compare -Wconversion -fno-strict-aliasing -pedantic\n",
      "    multidict/_multidict.c:1:10: fatal error: Python.h: No such file or directory\n",
      "     #include \"Python.h\"\n",
      "              ^~~~~~~~~~\n",
      "    compilation terminated.\n",
      "    error: command 'gcc' failed with exit status 1\n",
      "    \n",
      "    ----------------------------------------\n",
      "\n",
      "Collecting fsspec\n",
      "  Using cached https://files.pythonhosted.org/packages/a5/8b/1df260f860f17cb08698170153ef7db672c497c1840dcc8613ce26a8a005/fsspec-0.8.4-py3-none-any.whl\n",
      "Installing collected packages: fsspec\n",
      "Successfully installed fsspec-0.8.4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Ignoring idna-ssl: markers 'python_version < \"3.7\"' don't match your environment\n",
      "  Ignoring typing-extensions: markers 'python_version < \"3.7\"' don't match your environment\n",
      "  Failed building wheel for multidict\n",
      "  Failed building wheel for yarl\n",
      "Command \"/tmp/1603597215298-0/bin/python -u -c \"import setuptools, tokenize;__file__='/mnt/tmp/pip-build-730hag_t/multidict/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-c5t6zlmz-record/install-record.txt --single-version-externally-managed --compile --install-headers /tmp/1603597215298-0/include/site/python3.7/multidict\" failed with error code 1 in /mnt/tmp/pip-build-730hag_t/multidict/"
     ]
    }
   ],
   "source": [
    "# Install libraries within the notebook scope\n",
    "sc.install_pypi_package(\"boto3\")\n",
    "sc.install_pypi_package(\"pandas\")\n",
    "sc.install_pypi_package(\"requests\")\n",
    "sc.install_pypi_package(\"s3fs\")\n",
    "sc.install_pypi_package(\"fsspec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ef73a3534e45b1abcea62148837f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import boto3\n",
    "from datetime import datetime\n",
    "import fsspec\n",
    "import pandas as pd\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql import functions as f, types as t, Window\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "import s3fs\n",
    "import subprocess\n",
    "import timeit\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Removes truncation of columns, column values in Pandas\n",
    "# by default\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "# Monkey patching the DataFrame transform method for Spark 2.4\n",
    "# This is available by default in Spark 3.0\n",
    "def transform(self, f):\n",
    "    return f(self)\n",
    "DataFrame.transform = transform\n",
    "\n",
    "# Override the timeit template to return the command's\n",
    "# return value in addition to the time\n",
    "# Reference: https://stackoverflow.com/questions/24812253/how-can-i-capture-return-value-with-python-timeit-module\n",
    "timeit.template = \"\"\"\n",
    "def inner(_it, _timer{init}):\n",
    "    {setup}\n",
    "    _t0 = _timer()\n",
    "    for _i in _it:\n",
    "        retval = {stmt}\n",
    "    _t1 = _timer()\n",
    "    return _t1 - _t0, retval\n",
    "\"\"\"\n",
    "\n",
    "def shell_cmd(cmd):\n",
    "    \"\"\"\n",
    "    Wrapper for running shell commands and printing the output\n",
    "    Some helpful recipes:\n",
    "    - List files on hdfs: shell_cmd(\"hdfs dfs -ls hdfs:///tmp/data/\")\n",
    "    - Remove files from hdfs: shell_cmd(\"hdfs dfs -rm -r hdfs:///tmp/data/test_pyspark\")\n",
    "    \"\"\"\n",
    "    for line in subprocess.check_output(cmd, shell=True).split(b'\\n'):\n",
    "        print(line)\n",
    "\n",
    "def timer_method(cmd):\n",
    "    \"\"\"\n",
    "    Wrapper for timeit that returns the value of a function and its runtime\n",
    "    To use, pass a string of the function you wish to time\n",
    "    Example: \n",
    "     run_time, result = timer_method(\"myfunction(arg1, arg2)\")\n",
    "    \"\"\"\n",
    "    # Setting globals = globals() enables the timeit function\n",
    "    # to return the value generated by cmd\n",
    "    return timeit.timeit(cmd, number=1, globals = globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set your s3 bucket name\n",
    "This should be data-scale-oreilly-{your name}   \n",
    "If you dont remember check the [S3 console](https://s3.console.aws.amazon.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19476269300f47cd9f0a44d68f7eb9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MY_BUCKET_NAME = \"data-scale-oreilly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting from an S3 bucket - NYC Taxi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "* Taxi data \n",
    "* Data dictionaries \n",
    "* Taxi zone lookup table\n",
    "\n",
    "Take a few minutes to look through the Data Dictionaries and Metadata and the Taxi Zone Maps and Lookup Tables. What are some things you notice about the data?\n",
    "\n",
    "Data ingestion has the ultimate goal of collecting, aggregating, and surfacing data for a specific purpose; an analysis, an API, a dashboard, etc. Think about how you might use the taxi data to answer the following questions:\n",
    "\n",
    "1. Which borough is the most popular pickup or drop off spot?\n",
    "1. Are green taxis more popular for trips within the same borough vs yellow taxis?\n",
    "1. Build a recommendation engine that predicts surge pricing for a given time of day based on historical data  \n",
    "\n",
    "With this in mind, lets work through bringing this data onto the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505d39da69f94456a78425a002875359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note, if you copy the link from the taxi data website you will see:\n",
    "# https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-01.csv\n",
    "# Two things - first, the portion of the URL following \"aws.com\" is the \n",
    "# bucket name. Second, in \"trip+data\" the \"+\" is a space\n",
    "taxi_data_path = \"s3://nyc-tlc/trip data/yellow_tripdata_2020-01.csv\"\n",
    "\n",
    "# When working with big data it can be challenging to view the data. How would you \n",
    "# go about getting a sample of this data? (download it, use requests, pandas, etc)\n",
    "\n",
    "# Pandas uses s3fs to read_csv from s3:\n",
    "pd.read_csv(taxi_data_path, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ea0250bf234f35b2000e7b2b022f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VendorID                  object\n",
      "tpep_pickup_datetime      object\n",
      "tpep_dropoff_datetime     object\n",
      "passenger_count           object\n",
      "trip_distance            float64\n",
      "RatecodeID                object\n",
      "store_and_fwd_flag        object\n",
      "PULocationID               int64\n",
      "DOLocationID               int64\n",
      "payment_type              object\n",
      "fare_amount              float64\n",
      "extra                    float64\n",
      "mta_tax                  float64\n",
      "tip_amount               float64\n",
      "tolls_amount             float64\n",
      "improvement_surcharge    float64\n",
      "total_amount             float64\n",
      "congestion_surcharge     float64\n",
      "dtype: object"
     ]
    }
   ],
   "source": [
    "# Take a look at the data. Notice how pandas will try to assign types. Is this desirable?\n",
    "# Why or why not?\n",
    "# Since we have column names it also seems this data has a header\n",
    "pd_df_taxi.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5a03a2c1864272942235a9430a6269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For reference, look at the Spark DataFrameReader, csv:\n",
    "# https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html\n",
    "ps_df_taxi = spark.read.option('header', True).csv(taxi_data_path)\n",
    "\n",
    "# Talk through the spark UI here, partcularly note that the cell number will show up in the SparkUI\n",
    "# Job list next to the Job id. For example. this would be Job 2 (cell number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af2b210d47247ba8f582b0476eecf2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|1       |2020-01-01 00:28:15 |2020-01-01 00:33:03  |1              |1.20         |1         |N                 |238         |239         |1           |6          |3    |0.5    |1.47      |0           |0.3                  |11.27       |2.5                 |\n",
      "|1       |2020-01-01 00:35:39 |2020-01-01 00:43:04  |1              |1.20         |1         |N                 |239         |238         |1           |7          |3    |0.5    |1.5       |0           |0.3                  |12.3        |2.5                 |\n",
      "|1       |2020-01-01 00:47:41 |2020-01-01 00:53:52  |1              |.60          |1         |N                 |238         |238         |1           |6          |3    |0.5    |1         |0           |0.3                  |10.8        |2.5                 |\n",
      "|1       |2020-01-01 00:55:23 |2020-01-01 01:00:14  |1              |.80          |1         |N                 |238         |151         |1           |5.5        |0.5  |0.5    |1.36      |0           |0.3                  |8.16        |0                   |\n",
      "|2       |2020-01-01 00:01:58 |2020-01-01 00:04:16  |1              |.00          |1         |N                 |193         |193         |2           |3.5        |0.5  |0.5    |0         |0           |0.3                  |4.8         |0                   |\n",
      "|2       |2020-01-01 00:09:44 |2020-01-01 00:10:37  |1              |.03          |1         |N                 |7           |193         |2           |2.5        |0.5  |0.5    |0         |0           |0.3                  |3.8         |0                   |\n",
      "|2       |2020-01-01 00:39:25 |2020-01-01 00:39:29  |1              |.00          |1         |N                 |193         |193         |1           |2.5        |0.5  |0.5    |0.01      |0           |0.3                  |3.81        |0                   |\n",
      "|2       |2019-12-18 15:27:49 |2019-12-18 15:28:59  |1              |.00          |5         |N                 |193         |193         |1           |0.01       |0    |0      |0         |0           |0.3                  |2.81        |2.5                 |\n",
      "|2       |2019-12-18 15:30:35 |2019-12-18 15:31:35  |4              |.00          |1         |N                 |193         |193         |1           |2.5        |0.5  |0.5    |0         |0           |0.3                  |6.3         |2.5                 |\n",
      "|1       |2020-01-01 00:29:01 |2020-01-01 00:40:28  |2              |.70          |1         |N                 |246         |48          |1           |8          |3    |0.5    |2.35      |0           |0.3                  |14.15       |2.5                 |\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "ps_df_taxi.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895bc69774a94508b6437c4e13c1d1d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- congestion_surcharge: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "# Lets see how the spark dataframe reader interpreted the data\n",
    "# Talk about nullable vs non nullable and maybe a small bit about data schemas\n",
    "ps_df_taxi.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2319e1d62f6643a081865a179c196a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'path hdfs://ip-172-31-6-49.us-east-2.compute.internal:8020/tmp/input/taxi_data already exists.;'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 932, in csv\n",
      "    self._jwrite.csv(path)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: 'path hdfs://ip-172-31-6-49.us-east-2.compute.internal:8020/tmp/input/taxi_data already exists.;'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Talk through ingest practices around retaining original data vs augmenting\n",
    "# For example, we may want to keep the data in its default format so we can\n",
    "# refer back to it if there are bugs in our data ingestion code\n",
    "ps_df_taxi.write.option(\"header\", True).csv(\"hdfs:///tmp/input/taxi_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c504059d67b74027820b2df10457ceab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Found 9 items'\n",
      "b'-rw-r--r--   1 livy hadoop          0 2020-10-18 23:28 hdfs:///tmp/input/taxi_data/_SUCCESS'\n",
      "b'-rw-r--r--   1 livy hadoop   73917327 2020-10-18 23:28 hdfs:///tmp/input/taxi_data/part-00000-4ec8700e-82dd-454f-a072-d3a2943c8375-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   73919562 2020-10-18 23:28 hdfs:///tmp/input/taxi_data/part-00001-4ec8700e-82dd-454f-a072-d3a2943c8375-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   73920337 2020-10-18 23:28 hdfs:///tmp/input/taxi_data/part-00002-4ec8700e-82dd-454f-a072-d3a2943c8375-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   73921216 2020-10-18 23:28 hdfs:///tmp/input/taxi_data/part-00003-4ec8700e-82dd-454f-a072-d3a2943c8375-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   73918682 2020-10-18 23:28 hdfs:///tmp/input/taxi_data/part-00004-4ec8700e-82dd-454f-a072-d3a2943c8375-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   73921304 2020-10-18 23:28 hdfs:///tmp/input/taxi_data/part-00005-4ec8700e-82dd-454f-a072-d3a2943c8375-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   73919745 2020-10-18 23:28 hdfs:///tmp/input/taxi_data/part-00006-4ec8700e-82dd-454f-a072-d3a2943c8375-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   70423763 2020-10-18 23:28 hdfs:///tmp/input/taxi_data/part-00007-4ec8700e-82dd-454f-a072-d3a2943c8375-c000.csv'\n",
      "b''"
     ]
    }
   ],
   "source": [
    "# Discuss how spark writes files out\n",
    "shell_cmd(\"hdfs dfs -ls hdfs:///tmp/input/taxi_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559885d33f0a4ea39a5ed59a199e6f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|PULocationID|DOLocationID|fare_amount|tip_amount|\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+\n",
      "|2020-01-01 00:28:15 |2020-01-01 00:33:03  |1              |1.20         |238         |239         |6          |1.47      |\n",
      "|2020-01-01 00:35:39 |2020-01-01 00:43:04  |1              |1.20         |239         |238         |7          |1.5       |\n",
      "|2020-01-01 00:47:41 |2020-01-01 00:53:52  |1              |.60          |238         |238         |6          |1         |\n",
      "|2020-01-01 00:55:23 |2020-01-01 01:00:14  |1              |.80          |238         |151         |5.5        |1.36      |\n",
      "|2020-01-01 00:01:58 |2020-01-01 00:04:16  |1              |.00          |193         |193         |3.5        |0         |\n",
      "|2020-01-01 00:09:44 |2020-01-01 00:10:37  |1              |.03          |7           |193         |2.5        |0         |\n",
      "|2020-01-01 00:39:25 |2020-01-01 00:39:29  |1              |.00          |193         |193         |2.5        |0.01      |\n",
      "|2019-12-18 15:27:49 |2019-12-18 15:28:59  |1              |.00          |193         |193         |0.01       |0         |\n",
      "|2019-12-18 15:30:35 |2019-12-18 15:31:35  |4              |.00          |193         |193         |2.5        |0         |\n",
      "|2020-01-01 00:29:01 |2020-01-01 00:40:28  |2              |.70          |246         |48          |8          |2.35      |\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "# Examine the data, what do you notice?\n",
    "column_subset = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID', 'fare_amount', 'tip_amount']\n",
    "ps_df_taxi.select(*column_subset).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d15aa1d04143f78f2d0e60c830dccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---------------------+------------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "|summary|tpep_pickup_datetime|tpep_dropoff_datetime|   passenger_count|     trip_distance|      PULocationID|     DOLocationID|       fare_amount|        tip_amount|\n",
      "+-------+--------------------+---------------------+------------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "|  count|             6405008|              6405008|           6339567|           6405008|           6405008|          6405008|           6405008|           6405008|\n",
      "|   mean|                null|                 null|1.5153326717739555|2.9296439333096704|164.73225778952968|162.6626908194338|  12.6941081197704|2.1893418306424284|\n",
      "| stddev|                null|                 null|1.1515942134278123|  83.1591059732504| 65.54373944111667| 69.9126062949612|12.127295340046542| 2.760028392378421|\n",
      "|    min| 2003-01-01 00:07:17|  2003-01-01 14:16:59|                 0|              -.01|                 1|                1|             -0.01|             -0.01|\n",
      "|    max| 2021-01-02 01:12:10|  2021-01-02 01:25:01|                 9|             99.03|                99|               99|             99.99|             99.99|\n",
      "+-------+--------------------+---------------------+------------------+------------------+------------------+-----------------+------------------+------------------+"
     ]
    }
   ],
   "source": [
    "# Keep in mind the datatype when considering these results\n",
    "ps_df_taxi.select(*column_subset).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277dd5bfb85744efa89281cb2188b2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|passenger_count|\n",
      "+---------------+\n",
      "|           null|\n",
      "|              1|\n",
      "|              6|\n",
      "|              3|\n",
      "|              4|\n",
      "|              8|\n",
      "|              5|\n",
      "|              2|\n",
      "|              7|\n",
      "|              0|\n",
      "|              9|\n",
      "+---------------+"
     ]
    }
   ],
   "source": [
    "# Casting columns to a type\n",
    "(ps_df_taxi.select(\"passenger_count\")\n",
    " .withColumn(\"passenger_count\", f.col(\"passenger_count\").cast(t.IntegerType()))\n",
    ").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0189d1e67e045fc8e64d01dd101a5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid literal for int() with base 10: ''\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/pandas/core/generic.py\", line 5531, in astype\n",
      "    col.astype(dtype=dtype[col_name], copy=copy, errors=errors)\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/pandas/core/generic.py\", line 5546, in astype\n",
      "    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors,)\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/pandas/core/internals/managers.py\", line 595, in astype\n",
      "    return self.apply(\"astype\", dtype=dtype, copy=copy, errors=errors)\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/pandas/core/internals/managers.py\", line 406, in apply\n",
      "    applied = getattr(b, f)(**kwargs)\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/pandas/core/internals/blocks.py\", line 595, in astype\n",
      "    values = astype_nansafe(vals1d, dtype, copy=True)\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/pandas/core/dtypes/cast.py\", line 919, in astype_nansafe\n",
      "    return dtype.construct_array_type()._from_sequence(arr, dtype=dtype, copy=copy)\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/pandas/core/arrays/integer.py\", line 378, in _from_sequence\n",
      "    return integer_array(scalars, dtype=dtype, copy=copy)\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/pandas/core/arrays/integer.py\", line 160, in integer_array\n",
      "    values, mask = coerce_to_array(values, dtype=dtype, copy=copy)\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/pandas/core/arrays/integer.py\", line 277, in coerce_to_array\n",
      "    values = safe_cast(values, dtype, copy=False)\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/pandas/core/arrays/integer.py\", line 175, in safe_cast\n",
      "    casted = values.astype(dtype, copy=copy)\n",
      "ValueError: invalid literal for int() with base 10: ''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Casting pandas columns to a type - this will give an error on empty cells\n",
    "(pd_df_taxi[[*column_subset]]\n",
    "         .astype({'passenger_count': 'Int64'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ed290f28994f06b87dae5ed27ca4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IntegerArray>\n",
      "[1, 4, 2, 3, 6, 5, 0, 8, 7, 9, <NA>]\n",
      "Length: 11, dtype: Int64"
     ]
    }
   ],
   "source": [
    "# To convert to Integer using pandas, we have to first deal with the null values\n",
    "# to_numeric with 'coerce' will fill invalid integer values with np.NaN\n",
    "# the Int64 type in later versions of pandas will convert np.NaN to a nullable\n",
    "# integer type: https://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html\n",
    "pd.to_numeric(pd_df_taxi.passenger_count, errors='coerce').astype('Int64').unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.1 - Transform the column_subset of taxi data to data types that accurately represent the data\n",
    "\n",
    "Result: a transformed_taxi dataframe with the column_subset columns cast to an appropriate type\n",
    "\n",
    "Available types are listed in the pyspark.sql.types module https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#module-pyspark.sql.types  \n",
    "\n",
    "This is imported as `t`, so to apply the IntegerType use `t.IntegerType()`\n",
    "\n",
    "For pandas, see the following resources on converting types\n",
    "https://stackoverflow.com/questions/15891038/change-column-type-in-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43501a28d4d547f4a0e1d01a7c9c21a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Talk about transforming\n",
    "# the data into a type schema to surface for analytical operations - i.e. pay the penalty of time\n",
    "# on the ingest to transform strings to timestamps so the analysis side can use datetime methods\n",
    "# without having to remember to cast\n",
    "\n",
    "# Any gotchyas on transforming types we should discuss? Perhaps datetime casting\n",
    "def transform_taxi_ps(ps_df_taxi):\n",
    "    res = (ps_df_taxi\n",
    "            .select(*column_subset)\n",
    "            .withColumn(\"tpep_pickup_datetime\", f.col(\"tpep_pickup_datetime\").cast(t.TimestampType()))\n",
    "            .withColumn(\"tpep_dropoff_datetime\", f.col(\"tpep_dropoff_datetime\").cast(t.TimestampType()))\n",
    "            .withColumn(\"passenger_count\", f.col(\"passenger_count\").cast(t.IntegerType()))\n",
    "            .withColumn(\"trip_distance\", f.col(\"trip_distance\").cast(t.FloatType()))\n",
    "            .withColumn(\"PULocationID\", f.col(\"PULocationID\").cast(t.IntegerType()))\n",
    "            .withColumn(\"DOLocationID\", f.col(\"DOLocationID\").cast(t.IntegerType()))\n",
    "            .withColumn(\"fare_amount\", f.col(\"fare_amount\").cast(t.FloatType()))\n",
    "            .withColumn(\"tip_amount\", f.col(\"tip_amount\").cast(t.FloatType()))             \n",
    "           )\n",
    "    res.count()  # Run this to force pyspark to collect the data\n",
    "    return res\n",
    "\n",
    "def transform_taxi_pd(pd_df_taxi):\n",
    "    res = pd_df_taxi[[*column_subset]].copy()\n",
    "    res.tpep_pickup_datetime = pd.to_datetime(res.tpep_pickup_datetime)\n",
    "    res.tpep_dropoff_datetime = pd.to_datetime(res.tpep_dropoff_datetime)\n",
    "    res.passenger_count = pd.to_numeric(res.passenger_count, errors='coerce').astype('Int64')\n",
    "    res.trip_distance = pd.to_numeric(res.trip_distance, errors='coerce')\n",
    "    res.PULocationID = pd.to_numeric(res.PULocationID, errors='coerce').astype('Int64')\n",
    "    res.DOLocationID = pd.to_numeric(res.DOLocationID, errors='coerce').astype('Int64')\n",
    "    res.fare_amount = pd.to_numeric(res.fare_amount, errors='coerce')\n",
    "    res.tip_amount = pd.to_numeric(res.tip_amount, errors='coerce')\n",
    "    res.count()\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f051bd29a70456ea427fa79bd70bb9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ps_run_time, ps_transform_taxi = timer_method(\"transform_taxi_ps(ps_df_taxi)\")\n",
    "pd_run_time, pd_transform_taxi = timer_method(\"transform_taxi_pd(pd_df_taxi)\")\n",
    "print(f\"pyspark runtime: {ps_run_time} pandas runtime {pd_run_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c30b91c7d844b9ba51ee31a987b63b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name 'pd_transform_taxi' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'pd_transform_taxi' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pd_transform_taxi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64390b10a20f4726aa284e8163e63250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|PULocationID|DOLocationID|fare_amount|tip_amount|\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+\n",
      "| 2020-01-01 00:28:15|  2020-01-01 00:33:03|              1|          1.2|         238|         239|        6.0|      1.47|\n",
      "| 2020-01-01 00:35:39|  2020-01-01 00:43:04|              1|          1.2|         239|         238|        7.0|       1.5|\n",
      "| 2020-01-01 00:47:41|  2020-01-01 00:53:52|              1|          0.6|         238|         238|        6.0|       1.0|\n",
      "| 2020-01-01 00:55:23|  2020-01-01 01:00:14|              1|          0.8|         238|         151|        5.5|      1.36|\n",
      "| 2020-01-01 00:01:58|  2020-01-01 00:04:16|              1|          0.0|         193|         193|        3.5|       0.0|\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "ps_transform_taxi.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa16837e097a49e09c27bf1fbc17ef71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "|summary|   passenger_count|     trip_distance|      PULocationID|     DOLocationID|       fare_amount|        tip_amount|\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "|  count|           6339567|           6405008|           6405008|          6405008|           6405008|           6405008|\n",
      "|   mean|1.5153326717739555|2.9296439317743554|164.73225778952968|162.6626908194338|12.694108121822374|2.1893418282101753|\n",
      "| stddev| 1.151594213427813| 83.15910301291039| 65.54373944111757|69.91260629496095|12.127295342892479|2.7600283861848287|\n",
      "|    min|                 0|            -30.62|                 1|                1|           -1238.0|             -91.0|\n",
      "|    max|                 9|         210240.06|               265|              265|            4265.0|            1100.0|\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+------------------+"
     ]
    }
   ],
   "source": [
    "# Note that this only shows results for numeric and string columns - now that the data has been cast to \n",
    "# types, we can explore it a bit more\n",
    "ps_transform_taxi.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6bf59827ba408aa807c9e4dd2e7e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|tpep_pickup_datetime|\n",
      "+--------------------+\n",
      "|2003-01-01 00:07:17 |\n",
      "|2008-12-31 23:02:40 |\n",
      "|2008-12-31 23:02:50 |\n",
      "|2008-12-31 23:03:44 |\n",
      "|2008-12-31 23:03:48 |\n",
      "|2008-12-31 23:06:13 |\n",
      "|2008-12-31 23:17:15 |\n",
      "|2008-12-31 23:24:11 |\n",
      "|2008-12-31 23:34:13 |\n",
      "|2008-12-31 23:35:00 |\n",
      "+--------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "# Take a look at the date ranges...\n",
    "ps_transform_taxi.select(\"tpep_pickup_datetime\").sort(f.asc(\"tpep_pickup_datetime\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47b0160f34f4f809ef506c7d737a809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write out transformed data to EBS\n",
    "ps_transform_taxi.write.mode(\"append\").json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.2 - Write an ingestion function that does the following:\n",
    "Given a file path to a taxi data csv (i.e. s3://nyc-tlc/trip data/green_tripdata_2020-01.csv)\n",
    "1. Read the file into a Spark dataframe\n",
    "2. Transform the column_subset\n",
    "3. Write the data as json to hdfs in append mode to `hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json`\n",
    "\n",
    "Function signature:  \n",
    "`def ingest_taxi_data(file_name)`\n",
    "\n",
    "Inputs can be created from:  \n",
    "`taxi_data_prefix = \"s3://nyc-tlc/trip data/\"\n",
    "taxi_data_files = [\"yellow_tripdata_2019-01.csv\", \"yellow_tripdata_2018-01.csv\", \"yellow_tripdata_2017-01.csv\"]  `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f2cd673fa940e2b5fbb9ba081f19bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ingest_taxi_data(file_name):\n",
    "    (spark\n",
    "         .read\n",
    "         .option('header', True).csv(file_name)\n",
    "         .select(*column_subset)\n",
    "         .withColumn(\"tpep_pickup_datetime\", f.col(\"tpep_pickup_datetime\").cast(t.TimestampType()))\n",
    "         .withColumn(\"tpep_dropoff_datetime\", f.col(\"tpep_dropoff_datetime\").cast(t.TimestampType()))\n",
    "         .withColumn(\"passenger_count\", f.col(\"passenger_count\").cast(t.IntegerType()))\n",
    "         .withColumn(\"trip_distance\", f.col(\"trip_distance\").cast(t.FloatType()))\n",
    "         .withColumn(\"PULocationID\", f.col(\"PULocationID\").cast(t.IntegerType()))\n",
    "         .withColumn(\"DOLocationID\", f.col(\"DOLocationID\").cast(t.IntegerType()))\n",
    "         .withColumn(\"fare_amount\", f.col(\"fare_amount\").cast(t.FloatType()))\n",
    "         .withColumn(\"tip_amount\", f.col(\"tip_amount\").cast(t.FloatType()))             \n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c1c6acd92c406da5bbacb092943d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the ingest for several files\n",
    "taxi_data_prefix = \"s3://nyc-tlc/trip data/\"\n",
    "taxi_data_files = [\"yellow_tripdata_2019-01.csv\", \"yellow_tripdata_2018-01.csv\", \"yellow_tripdata_2017-01.csv\"]\n",
    "for file_name in taxi_data_files: \n",
    "    taxi_data_path = f\"{taxi_data_prefix}{file_name}\"\n",
    "    ingest_taxi_data(taxi_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511c35802f144b33b7c5e553c8ceec13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- ingested_on: string (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "df.printSchema()\n",
    "\n",
    "# Talk through how the read.json interpreted the Integers as Longs, set stage for using schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing ingestion code\n",
    "\n",
    "The `ingest_taxi_data` method is not well structured for testing:\n",
    "* Writes to the file system\n",
    "* Requires an input file to test\n",
    "* What other shortcomings?\n",
    "\n",
    "To make this code more testable, split out the transformation logic so it can be unit tested.  \n",
    "Definining a transformation function that takes a dataframe and returns a dataframe provides a better interface for unit testing, and a more extensible structure in case we need to add more dataframe functions before or after the transformation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81edf74529df4efdb19f8e293e855206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform_taxi_data(df):\n",
    "    return (df.withColumn(\"tpep_pickup_datetime\", f.col(\"tpep_pickup_datetime\").cast(t.TimestampType()))\n",
    "     .withColumn(\"tpep_dropoff_datetime\", f.col(\"tpep_dropoff_datetime\").cast(t.TimestampType()))\n",
    "     .withColumn(\"passenger_count\", f.col(\"passenger_count\").cast(t.IntegerType()))\n",
    "     .withColumn(\"trip_distance\", f.col(\"trip_distance\").cast(t.FloatType()))\n",
    "     .withColumn(\"PULocationID\", f.col(\"PULocationID\").cast(t.IntegerType()))\n",
    "     .withColumn(\"DOLocationID\", f.col(\"DOLocationID\").cast(t.IntegerType()))\n",
    "     .withColumn(\"fare_amount\", f.col(\"fare_amount\").cast(t.FloatType()))\n",
    "     .withColumn(\"tip_amount\", f.col(\"tip_amount\").cast(t.FloatType())))\n",
    "     \n",
    "def ingest_taxi_data_transform(file_name):\n",
    "    # Requires patching of Dataframe.transform method in Spark 2.4, but available natively\n",
    "    # in Spark 3.0 https://mungingdata.com/pyspark/chaining-dataframe-transformations/\n",
    "    df_input = (spark\n",
    "         .read\n",
    "         .option('header', True).csv(taxi_data_path)\n",
    "         .select(*column_subset)\n",
    "         .transform(transform_taxi_data)\n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )\n",
    "    \n",
    "def ingest_taxi_data_method(file_name):\n",
    "    # Equivalent code without using the monkey-patched transform method for DataFrame\n",
    "    df_input = (spark\n",
    "         .read\n",
    "         .option('header', True).csv(taxi_data_path)\n",
    "         .select(*column_subset))\n",
    "    \n",
    "    (transform_taxi_data(df_input)\n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4303b99da2964207bdf17bc5c757a45d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = [\n",
    "    \"{'tpep_pickup_datetime': '2020-05-23 21:05:23', 'tpep_dropoff_datetime': '2020-05-23 08:05:23', 'passenger_count': 0, 'trip_distance': 10.5, 'PULocationID': 1, 'DOLocationID': 254, 'fare_amount': 0.05, 'tip_amount': 1.00}\",\n",
    "    \"{'tpep_pickup_datetime': '2020-10-01 01:05:23', 'tpep_dropoff_datetime': '2020-10-01 02:05:23', 'passenger_count': 1, 'trip_distance': 0.1, 'PULocationID': 45, 'DOLocationID': 3, 'fare_amount': 10.0, 'tip_amount': 5.00}\",\n",
    "    \"{'tpep_pickup_datetime': '2020-02-02 15:22:23', 'tpep_dropoff_datetime': '2020-02-03 15:44:23', 'passenger_count': 3, 'trip_distance': 3.25, 'PULocationID': 10, 'DOLocationID': 24, 'fare_amount': 5.05, 'tip_amount': 1.00}\"\n",
    "]\n",
    "expected_types = {'DOLocationID': 'int', 'PULocationID': 'int', 'fare_amount': 'float', 'passenger_count': 'int', 'tip_amount': 'float', 'tpep_dropoff_datetime': 'timestamp', 'tpep_pickup_datetime': 'timestamp', 'trip_distance': 'float'}\n",
    "\n",
    "test_df = spark.read.json(sc.parallelize(test_data))\n",
    "test = transform_taxi_data(test_df)\n",
    "test_types = {item[0]:item[1] for item in test.dtypes}\n",
    "\n",
    "assert expected_types == test_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c898379cf1e40e781994e9e21602c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/_SUCCESS'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00000-9f370f87-7731-4a6c-95ef-e39fda4a71cb-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00000-be7dcba6-3674-4feb-804d-c9182e22e90c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00000-dab6c9ed-a352-48cb-b5e9-402b56f65a64-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00000-f6e04c76-02bf-437a-9790-15cf365abefa-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00001-9f370f87-7731-4a6c-95ef-e39fda4a71cb-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00001-be7dcba6-3674-4feb-804d-c9182e22e90c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00001-dab6c9ed-a352-48cb-b5e9-402b56f65a64-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00001-f6e04c76-02bf-437a-9790-15cf365abefa-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00002-9f370f87-7731-4a6c-95ef-e39fda4a71cb-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00002-be7dcba6-3674-4feb-804d-c9182e22e90c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00002-dab6c9ed-a352-48cb-b5e9-402b56f65a64-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00002-f6e04c76-02bf-437a-9790-15cf365abefa-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00003-9f370f87-7731-4a6c-95ef-e39fda4a71cb-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00003-be7dcba6-3674-4feb-804d-c9182e22e90c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00003-dab6c9ed-a352-48cb-b5e9-402b56f65a64-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00003-f6e04c76-02bf-437a-9790-15cf365abefa-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00004-9f370f87-7731-4a6c-95ef-e39fda4a71cb-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00004-be7dcba6-3674-4feb-804d-c9182e22e90c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00004-dab6c9ed-a352-48cb-b5e9-402b56f65a64-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00004-f6e04c76-02bf-437a-9790-15cf365abefa-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00005-9f370f87-7731-4a6c-95ef-e39fda4a71cb-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00005-be7dcba6-3674-4feb-804d-c9182e22e90c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00005-dab6c9ed-a352-48cb-b5e9-402b56f65a64-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00005-f6e04c76-02bf-437a-9790-15cf365abefa-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00006-9f370f87-7731-4a6c-95ef-e39fda4a71cb-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00006-dab6c9ed-a352-48cb-b5e9-402b56f65a64-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00007-9f370f87-7731-4a6c-95ef-e39fda4a71cb-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00007-dab6c9ed-a352-48cb-b5e9-402b56f65a64-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00008-9f370f87-7731-4a6c-95ef-e39fda4a71cb-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00009-9f370f87-7731-4a6c-95ef-e39fda4a71cb-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00010-9f370f87-7731-4a6c-95ef-e39fda4a71cb-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00011-9f370f87-7731-4a6c-95ef-e39fda4a71cb-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00012-9f370f87-7731-4a6c-95ef-e39fda4a71cb-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00013-9f370f87-7731-4a6c-95ef-e39fda4a71cb-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00014-9f370f87-7731-4a6c-95ef-e39fda4a71cb-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00015-9f370f87-7731-4a6c-95ef-e39fda4a71cb-c000.json'\n",
      "b''"
     ]
    }
   ],
   "source": [
    "shell_cmd(\"hdfs dfs -rm hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18273738da540d093ee460fe9da8eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Re run the ingestion using the functions with the transformation broken out\n",
    "taxi_data_prefix = \"s3://nyc-tlc/trip data/\"\n",
    "taxi_data_files = [\"yellow_tripdata_2019-01.csv\", \"yellow_tripdata_2018-01.csv\", \"yellow_tripdata_2017-01.csv\"]\n",
    "for file_name in taxi_data_files: \n",
    "    taxi_data_path = f\"{taxi_data_prefix}{file_name}\"\n",
    "    ingest_taxi_data_method(file_name)\n",
    "    #ingest_taxi_data_transform(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95cd43e32c6245368366514b06842be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26137790"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets try running the ingestion code on the other taxi data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a330a89579442a8a998dc78789a675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"cannot resolve '`tpep_pickup_datetime`' given input columns: [tolls_amount, congestion_surcharge, payment_type, extra, mta_tax, store_and_fwd_flag, lpep_dropoff_datetime, trip_type, PULocationID, improvement_surcharge, fare_amount, total_amount, tip_amount, ehail_fee, DOLocationID, lpep_pickup_datetime, RatecodeID, VendorID, trip_distance, passenger_count];;\\n'Project ['tpep_pickup_datetime, 'tpep_dropoff_datetime, passenger_count#2458, trip_distance#2459, PULocationID#2456, DOLocationID#2457, fare_amount#2460, tip_amount#2463]\\n+- Relation[VendorID#2451,lpep_pickup_datetime#2452,lpep_dropoff_datetime#2453,store_and_fwd_flag#2454,RatecodeID#2455,PULocationID#2456,DOLocationID#2457,passenger_count#2458,trip_distance#2459,fare_amount#2460,extra#2461,mta_tax#2462,tip_amount#2463,tolls_amount#2464,ehail_fee#2465,improvement_surcharge#2466,total_amount#2467,payment_type#2468,trip_type#2469,congestion_surcharge#2470] csv\\n\"\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 17, in ingest_taxi_data_transform\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 1327, in select\n",
      "    jdf = self._jdf.select(self._jcols(*cols))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: \"cannot resolve '`tpep_pickup_datetime`' given input columns: [tolls_amount, congestion_surcharge, payment_type, extra, mta_tax, store_and_fwd_flag, lpep_dropoff_datetime, trip_type, PULocationID, improvement_surcharge, fare_amount, total_amount, tip_amount, ehail_fee, DOLocationID, lpep_pickup_datetime, RatecodeID, VendorID, trip_distance, passenger_count];;\\n'Project ['tpep_pickup_datetime, 'tpep_dropoff_datetime, passenger_count#2458, trip_distance#2459, PULocationID#2456, DOLocationID#2457, fare_amount#2460, tip_amount#2463]\\n+- Relation[VendorID#2451,lpep_pickup_datetime#2452,lpep_dropoff_datetime#2453,store_and_fwd_flag#2454,RatecodeID#2455,PULocationID#2456,DOLocationID#2457,passenger_count#2458,trip_distance#2459,fare_amount#2460,extra#2461,mta_tax#2462,tip_amount#2463,tolls_amount#2464,ehail_fee#2465,improvement_surcharge#2466,total_amount#2467,payment_type#2468,trip_type#2469,congestion_surcharge#2470] csv\\n\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try using the ingest code we created for yellow taxi for all the taxis\n",
    "# This will fail because the datetime fields have different names across different servcies\n",
    "\n",
    "taxi_data_prefix = \"s3://nyc-tlc/trip data/\"\n",
    "taxi_data_files = [\"green_tripdata_2020-01.csv\", \"fhv_tripdata_2020-01.csv\", \"fhvhv_tripdata_2020-01.csv\"]\n",
    "for file_name in taxi_data_files: \n",
    "    taxi_data_path = f\"{taxi_data_prefix}{file_name}\"\n",
    "    ingest_taxi_data_transform(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data modeling\n",
    "Create a model for all taxi data, given that there are differences across the services in the kind of data collected\n",
    "\n",
    "Commmon fields across all services:\n",
    "* PULocationID\n",
    "* DOLocationID\n",
    "\n",
    "Fields we want to normalize across all services - this data is in all services but is named differently\n",
    "* pickup datetime\n",
    "* drop off datetime\n",
    "\n",
    "Service specific fields. These are only in green or yellow data\n",
    "* Passenger_count\n",
    "* Trip_distance\n",
    "* Fare_amount\n",
    "* Tip_amount\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.3 - Write different transformation functions for each taxi service type to match the following signature and schema:\n",
    "Fields / Types:\n",
    "\n",
    "* pickup_datetime Timestamp\n",
    "* dropoff_datetime Timestamp\n",
    "* passenger_count Integer\n",
    "* fare_amount Float\n",
    "* tip_amount Float\n",
    "* PULocationID Integer\n",
    "* DOLocationID Integer\n",
    "\n",
    "`def transform_function(dataframe):  \n",
    "    return transformed_dataframe\n",
    "`\n",
    "\n",
    "Once the transformation functions are done, rewrite `ingest_taxi_data` to use these new functions depending on the file being processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32bc1fa1bd1442d28cade0b11b1af02a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform_yellow_taxi(df):\n",
    "    return (df.withColumn(\"pickup_datetime\", f.col(\"tpep_pickup_datetime\").cast(t.TimestampType()))\n",
    "        .withColumn(\"dropoff_datetime\", f.col(\"tpep_dropoff_datetime\").cast(t.TimestampType()))\n",
    "        .withColumn(\"passenger_count\", f.col(\"passenger_count\").cast(t.IntegerType()))\n",
    "        .withColumn(\"trip_distance\", f.col(\"trip_distance\").cast(t.FloatType()))\n",
    "        .withColumn(\"fare_amount\", f.col(\"fare_amount\").cast(t.FloatType()))\n",
    "        .withColumn(\"tip_amount\", f.col(\"tip_amount\").cast(t.FloatType()))\n",
    "        .withColumn(\"PULocationID\", f.col(\"PULocationID\").cast(t.IntegerType()))\n",
    "        .withColumn(\"DOLocationID\", f.col(\"DOLocationID\").cast(t.IntegerType())))\n",
    "        \n",
    "        \n",
    "def transform_green_taxi(df):\n",
    "    return (df.withColumn(\"pickup_datetime\", f.col(\"lpep_pickup_datetime\").cast(t.TimestampType()))\n",
    "        .withColumn(\"dropoff_datetime\", f.col(\"lpep_dropoff_datetime\").cast(t.TimestampType()))\n",
    "        .withColumn(\"passenger_count\", f.col(\"passenger_count\").cast(t.IntegerType()))\n",
    "        .withColumn(\"trip_distance\", f.col(\"trip_distance\").cast(t.FloatType()))\n",
    "        .withColumn(\"fare_amount\", f.col(\"fare_amount\").cast(t.FloatType()))\n",
    "        .withColumn(\"tip_amount\", f.col(\"tip_amount\").cast(t.FloatType()))\n",
    "        .withColumn(\"PULocationID\", f.col(\"PULocationID\").cast(t.IntegerType()))\n",
    "        .withColumn(\"DOLocationID\", f.col(\"DOLocationID\").cast(t.IntegerType())))\n",
    " \n",
    "\n",
    "def transform_fhv(df):\n",
    "    return (df.withColumn(\"pickup_datetime\", f.col(\"pickup_datetime\").cast(t.TimestampType()))\n",
    "        .withColumn(\"dropoff_datetime\", f.col(\"dropoff_datetime\").cast(t.TimestampType()))\n",
    "        .withColumn(\"PULocationID\", f.col(\"PULocationID\").cast(t.IntegerType()))\n",
    "        .withColumn(\"DOLocationID\", f.col(\"DOLocationID\").cast(t.IntegerType())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the ability to read all the taxi data into the same dataset, how will we be able to tell where the original data came from? The file name provides information including:\n",
    "* Service type (yellow, green, etc)\n",
    "* File date\n",
    "\n",
    "We want to augment the taxi data with this information so we can refer back to it in analysis.\n",
    "\n",
    "Is there other data we might want to augment the raw data with? Some things to consider:\n",
    "* Additional fields that could help with analysis\n",
    "* Metadata, like when the record was last updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40cd031fcd24bd398c9e9259e8ecedb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using matched groups, we can extract information from the taxi file names\n",
    "TAXI_DATA_PATTERN = \"(?P<service>[a-zA-Z0-9]+)_tripdata_(?P<year>[0-9]{4})-(?P<month>[0-9]{2}).csv\"\n",
    "\n",
    "def extract_file_info(file_name):\n",
    "    m = re.match(TAXI_DATA_PATTERN, file_name)\n",
    "    if m is not None:\n",
    "        return (m.group(1), m.group(2), m.group(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5b27fa62144e2988311584f6cc9de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ingest_taxi_data_multi_service(file_name, ingested_on):\n",
    "    print(f\"Processing {file_name}\")\n",
    "    (service, year, month) = extract_file_info(Path(file_name).name)\n",
    "    input_df = spark.read.option('header', True).csv(file_name)\n",
    "    \n",
    "    if service == 'yellow':\n",
    "        df_transform = transform_yellow_taxi(input_df)\n",
    "    elif service == 'green':\n",
    "        df_transform = transform_green_taxi(input_df)\n",
    "    else:\n",
    "        # FHV. What happens if there are more taxi services added?\n",
    "        df_transform = transform_fhv(input_df)\n",
    "\n",
    "    (df_transform\n",
    "         .withColumn(\"service\", f.lit(service))\n",
    "         .withColumn(\"year\", f.lit(year))\n",
    "         .withColumn(\"month\", f.lit(month))\n",
    "         .withColumn(\"ingested_on\", f.lit(ingest_timestamp))\n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a8797a40464a3690762d57be8a6bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json'\n",
      "b''"
     ]
    }
   ],
   "source": [
    "shell_cmd(\"hdfs dfs -rm -r hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154eed8580744279b87d877436d6722c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing s3://nyc-tlc/trip data/yellow_tripdata_2020-01.csv"
     ]
    }
   ],
   "source": [
    "ingest_timestamp = datetime.strftime(datetime.now(), \"%Y-%m-%d %H:%M:%S%z\")\n",
    "taxi_data_prefix = \"s3://nyc-tlc/trip data/\"\n",
    "taxi_data_files = [\"yellow_tripdata_2020-01.csv\", \"green_tripdata_2020-01.csv\", \"fhv_tripdata_2020-01.csv\", \"fhvhv_tripdata_2020-01.csv\"]\n",
    "for file_name in taxi_data_files: \n",
    "    taxi_data_path = f\"{taxi_data_prefix}{file_name}\"\n",
    "    ingest_taxi_data_multi_service(taxi_data_path, ingest_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ecf80a760c47bbb3702d73efe2ee89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_taxi_output = spark.read.json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a14e1e96ab4cb4b8ffbfee3eb72454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|service|  count|\n",
      "+-------+-------+\n",
      "| yellow|6405008|\n",
      "+-------+-------+"
     ]
    }
   ],
   "source": [
    "df_taxi_output.groupby(\"service\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling bad data\n",
    "How to design for the inevitability of bad data  \n",
    "Reference: https://blog.knoldus.com/apache-spark-handle-corrupt-bad-records/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbaec4e88b804afc8cfad6bc63e99084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bad_data = [\n",
    "    \"{'pickup_datetime': '2020-05-23 21:05:23', 'fare_amount': '0.05'}\",\n",
    "    \"{'pickup_datetime': '2020-05-23 08:05:23', 'fare_amount': '10.05'}\",\n",
    "    \"{'pickup_datetime': '2020-05-23 21:05:23', 'fare_amount}\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa6f3d16dfe484eb06c2b9e5d08b27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-------------------+\n",
      "|     _corrupt_record|fare_amount|    pickup_datetime|\n",
      "+--------------------+-----------+-------------------+\n",
      "|                null|       0.05|2020-05-23 21:05:23|\n",
      "|                null|      10.05|2020-05-23 08:05:23|\n",
      "|{'pickup_datetime...|       null|               null|\n",
      "+--------------------+-----------+-------------------+"
     ]
    }
   ],
   "source": [
    "corrupt_df = spark.read.json(sc.parallelize(bad_data), mode=\"PERMISSIVE\", columnNameOfCorruptRecord=\"_corrupt_record\")\n",
    "corrupt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ea22048a2d4cf7a09b4bac20911d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|fare_amount|    pickup_datetime|\n",
      "+-----------+-------------------+\n",
      "|       0.05|2020-05-23 21:05:23|\n",
      "|      10.05|2020-05-23 08:05:23|\n",
      "+-----------+-------------------+"
     ]
    }
   ],
   "source": [
    "corrupt_df = spark.read.json(sc.parallelize(bad_data), mode=\"DROPMALFORMED\", columnNameOfCorruptRecord=\"_corrupt_record\")\n",
    "corrupt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ba8eb63d27440aa85c68d9d0c2b868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o900.json.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 69.0 failed 4 times, most recent failure: Lost task 15.3 in stage 69.0 (TID 903, ip-172-31-8-43.us-east-2.compute.internal, executor 6): org.apache.spark.SparkException: Malformed records are detected in schema inference. Parse Mode: FAILFAST.\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:66)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:53)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n",
      "\tat scala.collection.Iterator$class.isEmpty(Iterator.scala:331)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceLeftOption(TraversableOnce.scala:203)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceOption(TraversableOnce.scala:210)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1334)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:70)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:50)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: com.fasterxml.jackson.core.JsonParseException: Unexpected end-of-input in field name\n",
      " at [Source: [B@50fb5101; line: 1, column: 113]\n",
      "\tat com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1581)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:533)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:470)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseAposName(UTF8StreamJsonParser.java:2153)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleOddName(UTF8StreamJsonParser.java:1998)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseName(UTF8StreamJsonParser.java:1650)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:740)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonUtils$.nextUntil(JacksonUtils.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.inferField(JsonInferSchema.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:57)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:55)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2619)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:55)\n",
      "\t... 24 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.infer(JsonInferSchema.scala:83)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$inferFromDataset$1.apply(JsonDataSource.scala:109)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$inferFromDataset$1.apply(JsonDataSource.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:108)\n",
      "\tat org.apache.spark.sql.DataFrameReader$$anonfun$2.apply(DataFrameReader.scala:440)\n",
      "\tat org.apache.spark.sql.DataFrameReader$$anonfun$2.apply(DataFrameReader.scala:440)\n",
      "\tat scala.Option.getOrElse(Option.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:439)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:420)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:406)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Malformed records are detected in schema inference. Parse Mode: FAILFAST.\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:66)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:53)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n",
      "\tat scala.collection.Iterator$class.isEmpty(Iterator.scala:331)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceLeftOption(TraversableOnce.scala:203)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceOption(TraversableOnce.scala:210)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1334)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:70)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:50)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: com.fasterxml.jackson.core.JsonParseException: Unexpected end-of-input in field name\n",
      " at [Source: UNKNOWN; line: 1, column: 113]\n",
      "\tat com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1581)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:533)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:470)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseAposName(UTF8StreamJsonParser.java:2153)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleOddName(UTF8StreamJsonParser.java:1998)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseName(UTF8StreamJsonParser.java:1650)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:740)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonUtils$.nextUntil(JacksonUtils.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.inferField(JsonInferSchema.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:57)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:55)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2619)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:55)\n",
      "\t... 24 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 286, in json\n",
      "    return self._df(self._jreader.json(jrdd))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o900.json.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 69.0 failed 4 times, most recent failure: Lost task 15.3 in stage 69.0 (TID 903, ip-172-31-8-43.us-east-2.compute.internal, executor 6): org.apache.spark.SparkException: Malformed records are detected in schema inference. Parse Mode: FAILFAST.\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:66)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:53)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n",
      "\tat scala.collection.Iterator$class.isEmpty(Iterator.scala:331)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceLeftOption(TraversableOnce.scala:203)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceOption(TraversableOnce.scala:210)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1334)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:70)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:50)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: com.fasterxml.jackson.core.JsonParseException: Unexpected end-of-input in field name\n",
      " at [Source: [B@50fb5101; line: 1, column: 113]\n",
      "\tat com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1581)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:533)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:470)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseAposName(UTF8StreamJsonParser.java:2153)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleOddName(UTF8StreamJsonParser.java:1998)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseName(UTF8StreamJsonParser.java:1650)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:740)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonUtils$.nextUntil(JacksonUtils.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.inferField(JsonInferSchema.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:57)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:55)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2619)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:55)\n",
      "\t... 24 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.infer(JsonInferSchema.scala:83)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$inferFromDataset$1.apply(JsonDataSource.scala:109)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$inferFromDataset$1.apply(JsonDataSource.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:108)\n",
      "\tat org.apache.spark.sql.DataFrameReader$$anonfun$2.apply(DataFrameReader.scala:440)\n",
      "\tat org.apache.spark.sql.DataFrameReader$$anonfun$2.apply(DataFrameReader.scala:440)\n",
      "\tat scala.Option.getOrElse(Option.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:439)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:420)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:406)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Malformed records are detected in schema inference. Parse Mode: FAILFAST.\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:66)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:53)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n",
      "\tat scala.collection.Iterator$class.isEmpty(Iterator.scala:331)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceLeftOption(TraversableOnce.scala:203)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceOption(TraversableOnce.scala:210)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1334)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:70)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:50)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: com.fasterxml.jackson.core.JsonParseException: Unexpected end-of-input in field name\n",
      " at [Source: UNKNOWN; line: 1, column: 113]\n",
      "\tat com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1581)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:533)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:470)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseAposName(UTF8StreamJsonParser.java:2153)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleOddName(UTF8StreamJsonParser.java:1998)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseName(UTF8StreamJsonParser.java:1650)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:740)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonUtils$.nextUntil(JacksonUtils.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.inferField(JsonInferSchema.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:57)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:55)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2619)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:55)\n",
      "\t... 24 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corrupt_df = spark.read.json(sc.parallelize(bad_data), mode=\"FAILFAST\", columnNameOfCorruptRecord=\"_corrupt_record\")\n",
    "corrupt_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.4 - Write an ingestion for the taxi zone lookup\n",
    "File location - Yes, there is a space between taxi and the '_'  \n",
    "\n",
    "s3://nyc-tlc/misc/taxi _zone_lookup.csv\n",
    "\n",
    "`def ingest_taxi_lookup():`\n",
    "1. Read taxi data\n",
    "1. Cast to correct data types\n",
    "1. Save to hdfs:///tmp/data/nyc-taxi/zone-lookup/output/section2/json\n",
    "\n",
    "Refer back to Taxi Data page for more info: https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec85612451a4df3a04397a6ed0cbb85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def taxi_zone_transform(df):\n",
    "    return (df.withColumn(\"LocationID\", f.col(\"LocationID\").cast(t.IntegerType()))\n",
    "            .withColumn(\"Borough\", f.col(\"Borough\").cast(t.StringType()))\n",
    "            .withColumn(\"Zone\", f.col(\"Zone\").cast(t.StringType()))\n",
    "            .withColumn(\"service_zone\", f.col(\"service_zone\").cast(t.StringType())))\n",
    "\n",
    "def ingest_taxi_lookup(ingest_timestamp):\n",
    "    (spark.read\n",
    "    .option(\"header\", True)\n",
    "    .csv(\"s3://nyc-tlc/misc/taxi _zone_lookup.csv\")\n",
    "    .transform(taxi_zone_transform)\n",
    "    .withColumn(\"ingested_on\", f.lit(ingest_timestamp))\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .json(\"hdfs:///tmp/data/nyc-taxi/zone-lookup/output/section2/json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6626cb0e274fed9b575ba2d69f6f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9028045780000866, None)"
     ]
    }
   ],
   "source": [
    "ingest_timestamp = datetime.strftime(datetime.now(), \"%Y-%m-%d %H:%M:%S%z\")\n",
    "print(timer_method(\"ingest_taxi_lookup(ingest_timestamp)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather data ingestion\n",
    "\n",
    "NOAA GHCND dataset  \n",
    "https://docs.opendata.aws/noaa-ghcn-pds/readme.html  \n",
    "\n",
    "Scroll down to 'FORMAT OF “ghcnd-stations.txt” file' for the schema of the fixed-width stations data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ff9d478d144170b73b09db632459f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ghcnd_stations_path = \"https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt\"\n",
    "stations_s3 = f\"s3://{MY_BUCKET_NAME}/data/ghcnd/stations/input/ghcnd_stations.txt\"\n",
    "stations_local = \"hdfs:///tmp/data/ghcnd/stations/input/ghcnd-stations.txt\"\n",
    "stations_output = \"hdfs:///tmp/data/ghcnd/stations/output/ghcnd-stations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8e06a65727417c86e468d57d61654e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Workaround reading HTTPS -> HDFS, HTTPS -> S3 -> HDFS\n",
    "# Spark cant read data directly from HTTP, so copy the file to S3 and read into a dataframe from there\n",
    "# Then save the file to HDFS for further processing\n",
    "ingest_timestamp = datetime.strftime(datetime.now(), \"%Y-%m-%d %H:%M:%S%z\")\n",
    "resp = requests.get(ghcnd_stations_path)\n",
    "if resp.status_code != 200:\n",
    "    print(\"Couldn't get station data\")\n",
    "else:\n",
    "    s3 = boto3.client('s3')\n",
    "    res = s3.put_object(Body=resp.content, Bucket=MY_BUCKET_NAME, Key=f\"data/ghcnd/stations/input/ghcnd_stations.txt\")\n",
    "    if res['ResponseMetadata']['HTTPStatusCode'] != 200:\n",
    "        print(f\"Unable to create ghcnd_stations.txt in s3, response {res['ResponseMetadata']['HTTPStatusCode']}\")\n",
    "    else:\n",
    "        (spark\n",
    "         .read\n",
    "         .text(stations_s3)\n",
    "         .write\n",
    "         .format(\"text\")\n",
    "         .mode(\"overwrite\")\n",
    "         .save(stations_local))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33aa825201d481aa793e9ed65536878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------+\n",
      "|value                                                                                |\n",
      "+-------------------------------------------------------------------------------------+\n",
      "|ACW00011604  17.1167  -61.7833   10.1    ST JOHNS COOLIDGE FLD                       |\n",
      "|ACW00011647  17.1333  -61.7833   19.2    ST JOHNS                                    |\n",
      "|AE000041196  25.3330   55.5170   34.0    SHARJAH INTER. AIRP            GSN     41196|\n",
      "|AEM00041194  25.2550   55.3640   10.4    DUBAI INTL                             41194|\n",
      "|AEM00041217  24.4330   54.6510   26.8    ABU DHABI INTL                         41217|\n",
      "|AEM00041218  24.2620   55.6090  264.9    AL AIN INTL                            41218|\n",
      "|AF000040930  35.3170   69.0170 3366.0    NORTH-SALANG                   GSN     40930|\n",
      "|AFM00040938  34.2100   62.2280  977.2    HERAT                                  40938|\n",
      "|AFM00040948  34.5660   69.2120 1791.3    KABUL INTL                             40948|\n",
      "|AFM00040990  31.5000   65.8500 1010.0    KANDAHAR AIRPORT                       40990|\n",
      "+-------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "# Take a look at the stations file we just saved to HDFS\n",
    "stations = spark.read.text(stations_local)\n",
    "stations.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20caaa972c41478786793ce2b2f03edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|id         |\n",
      "+-----------+\n",
      "|ACW00011604|\n",
      "|ACW00011647|\n",
      "|AE000041196|\n",
      "|AEM00041194|\n",
      "|AEM00041217|\n",
      "|AEM00041218|\n",
      "|AF000040930|\n",
      "|AFM00040938|\n",
      "|AFM00040948|\n",
      "|AFM00040990|\n",
      "+-----------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "# Example of doing a substring transformation\n",
    "(stations\n",
    "    .withColumn(\"id\", f.col(\"value\").substr(0, 11))\n",
    "    .drop(\"value\")\n",
    ").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.5 - Build the ingestion for the weather stations data\n",
    "\n",
    "Reference the fixed width schema provided under **FORMAT OF “ghcnd-stations.txt” file**   \n",
    "https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt  \n",
    "#### Important note - FORMAT OF is incorrect for: elevation - should be 7 char not 6\n",
    "\n",
    "Create transformation code to convert the value column into the following schema:\n",
    "* id String\n",
    "* lat Float\n",
    "* long Float\n",
    "* elevation Float\n",
    "* state String\n",
    "* name String\n",
    "\n",
    "Drop the value column, save the data in JSON format to s3://data-scale-oreilly/data/ghcnd/stations/output/section2_json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9022d1ffc2b4cb495bb9451b843f750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform_stations_ps(stations_local):\n",
    "    return (spark.read.text(stations_local)\n",
    "     .withColumn(\"id\", f.col(\"value\").substr(0, 11))\n",
    "     .withColumn(\"lat\", f.col(\"value\").substr(13, 8))\n",
    "     .withColumn(\"long\", f.col(\"value\").substr(22, 9))\n",
    "     .withColumn(\"elevation\", f.col(\"value\").substr(32, 7))\n",
    "     .withColumn(\"state\", f.col(\"value\").substr(39, 2))\n",
    "     .withColumn(\"name\", f.col(\"value\").substr(42, 30))\n",
    "     .drop(\"value\")\n",
    "    )\n",
    "\n",
    "def transform_stations_pd(stations_s3):\n",
    "    return pd.read_fwf(stations_s3, \n",
    "                     [(0,10), (13, 20), (22, 30), (32, 38), (39, 40), (41, 71)],\n",
    "                    names=[\"id\", \"lat\", \"long\", \"elevation\", \"state\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb2c2c15af5434d94810feb66a731c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ps_run_time, stations_df = timer_method(\"transform_stations_ps(stations_local)\")\n",
    "pd_run_time, pd_stations_df = timer_method(\"transform_stations_pd(stations_s3)\")\n",
    "print(f\"pyspark runtime: {ps_run_time} pandas runtime {pd_run_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A key aspect of designing scalable systems is to be judicious about the data being stored and processed. \n",
    "#### The GHCND stations file contains data on stations across the US, but we are only interested in data near NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94280305ae0b475bb935292cf31282e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115084"
     ]
    }
   ],
   "source": [
    "stations_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e0e9b6c8bc4468bd72735a57c68cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets look at some performance tradeoffs between pyspark and pandas\n",
    "# The .toPandas() function in pyspark can be convenient if you are familiar with pandas manipulations\n",
    "# but this can quickly become very expensive as it collects all data on the driver to do the conversion.\n",
    "\n",
    "def filter_ny_stations_pandas(stations_df):\n",
    "    ny_stations = stations_df.filter(\"state == 'NY'\")\n",
    "\n",
    "    # filter down to just stations in NY in NYC. Lat of south Yonkers ~40.9124\n",
    "    ny_pandas = ny_stations.toPandas()\n",
    "    ny_pandas[ny_pandas.columns] = ny_pandas.apply(lambda x: x.str.strip())\n",
    "    nyc_stations = ny_pandas[ny_pandas['lat'].apply(lambda x: float(x)) < 40.9124]\n",
    "    res = spark.createDataFrame(nyc_stations)\n",
    "    res.count()\n",
    "    return res\n",
    "\n",
    "def filter_ny_stations_pyspark(stations_df):\n",
    "    print(\"Filtering stations to NY only\")\n",
    "    ny_stations = stations_df.filter(\"state == 'NY'\")\n",
    "    res = (ny_stations\n",
    "            .withColumn(\"lat\", f.col(\"lat\").cast(t.FloatType()))\n",
    "            .filter(\"lat < 40.9124\"))\n",
    "    res.count()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97385db59f9490f8426b7482d4a1521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime 15.594902415999968"
     ]
    }
   ],
   "source": [
    "pd_run_time, pd_ny_stations = timer_method(\"filter_ny_stations_pandas(stations_df)\")\n",
    "print(f\"runtime {pd_run_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b7b292cdef4fdb803c5ca803fe8e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering stations to NY only\n",
      "runtime 0.3877436080001644"
     ]
    }
   ],
   "source": [
    "ps_run_time, ps_ny_stations = timer_method(\"filter_ny_stations_pyspark(stations_df)\")\n",
    "print(f\"runtime {ps_run_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Lab 2.6 - Write two ingest functions for the stations data, one using filter_ny_stations_pandas and the other using filter_ny_stations_pyspark. What do you notice about the differences?\n",
    "\n",
    "The functions should:\n",
    "* Read the station data from local or s3\n",
    "* Transform the station data into columns from the fixed width format\n",
    "* Use the above filter functions\n",
    "* Write the output to stations_output as json in overwrite mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "458b5f240f044af4972eb28ef092fa8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform_station(stations):\n",
    "    return (stations\n",
    "     .withColumn(\"id\", f.col(\"value\").substr(0, 11))\n",
    "     .withColumn(\"lat\", f.col(\"value\").substr(13, 8))\n",
    "     .withColumn(\"long\", f.col(\"value\").substr(22, 9))\n",
    "     .withColumn(\"elevation\", f.col(\"value\").substr(32, 7))\n",
    "     .withColumn(\"state\", f.col(\"value\").substr(39, 2))\n",
    "     .withColumn(\"name\", f.col(\"value\").substr(42, 30))\n",
    "     .drop(\"value\")\n",
    "    )\n",
    "\n",
    "def ingest_station_pandas():\n",
    "    stations = spark.read.text(stations_local)\n",
    "    (stations.transform(transform_station)\n",
    "     .transform(filter_ny_stations_pandas)\n",
    "     .coalesce(1)\n",
    "     .write\n",
    "     .mode(\"overwrite\")\n",
    "     .json(stations_output)\n",
    "    )\n",
    "    \n",
    "def ingest_station_pyspark():\n",
    "    stations = spark.read.text(stations_s3)\n",
    "    (stations.transform(transform_station)\n",
    "     .transform(filter_ny_stations_pyspark)\n",
    "     .coalesce(1)\n",
    "     .write\n",
    "     .mode(\"overwrite\")\n",
    "     .json(stations_output)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0b208734124957a78d64c6948b35cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#shell_cmd(\"hdfs dfs -rm -r hdfs:///tmp/data/test_pyspark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac64ff934a14bfebc7830cef765e2df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering stations to NY only\n",
      "pandas: (0.9616486069999155, None) pyspark: (2.0233572219999587, None)"
     ]
    }
   ],
   "source": [
    "ps_result = timer_method(\"ingest_station_pyspark()\")\n",
    "pd_result = timer_method(\"ingest_station_pandas()\")\n",
    "print(f\"pandas: {pd_result} pyspark: {ps_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.7 - Case Study 1: Month over month, get the total count of of pickups per borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2b0b4ed8254200a05741cfa4d71e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "taxiPath = \"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/\"\n",
    "taxiLookupPath = \"hdfs:///tmp/data/nyc-taxi/zone-lookup/output/section2/json/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8e78cbc56a4df4aa9a4ebd6664e7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'Detected implicit cartesian product for INNER join between logical plans\\nJoin Inner, ((LocationID#3838L = PULocationID#3774L) && (LocationID#3838L = DOLocationID#3773L))\\n:- Filter ((isnotnull(pickup_datetime#3791) && isnotnull(dropoff_datetime#3780)) && (((DOLocationID#3773L = PULocationID#3774L) && isnotnull(PULocationID#3774L)) && isnotnull(DOLocationID#3773L)))\\n:  +- Relation[DOLocationID#3773L,PULocationID#3774L,RatecodeID#3775,SR_Flag#3776,VendorID#3777,congestion_surcharge#3778,dispatching_base_num#3779,dropoff_datetime#3780,extra#3781,fare_amount#3782,hvfhs_license_num#3783,improvement_surcharge#3784,lpep_dropoff_datetime#3785,lpep_pickup_datetime#3786,month#3787,mta_tax#3788,passenger_count#3789L,payment_type#3790,pickup_datetime#3791,service#3792,store_and_fwd_flag#3793,tip_amount#3794,tolls_amount#3795,total_amount#3796,... 5 more fields] json\\n+- Project [LocationID#3838L, Borough#3837 AS PUBorough#3847]\\n   +- Filter isnotnull(LocationID#3838L)\\n      +- Relation[Borough#3837,LocationID#3838L,Zone#3839,service_zone#3840] json\\nand\\nProject [LocationID#3949L, Borough#3948 AS DOBorough#3945]\\n+- Relation[Borough#3948,LocationID#3949L,Zone#3950,service_zone#3951] json\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 382, in show\n",
      "    print(self._jdf.showString(n, 20, vertical))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: 'Detected implicit cartesian product for INNER join between logical plans\\nJoin Inner, ((LocationID#3838L = PULocationID#3774L) && (LocationID#3838L = DOLocationID#3773L))\\n:- Filter ((isnotnull(pickup_datetime#3791) && isnotnull(dropoff_datetime#3780)) && (((DOLocationID#3773L = PULocationID#3774L) && isnotnull(PULocationID#3774L)) && isnotnull(DOLocationID#3773L)))\\n:  +- Relation[DOLocationID#3773L,PULocationID#3774L,RatecodeID#3775,SR_Flag#3776,VendorID#3777,congestion_surcharge#3778,dispatching_base_num#3779,dropoff_datetime#3780,extra#3781,fare_amount#3782,hvfhs_license_num#3783,improvement_surcharge#3784,lpep_dropoff_datetime#3785,lpep_pickup_datetime#3786,month#3787,mta_tax#3788,passenger_count#3789L,payment_type#3790,pickup_datetime#3791,service#3792,store_and_fwd_flag#3793,tip_amount#3794,tolls_amount#3795,total_amount#3796,... 5 more fields] json\\n+- Project [LocationID#3838L, Borough#3837 AS PUBorough#3847]\\n   +- Filter isnotnull(LocationID#3838L)\\n      +- Relation[Borough#3837,LocationID#3838L,Zone#3839,service_zone#3840] json\\nand\\nProject [LocationID#3949L, Borough#3948 AS DOBorough#3945]\\n+- Relation[Borough#3948,LocationID#3949L,Zone#3950,service_zone#3951] json\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join boroughs\n",
    "# Expected error cartesian join. most likely a carryover bug from 2.0\n",
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", \"false\") #<-- default\n",
    "df_taxi = spark.read.json(taxiPath)\n",
    "df_taxi_lookup = spark.read.json(taxiLookupPath)\n",
    "\n",
    "taxi_filtered = (df_taxi\n",
    " .filter(df_taxi.pickup_datetime.isNotNull())\n",
    " .filter(df_taxi.dropoff_datetime.isNotNull()))\n",
    "taxi_pu = (taxi_filtered\n",
    ".join(df_taxi_lookup\n",
    "       .select(\"LocationID\", \"Borough\")\n",
    "       .withColumnRenamed(\"Borough\", \"PUBorough\"), \n",
    "       df_taxi_lookup.LocationID == df_taxi.PULocationID))\n",
    "taxi = (taxi_pu.join(df_taxi_lookup\n",
    "       .select(\"LocationID\", \"Borough\")\n",
    "       .withColumnRenamed(\"Borough\", \"DOBorough\"), \n",
    "       df_taxi_lookup.LocationID == taxi_pu.DOLocationID))\n",
    "taxi_pu.show()\n",
    "taxi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc3f88039ea4707803caf7c1f01001e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan(isFinalPlan=false)\n",
      "+- BroadcastHashJoin [PULocationID#806L], [LocationID#860L], Inner, BuildRight\n",
      "   :- Project [DOLocationID#805L, PULocationID#806L, RatecodeID#807, VendorID#808, congestion_surcharge#809, dropoff_datetime#810, extra#811, fare_amount#812, improvement_surcharge#813, ingested_on#814, month#815, mta_tax#816, passenger_count#817L, payment_type#818, pickup_datetime#819, service#820, store_and_fwd_flag#821, tip_amount#822, tolls_amount#823, total_amount#824, tpep_dropoff_datetime#825, tpep_pickup_datetime#826, trip_distance#827, year#828]\n",
      "   :  +- Filter ((isnotnull(pickup_datetime#819) && isnotnull(dropoff_datetime#810)) && isnotnull(PULocationID#806L))\n",
      "   :     +- FileScan json [DOLocationID#805L,PULocationID#806L,RatecodeID#807,VendorID#808,congestion_surcharge#809,dropoff_datetime#810,extra#811,fare_amount#812,improvement_surcharge#813,ingested_on#814,month#815,mta_tax#816,passenger_count#817L,payment_type#818,pickup_datetime#819,service#820,store_and_fwd_flag#821,tip_amount#822,tolls_amount#823,total_amount#824,tpep_dropoff_datetime#825,tpep_pickup_datetime#826,trip_distance#827,year#828] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://ip-172-31-14-119.ec2.internal:8020/tmp/data/nyc-taxi/taxi-data/output/se..., PartitionFilters: [], PushedFilters: [IsNotNull(pickup_datetime), IsNotNull(dropoff_datetime), IsNotNull(PULocationID)], ReadSchema: struct<DOLocationID:bigint,PULocationID:bigint,RatecodeID:string,VendorID:string,congestion_surch...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]))\n",
      "      +- Project [LocationID#860L, Borough#859 AS PUBorough#871]\n",
      "         +- Filter isnotnull(LocationID#860L)\n",
      "            +- FileScan json [Borough#859,LocationID#860L] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://ip-172-31-14-119.ec2.internal:8020/tmp/data/nyc-taxi/zone-lookup/output/..., PartitionFilters: [], PushedFilters: [IsNotNull(LocationID)], ReadSchema: struct<Borough:string,LocationID:bigint>\n",
      "+------------+------------+----------+--------+--------------------+--------------------+-----+-----------+---------------------+-------------------+-----+-------+---------------+------------+--------------------+-------+------------------+----------+------------+------------+---------------------+--------------------+-------------+----+----------+---------+\n",
      "|DOLocationID|PULocationID|RatecodeID|VendorID|congestion_surcharge|    dropoff_datetime|extra|fare_amount|improvement_surcharge|        ingested_on|month|mta_tax|passenger_count|payment_type|     pickup_datetime|service|store_and_fwd_flag|tip_amount|tolls_amount|total_amount|tpep_dropoff_datetime|tpep_pickup_datetime|trip_distance|year|LocationID|PUBorough|\n",
      "+------------+------------+----------+--------+--------------------+--------------------+-----+-----------+---------------------+-------------------+-----+-------+---------------+------------+--------------------+-------+------------------+----------+------------+------------+---------------------+--------------------+-------------+----+----------+---------+\n",
      "|         239|         238|         1|       1|                 2.5|2020-01-01T00:33:...|    3|        6.0|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           1|2020-01-01T00:28:...| yellow|                 N|      1.47|           0|       11.27|  2020-01-01 00:33:03| 2020-01-01 00:28:15|          1.2|2020|       238|Manhattan|\n",
      "|         238|         239|         1|       1|                 2.5|2020-01-01T00:43:...|    3|        7.0|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           1|2020-01-01T00:35:...| yellow|                 N|       1.5|           0|        12.3|  2020-01-01 00:43:04| 2020-01-01 00:35:39|          1.2|2020|       239|Manhattan|\n",
      "|         238|         238|         1|       1|                 2.5|2020-01-01T00:53:...|    3|        6.0|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           1|2020-01-01T00:47:...| yellow|                 N|       1.0|           0|        10.8|  2020-01-01 00:53:52| 2020-01-01 00:47:41|          0.6|2020|       238|Manhattan|\n",
      "|         151|         238|         1|       1|                   0|2020-01-01T01:00:...|  0.5|        5.5|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           1|2020-01-01T00:55:...| yellow|                 N|      1.36|           0|        8.16|  2020-01-01 01:00:14| 2020-01-01 00:55:23|          0.8|2020|       238|Manhattan|\n",
      "|         193|         193|         1|       2|                   0|2020-01-01T00:04:...|  0.5|        3.5|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           2|2020-01-01T00:01:...| yellow|                 N|       0.0|           0|         4.8|  2020-01-01 00:04:16| 2020-01-01 00:01:58|          0.0|2020|       193|   Queens|\n",
      "|         193|           7|         1|       2|                   0|2020-01-01T00:10:...|  0.5|        2.5|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           2|2020-01-01T00:09:...| yellow|                 N|       0.0|           0|         3.8|  2020-01-01 00:10:37| 2020-01-01 00:09:44|         0.03|2020|         7|   Queens|\n",
      "|         193|         193|         1|       2|                   0|2020-01-01T00:39:...|  0.5|        2.5|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           1|2020-01-01T00:39:...| yellow|                 N|      0.01|           0|        3.81|  2020-01-01 00:39:29| 2020-01-01 00:39:25|          0.0|2020|       193|   Queens|\n",
      "|         193|         193|         5|       2|                 2.5|2019-12-18T15:28:...|    0|       0.01|                  0.3|2020-10-23 03:48:16|   01|      0|              1|           1|2019-12-18T15:27:...| yellow|                 N|       0.0|           0|        2.81|  2019-12-18 15:28:59| 2019-12-18 15:27:49|          0.0|2020|       193|   Queens|\n",
      "|         193|         193|         1|       2|                 2.5|2019-12-18T15:31:...|  0.5|        2.5|                  0.3|2020-10-23 03:48:16|   01|    0.5|              4|           1|2019-12-18T15:30:...| yellow|                 N|       0.0|           0|         6.3|  2019-12-18 15:31:35| 2019-12-18 15:30:35|          0.0|2020|       193|   Queens|\n",
      "|          48|         246|         1|       1|                 2.5|2020-01-01T00:40:...|    3|        8.0|                  0.3|2020-10-23 03:48:16|   01|    0.5|              2|           1|2020-01-01T00:29:...| yellow|                 N|      2.35|           0|       14.15|  2020-01-01 00:40:28| 2020-01-01 00:29:01|          0.7|2020|       246|Manhattan|\n",
      "|          79|         246|         1|       1|                 2.5|2020-01-01T01:12:...|    3|       12.0|                  0.3|2020-10-23 03:48:16|   01|    0.5|              2|           1|2020-01-01T00:55:...| yellow|                 N|      1.75|           0|       17.55|  2020-01-01 01:12:03| 2020-01-01 00:55:11|          2.4|2020|       246|Manhattan|\n",
      "|         161|         163|         1|       1|                 2.5|2020-01-01T00:51:...|    3|        9.5|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           2|2020-01-01T00:37:...| yellow|                 N|       0.0|           0|        13.3|  2020-01-01 00:51:41| 2020-01-01 00:37:15|          0.8|2020|       163|Manhattan|\n",
      "|         144|         161|         1|       1|                 2.5|2020-01-01T01:21:...|    3|       17.0|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           1|2020-01-01T00:56:...| yellow|                 N|      4.15|           0|       24.95|  2020-01-01 01:21:44| 2020-01-01 00:56:27|          3.3|2020|       161|Manhattan|\n",
      "|         239|          43|         1|       2|                 2.5|2020-01-01T00:27:...|  0.5|        6.0|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           1|2020-01-01T00:21:...| yellow|                 N|      1.96|           0|       11.76|  2020-01-01 00:27:31| 2020-01-01 00:21:54|         1.07|2020|        43|Manhattan|\n",
      "|          25|         143|         1|       2|                 2.5|2020-01-01T01:15:...|  0.5|       28.5|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           1|2020-01-01T00:38:...| yellow|                 N|      4.84|           0|       37.14|  2020-01-01 01:15:21| 2020-01-01 00:38:01|         7.76|2020|       143|Manhattan|\n",
      "|         234|         211|         1|       1|                 2.5|2020-01-01T00:27:...|    3|        9.0|                  0.3|2020-10-23 03:48:16|   01|    0.5|              3|           2|2020-01-01T00:15:...| yellow|                 N|       0.0|           0|        12.8|  2020-01-01 00:27:06| 2020-01-01 00:15:35|          1.6|2020|       211|Manhattan|\n",
      "|          90|         234|         1|       1|                 2.5|2020-01-01T00:44:...|    3|        4.0|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           1|2020-01-01T00:41:...| yellow|                 Y|       1.0|           0|         8.8|  2020-01-01 00:44:22| 2020-01-01 00:41:20|          0.5|2020|       234|Manhattan|\n",
      "|         142|         246|         1|       1|                 2.5|2020-01-01T01:13:...|    3|       11.5|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           2|2020-01-01T00:56:...| yellow|                 N|       0.0|           0|        15.3|  2020-01-01 01:13:34| 2020-01-01 00:56:38|          1.7|2020|       246|Manhattan|\n",
      "|         216|         138|         1|       2|                   0|2020-01-01T00:25:...|  0.5|       24.5|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           2|2020-01-01T00:08:...| yellow|                 N|       0.0|           0|        25.8|  2020-01-01 00:25:29| 2020-01-01 00:08:21|         8.45|2020|       138|   Queens|\n",
      "|         162|         170|         1|       1|                 2.5|2020-01-01T00:27:...|    3|        3.0|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           4|2020-01-01T00:25:...| yellow|                 N|       0.0|           0|         6.8|  2020-01-01 00:27:05| 2020-01-01 00:25:39|          0.0|2020|       170|Manhattan|\n",
      "+------------+------------+----------+--------+--------------------+--------------------+-----+-----------+---------------------+-------------------+-----+-------+---------------+------------+--------------------+-------+------------------+----------+------------+------------+---------------------+--------------------+-------------+----+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan(isFinalPlan=false)\n",
      "+- BroadcastNestedLoopJoin BuildRight, Inner\n",
      "   :- BroadcastHashJoin [PULocationID#806L, DOLocationID#805L], [LocationID#860L, LocationID#860L], Inner, BuildRight\n",
      "   :  :- Project [DOLocationID#805L, PULocationID#806L, RatecodeID#807, VendorID#808, congestion_surcharge#809, dropoff_datetime#810, extra#811, fare_amount#812, improvement_surcharge#813, ingested_on#814, month#815, mta_tax#816, passenger_count#817L, payment_type#818, pickup_datetime#819, service#820, store_and_fwd_flag#821, tip_amount#822, tolls_amount#823, total_amount#824, tpep_dropoff_datetime#825, tpep_pickup_datetime#826, trip_distance#827, year#828]\n",
      "   :  :  +- Filter ((((isnotnull(pickup_datetime#819) && isnotnull(dropoff_datetime#810)) && isnotnull(DOLocationID#805L)) && (DOLocationID#805L = PULocationID#806L)) && isnotnull(PULocationID#806L))\n",
      "   :  :     +- FileScan json [DOLocationID#805L,PULocationID#806L,RatecodeID#807,VendorID#808,congestion_surcharge#809,dropoff_datetime#810,extra#811,fare_amount#812,improvement_surcharge#813,ingested_on#814,month#815,mta_tax#816,passenger_count#817L,payment_type#818,pickup_datetime#819,service#820,store_and_fwd_flag#821,tip_amount#822,tolls_amount#823,total_amount#824,tpep_dropoff_datetime#825,tpep_pickup_datetime#826,trip_distance#827,year#828] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://ip-172-31-14-119.ec2.internal:8020/tmp/data/nyc-taxi/taxi-data/output/se..., PartitionFilters: [], PushedFilters: [IsNotNull(pickup_datetime), IsNotNull(dropoff_datetime), IsNotNull(DOLocationID), IsNotNull(PULo..., ReadSchema: struct<DOLocationID:bigint,PULocationID:bigint,RatecodeID:string,VendorID:string,congestion_surch...\n",
      "   :  +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true], input[0, bigint, true]))\n",
      "   :     +- Project [LocationID#860L, Borough#859 AS PUBorough#871]\n",
      "   :        +- Filter isnotnull(LocationID#860L)\n",
      "   :           +- FileScan json [Borough#859,LocationID#860L] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://ip-172-31-14-119.ec2.internal:8020/tmp/data/nyc-taxi/zone-lookup/output/..., PartitionFilters: [], PushedFilters: [IsNotNull(LocationID)], ReadSchema: struct<Borough:string,LocationID:bigint>\n",
      "   +- BroadcastExchange IdentityBroadcastMode\n",
      "      +- Project [LocationID#958L, Borough#957 AS DOBorough#954]\n",
      "         +- FileScan json [Borough#957,LocationID#958L] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://ip-172-31-14-119.ec2.internal:8020/tmp/data/nyc-taxi/zone-lookup/output/..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Borough:string,LocationID:bigint>"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")\n",
    "df_taxi = spark.read.json(taxiPath)\n",
    "df_taxi_lookup = spark.read.json(taxiLookupPath)\n",
    "\n",
    "taxi_filtered = (df_taxi\n",
    " .filter(df_taxi.pickup_datetime.isNotNull())\n",
    " .filter(df_taxi.dropoff_datetime.isNotNull()))\n",
    "taxi_pu = (taxi_filtered\n",
    ".join(df_taxi_lookup\n",
    "       .select(\"LocationID\", \"Borough\")\n",
    "       .withColumnRenamed(\"Borough\", \"PUBorough\"), \n",
    "       df_taxi_lookup.LocationID == df_taxi.PULocationID))\n",
    "taxi = (taxi_pu.join(df_taxi_lookup\n",
    "       .select(\"LocationID\", \"Borough\")\n",
    "       .withColumnRenamed(\"Borough\", \"DOBorough\"), \n",
    "       df_taxi_lookup.LocationID == taxi_pu.DOLocationID))\n",
    "taxi_pu.explain()\n",
    "taxi_pu.show()\n",
    "taxi.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae4568137db47e5a07d708ea2e82ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_monthly_totals_pyspark(taxiPath, taxiLookupPath):\n",
    "    taxi = spark.read.json(taxiPath)\n",
    "    taxi_lookup = spark.read.json(taxiLookupPath)\n",
    "    taxi_filtered = (taxi\n",
    "     .filter(taxi.pickup_datetime.isNotNull())\n",
    "     .filter(taxi.dropoff_datetime.isNotNull()))\n",
    "                     \n",
    "    groupDF = taxi_filtered.join(taxi_lookup, taxi_filtered.PULocationID == taxi_lookup.LocationID)\n",
    "    groupDF.select(\"ingested_on\").show() # expected error\n",
    "    return groupDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7454c164c82f45f6a58e4c98258f5a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"Reference 'ingested_on' is ambiguous, could be: ingested_on, ingested_on.;\"\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 57, in timer_method\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 232, in timeit\n",
      "    return Timer(stmt, setup, timer, globals).timeit(number)\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 176, in timeit\n",
      "    timing = self.inner(it, self.timer)\n",
      "  File \"<timeit-src>\", line 6, in inner\n",
      "  File \"<stdin>\", line 9, in get_monthly_totals_pyspark\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 1326, in select\n",
      "    jdf = self._jdf.select(self._jcols(*cols))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: \"Reference 'ingested_on' is ambiguous, could be: ingested_on, ingested_on.;\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_monthly_totals_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df994becb1bc413bbbd404ccec696498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_monthly_totals_pandas(taxiPath, taxiLookupPath):\n",
    "    taxi = pd.read_json(taxiPath)\n",
    "    taxi_lookup = pd.read_json(taxiLookupPath)\n",
    "    taxi_filtered = tax.dropna(subset=['pickup_datetime', 'dropoff_datetime'])\n",
    "    \n",
    "    groupDF = taxi_filtered.join(taxi_lookup.set_index('LocationID'), on='PULocationID')\n",
    "    groupDF['pickup_month'] = pd.to_datetime(groupDF['pickup_datetime'], format='%m%Y')\n",
    "    groupDF = groupDF.groupby('pickup_month', 'borough').agg('count').sort_values(by=['count', 'borough'], ascending=[False, True])\n",
    "    groupDF\n",
    "    return groupDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dabf3422209740078784a7c84835b299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyarrow and local java libraries required for HDFS\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 57, in timer_method\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 232, in timeit\n",
      "    return Timer(stmt, setup, timer, globals).timeit(number)\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 176, in timeit\n",
      "    timing = self.inner(it, self.timer)\n",
      "  File \"<timeit-src>\", line 6, in inner\n",
      "  File \"<stdin>\", line 2, in get_monthly_totals_pandas\n",
      "  File \"/tmp/1603424693133-0/lib/python3.7/site-packages/pandas/util/_decorators.py\", line 199, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/tmp/1603424693133-0/lib/python3.7/site-packages/pandas/util/_decorators.py\", line 296, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/tmp/1603424693133-0/lib/python3.7/site-packages/pandas/io/json/_json.py\", line 594, in read_json\n",
      "    path_or_buf, encoding=encoding, compression=compression\n",
      "  File \"/tmp/1603424693133-0/lib/python3.7/site-packages/pandas/io/common.py\", line 222, in get_filepath_or_buffer\n",
      "    filepath_or_buffer, mode=mode or \"rb\", **(storage_options or {})\n",
      "  File \"/tmp/1603424693133-0/lib/python3.7/site-packages/fsspec/core.py\", line 438, in open\n",
      "    **kwargs\n",
      "  File \"/tmp/1603424693133-0/lib/python3.7/site-packages/fsspec/core.py\", line 287, in open_files\n",
      "    expand=expand,\n",
      "  File \"/tmp/1603424693133-0/lib/python3.7/site-packages/fsspec/core.py\", line 600, in get_fs_token_paths\n",
      "    cls = get_filesystem_class(protocol)\n",
      "  File \"/tmp/1603424693133-0/lib/python3.7/site-packages/fsspec/registry.py\", line 204, in get_filesystem_class\n",
      "    raise ImportError(bit[\"err\"]) from e\n",
      "ImportError: pyarrow and local java libraries required for HDFS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_monthly_totals_pandas(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85488e362124e9587ee74d544c9e540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_monthly_totals_pandas(taxiPath, taxiLookupPath):\n",
    "    taxiPyspark = spark.read.json(taxiPath)\n",
    "    taxiLookupPySpark = spark.read.json(taxiLookupPath)\n",
    "    \n",
    "    taxi = taxiPyspark.toPandas()\n",
    "    taxi_lookup = taxiLookupPyspark.toPandas()\n",
    "    taxi_filtered = taxi.dropna(subset=['pickup_datetime', 'dropoff_datetime'])\n",
    "    \n",
    "    groupDF = taxi_filtered.join(taxi_lookup.set_index('LocationID'), on='PULocationID')\n",
    "    groupDF['pickup_month'] = pd.to_datetime(groupDF['pickup_datetime'], format='%m%Y')\n",
    "    groupDF = groupDF.groupby('pickup_month', 'borough').agg('count').sort_values(by=['count', 'borough'], ascending=[False, True])\n",
    "    groupDF\n",
    "    return groupDF\n",
    "\n",
    "def get_monthly_totals_pyspark(taxiPath, taxiLookupPath):\n",
    "    taxi = spark.read.json(taxiPath)\n",
    "    taxi_lookup = spark.read.json(taxiLookupPath)\n",
    "    \n",
    "    taxi_filtered = (taxi\n",
    "     .filter(taxi.pickup_datetime.isNotNull())\n",
    "     .filter(taxi.dropoff_datetime.isNotNull()))\n",
    "                     \n",
    "    groupDF = taxi_filtered.join(taxi_lookup.select(\"Borough\", \"LocationID\"), taxi_filtered.PULocationID == taxi_lookup.LocationID)\n",
    "    groupDF = groupDF.withColumn(\"pickup_month\", f.date_format(\"pickup_datetime\", \"yyyyMM\"))\n",
    "    groupDF = groupDF.groupBy(\"pickup_month\", \"borough\").count().orderBy(f.desc(\"pickup_month\"), f.desc(\"count\"), \"borough\")\n",
    "    groupDF.show()\n",
    "    return groupDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timer_method(\"get_monthly_totals_pandas(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3bf9f49ba64de0b824d6cd18f71158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+--------+\n",
      "|pickup_month|      borough|   count|\n",
      "+------------+-------------+--------+\n",
      "|      202101|    Manhattan|       3|\n",
      "|      202007|    Manhattan|       6|\n",
      "|      202006|    Manhattan|       1|\n",
      "|      202005|    Manhattan|       5|\n",
      "|      202004|    Manhattan|       1|\n",
      "|      202003|    Manhattan|       5|\n",
      "|      202002|    Manhattan|      34|\n",
      "|      202002|       Queens|       9|\n",
      "|      202002|     Brooklyn|       3|\n",
      "|      202002|        Bronx|       1|\n",
      "|      202002|      Unknown|       1|\n",
      "|      202001|    Manhattan|14817800|\n",
      "|      202001|     Brooklyn| 5628274|\n",
      "|      202001|       Queens| 4509457|\n",
      "|      202001|        Bronx| 2536611|\n",
      "|      202001|      Unknown| 1615247|\n",
      "|      202001|Staten Island|  260258|\n",
      "|      202001|          EWR|    3779|\n",
      "|      201912|    Manhattan|     129|\n",
      "|      201912|       Queens|      17|\n",
      "+------------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "(46.01737120200005, DataFrame[pickup_month: string, borough: string, count: bigint])"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_monthly_totals_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fafbdcd07fa24578a6eb945e051eab70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5G"
     ]
    }
   ],
   "source": [
    "## Expected error for maxResultSize: This won't work. Could try the subsequent cells\n",
    "## Those restart the state of the notebook and don't work as expected\n",
    "## Need to restart the cluster and edit the Software config with: [{\"classification\":\"spark-defaults\", \"properties\":{\"spark.driver.maxResultSize\":\"5G\", \"spark.ui.killEnabled\":\"true\"}, \"configurations\":[]}]\n",
    "## Then need to reun the taxi and taxi lookup ingests\n",
    "## Run -> Run All Above Selected Cell\n",
    "## Second expected error for {\"msg\":\"requirement failed: Session isn't active.\"} and will hang. Driver node ran out of mem. Will need to go and upscale\n",
    "spark.conf.set(\"spark.driver.maxResultSize\", \"5G\")\n",
    "print(spark.conf.get('spark.driver.maxResultSize'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1603063175185_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-6-49.us-east-2.compute.internal:20888/proxy/application_1603063175185_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-4-197.us-east-2.compute.internal:8042/node/containerlogs/container_1603063175185_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.driver.maxResultSize': '5G'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1603063175185_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-6-49.us-east-2.compute.internal:20888/proxy/application_1603063175185_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-4-197.us-east-2.compute.internal:8042/node/containerlogs/container_1603063175185_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"conf\":{\"spark.driver.maxResultSize\":\"5G\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': 'python3', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1603063175185_0003</td><td>pyspark</td><td>dead</td><td><a target=\"_blank\" href=\"http://ip-172-31-6-49.us-east-2.compute.internal:20888/proxy/application_1603063175185_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-6-49.us-east-2.compute.internal:8188/applicationhistory/logs/ip-172-31-8-43.us-east-2.compute.internal:8041/container_1603063175185_0003_01_000001/container_1603063175185_0003_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cdd718939e944de95a5a4469bcd94d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_monthly_totals_concat_pandas(taxiPath, taxiLookupPath):\n",
    "    taxiPyspark = spark.read.json(taxiPath)\n",
    "    taxiLookupPySpark = spark.read.json(taxiLookupPath)\n",
    "    \n",
    "    taxi = taxiPyspark.toPandas()\n",
    "    taxi_lookup = taxiLookupPyspark.toPandas()\n",
    "    taxi_filtered = taxi.dropna(subset=['pickup_datetime', 'dropoff_datetime'])\n",
    "    \n",
    "    groupDF = taxi_filtered.join(taxi_lookup.set_index('LocationID'), on='PULocationID')\n",
    "    groupDF['pickup_month'] = groupDF['month'] + groupDF['year']\n",
    "    groupDF = groupDF.groupby('pickup_month', 'borough').agg('count').sort_values(by=['count', 'borough'], ascending=[False, True])\n",
    "    groupDF\n",
    "    return groupDF\n",
    "    \n",
    "def get_monthly_totals_concat_pyspark(taxiPath, taxiLookupPath):\n",
    "    taxi = spark.read.json(taxiPath)\n",
    "    taxi_lookup = spark.read.json(taxiLookupPath)\n",
    "    taxi_filtered = (taxi\n",
    "     .filter(taxi.pickup_datetime.isNotNull())\n",
    "     .filter(taxi.dropoff_datetime.isNotNull()))\n",
    "         \n",
    "    groupDF = taxi_filtered.withColumn(\"pickup_month\", f.concat(\"month\", \"year\")).select(\"pickup_datetime\", \"borough\", \"pickup_month\")\n",
    "    groupDF = groupDF.groupBy(\"pickup_month\", \"borough\").count().orderBy(f.desc(\"pickup_month\"), f.desc(\"count\"), \"borough\")\n",
    "    groupDF.show()\n",
    "    return groupDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ecd8b0817bf4817af3d042b0e338847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Interpreter died:\n",
      "\n",
      "\n",
      "ERROR:fake_shell:execute_reply\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/8024848134341106643\", line 229, in execute\n",
      "    exec(code, global_dict)\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "  File \"<stdin>\", line 57, in timer_method\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 232, in timeit\n",
      "    return Timer(stmt, setup, timer, globals).timeit(number)\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 176, in timeit\n",
      "    timing = self.inner(it, self.timer)\n",
      "  File \"<timeit-src>\", line 6, in inner\n",
      "NameError: name 'taxiPath' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/8024848134341106643\", line 318, in execute_request\n",
      "    result = node.execute()\n",
      "  File \"/tmp/8024848134341106643\", line 233, in execute\n",
      "    raise ExecutionError(sys.exc_info())\n",
      "ExecutionError: (<class 'NameError'>, NameError(\"name 'taxiPath' is not defined\"), <traceback object at 0x7fd1b192bf00>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_monthly_totals_concat_pandas(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aebeaf6cbaab4b8bb31d0974f94a35db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"cannot resolve '`borough`' given input columns: [tpep_dropoff_datetime, extra, service, month, improvement_surcharge, tip_amount, VendorID, passenger_count, pickup_datetime, store_and_fwd_flag, DOLocationID, trip_distance, year, ingested_on, dropoff_datetime, total_amount, tolls_amount, RatecodeID, PULocationID, pickup_month, tpep_pickup_datetime, mta_tax, congestion_surcharge, fare_amount, payment_type];;\\n'Project [pickup_datetime#143, 'borough, pickup_month#193]\\n+- Project [DOLocationID#129L, PULocationID#130L, RatecodeID#131, VendorID#132, congestion_surcharge#133, dropoff_datetime#134, extra#135, fare_amount#136, improvement_surcharge#137, ingested_on#138, month#139, mta_tax#140, passenger_count#141L, payment_type#142, pickup_datetime#143, service#144, store_and_fwd_flag#145, tip_amount#146, tolls_amount#147, total_amount#148, tpep_dropoff_datetime#149, tpep_pickup_datetime#150, trip_distance#151, year#152, concat(month#139, year#152) AS pickup_month#193]\\n   +- Filter isnotnull(dropoff_datetime#134)\\n      +- Filter isnotnull(pickup_datetime#143)\\n         +- Relation[DOLocationID#129L,PULocationID#130L,RatecodeID#131,VendorID#132,congestion_surcharge#133,dropoff_datetime#134,extra#135,fare_amount#136,improvement_surcharge#137,ingested_on#138,month#139,mta_tax#140,passenger_count#141L,payment_type#142,pickup_datetime#143,service#144,store_and_fwd_flag#145,tip_amount#146,tolls_amount#147,total_amount#148,tpep_dropoff_datetime#149,tpep_pickup_datetime#150,trip_distance#151,year#152] json\\n\"\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 57, in timer_method\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 232, in timeit\n",
      "    return Timer(stmt, setup, timer, globals).timeit(number)\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 176, in timeit\n",
      "    timing = self.inner(it, self.timer)\n",
      "  File \"<timeit-src>\", line 6, in inner\n",
      "  File \"<stdin>\", line 22, in get_monthly_totals_concat_pyspark\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 1326, in select\n",
      "    jdf = self._jdf.select(self._jcols(*cols))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: \"cannot resolve '`borough`' given input columns: [tpep_dropoff_datetime, extra, service, month, improvement_surcharge, tip_amount, VendorID, passenger_count, pickup_datetime, store_and_fwd_flag, DOLocationID, trip_distance, year, ingested_on, dropoff_datetime, total_amount, tolls_amount, RatecodeID, PULocationID, pickup_month, tpep_pickup_datetime, mta_tax, congestion_surcharge, fare_amount, payment_type];;\\n'Project [pickup_datetime#143, 'borough, pickup_month#193]\\n+- Project [DOLocationID#129L, PULocationID#130L, RatecodeID#131, VendorID#132, congestion_surcharge#133, dropoff_datetime#134, extra#135, fare_amount#136, improvement_surcharge#137, ingested_on#138, month#139, mta_tax#140, passenger_count#141L, payment_type#142, pickup_datetime#143, service#144, store_and_fwd_flag#145, tip_amount#146, tolls_amount#147, total_amount#148, tpep_dropoff_datetime#149, tpep_pickup_datetime#150, trip_distance#151, year#152, concat(month#139, year#152) AS pickup_month#193]\\n   +- Filter isnotnull(dropoff_datetime#134)\\n      +- Filter isnotnull(pickup_datetime#143)\\n         +- Relation[DOLocationID#129L,PULocationID#130L,RatecodeID#131,VendorID#132,congestion_surcharge#133,dropoff_datetime#134,extra#135,fare_amount#136,improvement_surcharge#137,ingested_on#138,month#139,mta_tax#140,passenger_count#141L,payment_type#142,pickup_datetime#143,service#144,store_and_fwd_flag#145,tip_amount#146,tolls_amount#147,total_amount#148,tpep_dropoff_datetime#149,tpep_pickup_datetime#150,trip_distance#151,year#152] json\\n\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_monthly_totals_concat_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.8 - Case Study 2: Month over month, get the borough with the most amount of pickups per month\n",
    "\n",
    "Add pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f62fb8e126e499fadc03a70ab1ed1a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_most_pickups_per_month_pandas(taxiPath, taxiLookupPath):\n",
    "    inputDF = get_monthly_totals_pandas(taxiPath, taxiLookupPath)\n",
    "    firstDF = inputDF.groupby('pickup_month').sort_values(by=['pickup_month', 'count'], ascending=[True, False]).first()\n",
    "    firstDF\n",
    "    return firstDF\n",
    "\n",
    "def get_most_pickups_per_month_pyspark(taxiPath, taxiLookupPath):\n",
    "    inputDF = get_monthly_totals_pyspark(taxiPath, taxiLookupPath)\n",
    "    firstDF = inputDF.groupBy(\"pickup_month\").orderBy(f.desc(\"pickup_month\"), f.desc(\"count\")).agg(f.first(\"borough\"))\n",
    "    firstDF.explain()\n",
    "    firstDF.show()\n",
    "    return firstDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c5498902794932935e9ed17e076c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Interpreter died:\n",
      "\n",
      "\n",
      "ERROR:fake_shell:execute_reply\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/5988482819598318708\", line 229, in execute\n",
      "    exec(code, global_dict)\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "  File \"<stdin>\", line 57, in timer_method\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 232, in timeit\n",
      "    return Timer(stmt, setup, timer, globals).timeit(number)\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 176, in timeit\n",
      "    timing = self.inner(it, self.timer)\n",
      "  File \"<timeit-src>\", line 6, in inner\n",
      "NameError: name 'taxiPath' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/5988482819598318708\", line 318, in execute_request\n",
      "    result = node.execute()\n",
      "  File \"/tmp/5988482819598318708\", line 233, in execute\n",
      "    raise ExecutionError(sys.exc_info())\n",
      "ExecutionError: (<class 'NameError'>, NameError(\"name 'taxiPath' is not defined\"), <traceback object at 0x7f6f559d6e10>)\n",
      "ERROR:fake_shell:execute_reply\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o171.count.\n",
      ": org.apache.spark.sql.AnalysisException: cannot resolve '`borough`' given input columns: [total_amount, dropoff_datetime, pickup_datetime, tpep_pickup_datetime, tolls_amount, improvement_surcharge, DOLocationID, year, ingested_on, pickup_month, payment_type, extra, fare_amount, tpep_dropoff_datetime, congestion_surcharge, trip_distance, month, store_and_fwd_flag, mta_tax, PULocationID, VendorID, RatecodeID, tip_amount, passenger_count, service];;\n",
      "'Aggregate [pickup_month#70, 'borough], [pickup_month#70, 'borough, count(1) AS count#122L]\n",
      "+- Project [DOLocationID#6L, PULocationID#7L, RatecodeID#8, VendorID#9, congestion_surcharge#10, dropoff_datetime#11, extra#12, fare_amount#13, improvement_surcharge#14, ingested_on#15, month#16, mta_tax#17, passenger_count#18L, payment_type#19, pickup_datetime#20, service#21, store_and_fwd_flag#22, tip_amount#23, tolls_amount#24, total_amount#25, tpep_dropoff_datetime#26, tpep_pickup_datetime#27, trip_distance#28, year#29, concat(month#16, year#29) AS pickup_month#70]\n",
      "   +- Filter isnotnull(dropoff_datetime#11)\n",
      "      +- Filter isnotnull(pickup_datetime#20)\n",
      "         +- Relation[DOLocationID#6L,PULocationID#7L,RatecodeID#8,VendorID#9,congestion_surcharge#10,dropoff_datetime#11,extra#12,fare_amount#13,improvement_surcharge#14,ingested_on#15,month#16,mta_tax#17,passenger_count#18L,payment_type#19,pickup_datetime#20,service#21,store_and_fwd_flag#22,tip_amount#23,tolls_amount#24,total_amount#25,tpep_dropoff_datetime#26,tpep_pickup_datetime#27,trip_distance#28,year#29] json\n",
      "\n",
      "\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:310)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:310)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:309)\n",
      "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n",
      "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n",
      "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n",
      "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n",
      "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n",
      "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
      "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n",
      "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:213)\n",
      "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n",
      "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:153)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:128)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:125)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)\n",
      "\tat org.apache.spark.sql.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:65)\n",
      "\tat org.apache.spark.sql.RelationalGroupedDataset.count(RelationalGroupedDataset.scala:237)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/5988482819598318708\", line 229, in execute\n",
      "    exec(code, global_dict)\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "  File \"<stdin>\", line 57, in timer_method\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 232, in timeit\n",
      "    return Timer(stmt, setup, timer, globals).timeit(number)\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 176, in timeit\n",
      "    timing = self.inner(it, self.timer)\n",
      "  File \"<timeit-src>\", line 6, in inner\n",
      "  File \"<stdin>\", line 23, in get_monthly_totals_concat_pyspark\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/group.py\", line 32, in _api\n",
      "    jdf = getattr(self._jgd, name)()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: \"cannot resolve '`borough`' given input columns: [total_amount, dropoff_datetime, pickup_datetime, tpep_pickup_datetime, tolls_amount, improvement_surcharge, DOLocationID, year, ingested_on, pickup_month, payment_type, extra, fare_amount, tpep_dropoff_datetime, congestion_surcharge, trip_distance, month, store_and_fwd_flag, mta_tax, PULocationID, VendorID, RatecodeID, tip_amount, passenger_count, service];;\\n'Aggregate [pickup_month#70, 'borough], [pickup_month#70, 'borough, count(1) AS count#122L]\\n+- Project [DOLocationID#6L, PULocationID#7L, RatecodeID#8, VendorID#9, congestion_surcharge#10, dropoff_datetime#11, extra#12, fare_amount#13, improvement_surcharge#14, ingested_on#15, month#16, mta_tax#17, passenger_count#18L, payment_type#19, pickup_datetime#20, service#21, store_and_fwd_flag#22, tip_amount#23, tolls_amount#24, total_amount#25, tpep_dropoff_datetime#26, tpep_pickup_datetime#27, trip_distance#28, year#29, concat(month#16, year#29) AS pickup_month#70]\\n   +- Filter isnotnull(dropoff_datetime#11)\\n      +- Filter isnotnull(pickup_datetime#20)\\n         +- Relation[DOLocationID#6L,PULocationID#7L,RatecodeID#8,VendorID#9,congestion_surcharge#10,dropoff_datetime#11,extra#12,fare_amount#13,improvement_surcharge#14,ingested_on#15,month#16,mta_tax#17,passenger_count#18L,payment_type#19,pickup_datetime#20,service#21,store_and_fwd_flag#22,tip_amount#23,tolls_amount#24,total_amount#25,tpep_dropoff_datetime#26,tpep_pickup_datetime#27,trip_distance#28,year#29] json\\n\"\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/5988482819598318708\", line 318, in execute_request\n",
      "    result = node.execute()\n",
      "  File \"/tmp/5988482819598318708\", line 233, in execute\n",
      "    raise ExecutionError(sys.exc_info())\n",
      "ExecutionError: (<class 'pyspark.sql.utils.AnalysisException'>, AnalysisException(\"cannot resolve '`borough`' given input columns: [total_amount, dropoff_datetime, pickup_datetime, tpep_pickup_datetime, tolls_amount, improvement_surcharge, DOLocationID, year, ingested_on, pickup_month, payment_type, extra, fare_amount, tpep_dropoff_datetime, congestion_surcharge, trip_distance, month, store_and_fwd_flag, mta_tax, PULocationID, VendorID, RatecodeID, tip_amount, passenger_count, service];;\\n'Aggregate [pickup_month#70, 'borough], [pickup_month#70, 'borough, count(1) AS count#122L]\\n+- Project [DOLocationID#6L, PULocationID#7L, RatecodeID#8, VendorID#9, congestion_surcharge#10, dropoff_datetime#11, extra#12, fare_amount#13, improvement_surcharge#14, ingested_on#15, month#16, mta_tax#17, passenger_count#18L, payment_type#19, pickup_datetime#20, service#21, store_and_fwd_flag#22, tip_amount#23, tolls_amount#24, total_amount#25, tpep_dropoff_datetime#26, tpep_pickup_datetime#27, trip_distance#28, year#29, concat(month#16, year#29) AS pickup_month#70]\\n   +- Filter isnotnull(dropoff_datetime#11)\\n      +- Filter isnotnull(pickup_datetime#20)\\n         +- Relation[DOLocationID#6L,PULocationID#7L,RatecodeID#8,VendorID#9,congestion_surcharge#10,dropoff_datetime#11,extra#12,fare_amount#13,improvement_surcharge#14,ingested_on#15,month#16,mta_tax#17,passenger_count#18L,payment_type#19,pickup_datetime#20,service#21,store_and_fwd_flag#22,tip_amount#23,tolls_amount#24,total_amount#25,tpep_dropoff_datetime#26,tpep_pickup_datetime#27,trip_distance#28,year#29] json\\n\", 'org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\\n\\t at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\\n\\t at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:310)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:310)\\n\\t at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:71)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:309)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\\n\\t at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:71)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\\n\\t at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\\n\\t at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\\n\\t at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\\n\\t at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\\n\\t at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\\n\\t at scala.collection.AbstractTraversable.map(Traversable.scala:104)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:213)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\\n\\t at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\\n\\t at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:153)\\n\\t at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\\n\\t at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:112)\\n\\t at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:128)\\n\\t at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:125)\\n\\t at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\\n\\t at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:125)\\n\\t at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\\n\\t at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\\n\\t at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\\n\\t at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)\\n\\t at org.apache.spark.sql.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:65)\\n\\t at org.apache.spark.sql.RelationalGroupedDataset.count(RelationalGroupedDataset.scala:237)\\n\\t at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\t at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\t at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\t at java.lang.reflect.Method.invoke(Method.java:498)\\n\\t at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\t at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\t at py4j.Gateway.invoke(Gateway.java:282)\\n\\t at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\t at py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\t at py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\\t at java.lang.Thread.run(Thread.java:748)'), <traceback object at 0x7f6f41f66f50>)\n",
      "ERROR:fake_shell:execute_reply\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o246.select.\n",
      ": org.apache.spark.sql.AnalysisException: cannot resolve '`borough`' given input columns: [tpep_dropoff_datetime, extra, service, month, improvement_surcharge, tip_amount, VendorID, passenger_count, pickup_datetime, store_and_fwd_flag, DOLocationID, trip_distance, year, ingested_on, dropoff_datetime, total_amount, tolls_amount, RatecodeID, PULocationID, pickup_month, tpep_pickup_datetime, mta_tax, congestion_surcharge, fare_amount, payment_type];;\n",
      "'Project [pickup_datetime#143, 'borough, pickup_month#193]\n",
      "+- Project [DOLocationID#129L, PULocationID#130L, RatecodeID#131, VendorID#132, congestion_surcharge#133, dropoff_datetime#134, extra#135, fare_amount#136, improvement_surcharge#137, ingested_on#138, month#139, mta_tax#140, passenger_count#141L, payment_type#142, pickup_datetime#143, service#144, store_and_fwd_flag#145, tip_amount#146, tolls_amount#147, total_amount#148, tpep_dropoff_datetime#149, tpep_pickup_datetime#150, trip_distance#151, year#152, concat(month#139, year#152) AS pickup_month#193]\n",
      "   +- Filter isnotnull(dropoff_datetime#134)\n",
      "      +- Filter isnotnull(pickup_datetime#143)\n",
      "         +- Relation[DOLocationID#129L,PULocationID#130L,RatecodeID#131,VendorID#132,congestion_surcharge#133,dropoff_datetime#134,extra#135,fare_amount#136,improvement_surcharge#137,ingested_on#138,month#139,mta_tax#140,passenger_count#141L,payment_type#142,pickup_datetime#143,service#144,store_and_fwd_flag#145,tip_amount#146,tolls_amount#147,total_amount#148,tpep_dropoff_datetime#149,tpep_pickup_datetime#150,trip_distance#151,year#152] json\n",
      "\n",
      "\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:310)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:310)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:309)\n",
      "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n",
      "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n",
      "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n",
      "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n",
      "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n",
      "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
      "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n",
      "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:213)\n",
      "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n",
      "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:153)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:128)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:125)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)\n",
      "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3418)\n",
      "\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1342)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/5988482819598318708\", line 229, in execute\n",
      "    exec(code, global_dict)\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "  File \"<stdin>\", line 57, in timer_method\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 232, in timeit\n",
      "    return Timer(stmt, setup, timer, globals).timeit(number)\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 176, in timeit\n",
      "    timing = self.inner(it, self.timer)\n",
      "  File \"<timeit-src>\", line 6, in inner\n",
      "  File \"<stdin>\", line 22, in get_monthly_totals_concat_pyspark\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 1326, in select\n",
      "    jdf = self._jdf.select(self._jcols(*cols))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: \"cannot resolve '`borough`' given input columns: [tpep_dropoff_datetime, extra, service, month, improvement_surcharge, tip_amount, VendorID, passenger_count, pickup_datetime, store_and_fwd_flag, DOLocationID, trip_distance, year, ingested_on, dropoff_datetime, total_amount, tolls_amount, RatecodeID, PULocationID, pickup_month, tpep_pickup_datetime, mta_tax, congestion_surcharge, fare_amount, payment_type];;\\n'Project [pickup_datetime#143, 'borough, pickup_month#193]\\n+- Project [DOLocationID#129L, PULocationID#130L, RatecodeID#131, VendorID#132, congestion_surcharge#133, dropoff_datetime#134, extra#135, fare_amount#136, improvement_surcharge#137, ingested_on#138, month#139, mta_tax#140, passenger_count#141L, payment_type#142, pickup_datetime#143, service#144, store_and_fwd_flag#145, tip_amount#146, tolls_amount#147, total_amount#148, tpep_dropoff_datetime#149, tpep_pickup_datetime#150, trip_distance#151, year#152, concat(month#139, year#152) AS pickup_month#193]\\n   +- Filter isnotnull(dropoff_datetime#134)\\n      +- Filter isnotnull(pickup_datetime#143)\\n         +- Relation[DOLocationID#129L,PULocationID#130L,RatecodeID#131,VendorID#132,congestion_surcharge#133,dropoff_datetime#134,extra#135,fare_amount#136,improvement_surcharge#137,ingested_on#138,month#139,mta_tax#140,passenger_count#141L,payment_type#142,pickup_datetime#143,service#144,store_and_fwd_flag#145,tip_amount#146,tolls_amount#147,total_amount#148,tpep_dropoff_datetime#149,tpep_pickup_datetime#150,trip_distance#151,year#152] json\\n\"\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/5988482819598318708\", line 318, in execute_request\n",
      "    result = node.execute()\n",
      "  File \"/tmp/5988482819598318708\", line 233, in execute\n",
      "    raise ExecutionError(sys.exc_info())\n",
      "ExecutionError: (<class 'pyspark.sql.utils.AnalysisException'>, AnalysisException(\"cannot resolve '`borough`' given input columns: [tpep_dropoff_datetime, extra, service, month, improvement_surcharge, tip_amount, VendorID, passenger_count, pickup_datetime, store_and_fwd_flag, DOLocationID, trip_distance, year, ingested_on, dropoff_datetime, total_amount, tolls_amount, RatecodeID, PULocationID, pickup_month, tpep_pickup_datetime, mta_tax, congestion_surcharge, fare_amount, payment_type];;\\n'Project [pickup_datetime#143, 'borough, pickup_month#193]\\n+- Project [DOLocationID#129L, PULocationID#130L, RatecodeID#131, VendorID#132, congestion_surcharge#133, dropoff_datetime#134, extra#135, fare_amount#136, improvement_surcharge#137, ingested_on#138, month#139, mta_tax#140, passenger_count#141L, payment_type#142, pickup_datetime#143, service#144, store_and_fwd_flag#145, tip_amount#146, tolls_amount#147, total_amount#148, tpep_dropoff_datetime#149, tpep_pickup_datetime#150, trip_distance#151, year#152, concat(month#139, year#152) AS pickup_month#193]\\n   +- Filter isnotnull(dropoff_datetime#134)\\n      +- Filter isnotnull(pickup_datetime#143)\\n         +- Relation[DOLocationID#129L,PULocationID#130L,RatecodeID#131,VendorID#132,congestion_surcharge#133,dropoff_datetime#134,extra#135,fare_amount#136,improvement_surcharge#137,ingested_on#138,month#139,mta_tax#140,passenger_count#141L,payment_type#142,pickup_datetime#143,service#144,store_and_fwd_flag#145,tip_amount#146,tolls_amount#147,total_amount#148,tpep_dropoff_datetime#149,tpep_pickup_datetime#150,trip_distance#151,year#152] json\\n\", 'org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\\n\\t at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\\n\\t at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:310)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:310)\\n\\t at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:71)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:309)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\\n\\t at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:71)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\\n\\t at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\\n\\t at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\\n\\t at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\\n\\t at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\\n\\t at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\\n\\t at scala.collection.AbstractTraversable.map(Traversable.scala:104)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:213)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\\n\\t at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\\n\\t at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:153)\\n\\t at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\\n\\t at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:112)\\n\\t at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:128)\\n\\t at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:125)\\n\\t at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\\n\\t at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:125)\\n\\t at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\\n\\t at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\\n\\t at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\\n\\t at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)\\n\\t at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3418)\\n\\t at org.apache.spark.sql.Dataset.select(Dataset.scala:1342)\\n\\t at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\t at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\t at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\t at java.lang.reflect.Method.invoke(Method.java:498)\\n\\t at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\t at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\t at py4j.Gateway.invoke(Gateway.java:282)\\n\\t at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\t at py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\t at py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\\t at java.lang.Thread.run(Thread.java:748)'), <traceback object at 0x7f6f41f660f0>)\n",
      "ERROR:fake_shell:execute_reply\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/5988482819598318708\", line 229, in execute\n",
      "    exec(code, global_dict)\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "  File \"<stdin>\", line 57, in timer_method\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 232, in timeit\n",
      "    return Timer(stmt, setup, timer, globals).timeit(number)\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 176, in timeit\n",
      "    timing = self.inner(it, self.timer)\n",
      "  File \"<timeit-src>\", line 6, in inner\n",
      "  File \"<stdin>\", line 2, in get_most_pickups_per_month_pandas\n",
      "NameError: name 'get_monthly_totals_pandas' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/5988482819598318708\", line 318, in execute_request\n",
      "    result = node.execute()\n",
      "  File \"/tmp/5988482819598318708\", line 233, in execute\n",
      "    raise ExecutionError(sys.exc_info())\n",
      "ExecutionError: (<class 'NameError'>, NameError(\"name 'get_monthly_totals_pandas' is not defined\"), <traceback object at 0x7f6f41f5ce10>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_most_pickups_per_month_pandas(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa98beafe9db4f9bba0ecc533db04d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3be9dfce8d543feb922051617b090b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '400' from https://172.31.6.49:18888/sessions/2/statements/90 with error payload: {\"msg\":\"requirement failed: Session isn't active.\"}\n"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_most_pickups_per_month_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c08702b3219447eb9b98053cd08e74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 2 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "#\n",
      "# There is insufficient memory for the Java Runtime Environment to continue.\n",
      "# Native memory allocation (mmap) failed to map 2498269184 bytes for committing reserved memory.\n",
      "# An error report file with more information is saved as:\n",
      "# /tmp/hs_err_pid20386.log\n",
      "\n",
      "stderr: \n",
      "20/10/19 00:18:13 INFO TaskSetManager: Finished task 31.0 in stage 118.0 (TID 2090) in 3819 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 28) (31/76)\n",
      "20/10/19 00:18:13 INFO BlockManagerInfo: Removed taskresult_2090 on ip-172-31-8-43.us-east-2.compute.internal:40247 in memory (size: 20.3 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:13 INFO BlockManagerInfo: Added taskresult_2091 in memory on ip-172-31-4-197.us-east-2.compute.internal:37927 (size: 20.4 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:13 INFO TaskSetManager: Starting task 47.0 in stage 118.0 (TID 2107, ip-172-31-4-197.us-east-2.compute.internal, executor 27, partition 47, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:13 INFO TaskSetManager: Finished task 28.0 in stage 118.0 (TID 2091) in 3820 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 27) (32/76)\n",
      "20/10/19 00:18:13 INFO BlockManagerInfo: Removed taskresult_2091 on ip-172-31-4-197.us-east-2.compute.internal:37927 in memory (size: 20.4 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:15 INFO BlockManagerInfo: Added taskresult_2093 in memory on ip-172-31-8-43.us-east-2.compute.internal:41187 (size: 20.4 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:15 INFO TaskSetManager: Starting task 48.0 in stage 118.0 (TID 2108, ip-172-31-8-43.us-east-2.compute.internal, executor 25, partition 48, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:15 INFO BlockManagerInfo: Added taskresult_2096 in memory on ip-172-31-4-197.us-east-2.compute.internal:43813 (size: 23.0 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:15 INFO BlockManagerInfo: Added taskresult_2095 in memory on ip-172-31-8-43.us-east-2.compute.internal:41187 (size: 23.1 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:15 INFO TaskSetManager: Starting task 53.0 in stage 118.0 (TID 2109, ip-172-31-4-197.us-east-2.compute.internal, executor 26, partition 53, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:15 INFO TaskSetManager: Starting task 49.0 in stage 118.0 (TID 2110, ip-172-31-8-43.us-east-2.compute.internal, executor 25, partition 49, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:15 INFO BlockManagerInfo: Added taskresult_2094 in memory on ip-172-31-8-43.us-east-2.compute.internal:41187 (size: 23.0 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:15 INFO BlockManagerInfo: Added taskresult_2092 in memory on ip-172-31-8-43.us-east-2.compute.internal:41187 (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:15 INFO BlockManagerInfo: Added taskresult_2097 in memory on ip-172-31-4-197.us-east-2.compute.internal:37927 (size: 23.0 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:15 INFO TaskSetManager: Starting task 50.0 in stage 118.0 (TID 2111, ip-172-31-8-43.us-east-2.compute.internal, executor 25, partition 50, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:15 INFO TaskSetManager: Starting task 51.0 in stage 118.0 (TID 2112, ip-172-31-8-43.us-east-2.compute.internal, executor 25, partition 51, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:15 INFO TaskSetManager: Starting task 54.0 in stage 118.0 (TID 2113, ip-172-31-4-197.us-east-2.compute.internal, executor 27, partition 54, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:15 INFO TaskSetManager: Finished task 34.0 in stage 118.0 (TID 2093) in 3533 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 25) (33/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2093 on ip-172-31-8-43.us-east-2.compute.internal:41187 in memory (size: 20.4 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2099 in memory on ip-172-31-4-197.us-east-2.compute.internal:37927 (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2098 in memory on ip-172-31-8-43.us-east-2.compute.internal:40247 (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 57.0 in stage 118.0 (TID 2114, ip-172-31-4-197.us-east-2.compute.internal, executor 27, partition 57, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 52.0 in stage 118.0 (TID 2115, ip-172-31-8-43.us-east-2.compute.internal, executor 28, partition 52, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 32.0 in stage 118.0 (TID 2096) in 3504 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 26) (34/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2096 on ip-172-31-4-197.us-east-2.compute.internal:43813 in memory (size: 23.0 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 41.0 in stage 118.0 (TID 2095) in 3619 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 25) (35/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2095 on ip-172-31-8-43.us-east-2.compute.internal:41187 in memory (size: 23.1 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 35.0 in stage 118.0 (TID 2097) in 3491 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 27) (36/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2097 on ip-172-31-4-197.us-east-2.compute.internal:37927 in memory (size: 23.0 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 38.0 in stage 118.0 (TID 2094) in 3770 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 25) (37/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2100 in memory on ip-172-31-8-43.us-east-2.compute.internal:40247 (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 55.0 in stage 118.0 (TID 2116, ip-172-31-8-43.us-east-2.compute.internal, executor 28, partition 55, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2094 on ip-172-31-8-43.us-east-2.compute.internal:41187 in memory (size: 23.0 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2105 in memory on ip-172-31-4-197.us-east-2.compute.internal:43813 (size: 22.9 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 36.0 in stage 118.0 (TID 2099) in 3426 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 27) (38/76)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 59.0 in stage 118.0 (TID 2117, ip-172-31-4-197.us-east-2.compute.internal, executor 26, partition 59, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2099 on ip-172-31-4-197.us-east-2.compute.internal:37927 in memory (size: 20.3 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 33.0 in stage 118.0 (TID 2092) in 3898 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 25) (39/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2102 in memory on ip-172-31-4-197.us-east-2.compute.internal:43813 (size: 20.4 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2092 on ip-172-31-8-43.us-east-2.compute.internal:41187 in memory (size: 20.3 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2104 in memory on ip-172-31-4-197.us-east-2.compute.internal:37927 (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 60.0 in stage 118.0 (TID 2118, ip-172-31-4-197.us-east-2.compute.internal, executor 26, partition 60, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 61.0 in stage 118.0 (TID 2119, ip-172-31-4-197.us-east-2.compute.internal, executor 27, partition 61, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 42.0 in stage 118.0 (TID 2098) in 3631 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 28) (40/76)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 44.0 in stage 118.0 (TID 2105) in 3361 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 26) (41/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2105 on ip-172-31-4-197.us-east-2.compute.internal:43813 in memory (size: 22.9 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2098 on ip-172-31-8-43.us-east-2.compute.internal:40247 in memory (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2106 in memory on ip-172-31-8-43.us-east-2.compute.internal:40247 (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 56.0 in stage 118.0 (TID 2120, ip-172-31-8-43.us-east-2.compute.internal, executor 28, partition 56, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 40.0 in stage 118.0 (TID 2104) in 3475 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 27) (42/76)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 39.0 in stage 118.0 (TID 2102) in 3636 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 26) (43/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2102 on ip-172-31-4-197.us-east-2.compute.internal:43813 in memory (size: 20.4 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2104 on ip-172-31-4-197.us-east-2.compute.internal:37927 in memory (size: 20.3 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 43.0 in stage 118.0 (TID 2100) in 3689 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 28) (44/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2101 in memory on ip-172-31-4-197.us-east-2.compute.internal:43813 (size: 20.4 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 62.0 in stage 118.0 (TID 2121, ip-172-31-4-197.us-east-2.compute.internal, executor 26, partition 62, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2100 on ip-172-31-8-43.us-east-2.compute.internal:40247 in memory (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2103 in memory on ip-172-31-8-43.us-east-2.compute.internal:40247 (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 58.0 in stage 118.0 (TID 2122, ip-172-31-8-43.us-east-2.compute.internal, executor 28, partition 58, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 46.0 in stage 118.0 (TID 2106) in 3494 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 28) (45/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2107 in memory on ip-172-31-4-197.us-east-2.compute.internal:37927 (size: 23.1 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 63.0 in stage 118.0 (TID 2123, ip-172-31-4-197.us-east-2.compute.internal, executor 27, partition 63, NODE_LOCAL, 8577 bytes)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2106 on ip-172-31-8-43.us-east-2.compute.internal:40247 in memory (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 37.0 in stage 118.0 (TID 2101) in 3820 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 26) (46/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2101 on ip-172-31-4-197.us-east-2.compute.internal:43813 in memory (size: 20.4 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 45.0 in stage 118.0 (TID 2103) in 3758 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 28) (47/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2103 on ip-172-31-8-43.us-east-2.compute.internal:40247 in memory (size: 20.3 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 47.0 in stage 118.0 (TID 2107) in 3398 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 27) (48/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2107 on ip-172-31-4-197.us-east-2.compute.internal:37927 in memory (size: 23.1 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:17 INFO BlockManagerInfo: Added taskresult_2109 in memory on ip-172-31-4-197.us-east-2.compute.internal:43813 (size: 14.9 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:17 INFO TaskSetManager: Starting task 64.0 in stage 118.0 (TID 2124, ip-172-31-4-197.us-east-2.compute.internal, executor 26, partition 64, NODE_LOCAL, 8767 bytes)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Added taskresult_2108 in memory on ip-172-31-8-43.us-east-2.compute.internal:41187 (size: 15.4 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Added taskresult_2110 in memory on ip-172-31-8-43.us-east-2.compute.internal:41187 (size: 15.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Finished task 53.0 in stage 118.0 (TID 2109) in 2181 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 26) (49/76)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Starting task 65.0 in stage 118.0 (TID 2125, ip-172-31-8-43.us-east-2.compute.internal, executor 25, partition 65, NODE_LOCAL, 8767 bytes)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Starting task 66.0 in stage 118.0 (TID 2126, ip-172-31-8-43.us-east-2.compute.internal, executor 25, partition 66, NODE_LOCAL, 8767 bytes)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Added taskresult_2112 in memory on ip-172-31-8-43.us-east-2.compute.internal:41187 (size: 15.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Added taskresult_2113 in memory on ip-172-31-4-197.us-east-2.compute.internal:37927 (size: 14.9 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Starting task 68.0 in stage 118.0 (TID 2127, ip-172-31-4-197.us-east-2.compute.internal, executor 27, partition 68, NODE_LOCAL, 8767 bytes)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Starting task 67.0 in stage 118.0 (TID 2128, ip-172-31-8-43.us-east-2.compute.internal, executor 25, partition 67, NODE_LOCAL, 8767 bytes)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Removed taskresult_2109 on ip-172-31-4-197.us-east-2.compute.internal:43813 in memory (size: 14.9 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Added taskresult_2111 in memory on ip-172-31-8-43.us-east-2.compute.internal:41187 (size: 15.2 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Starting task 69.0 in stage 118.0 (TID 2129, ip-172-31-8-43.us-east-2.compute.internal, executor 25, partition 69, NODE_LOCAL, 8957 bytes)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Finished task 49.0 in stage 118.0 (TID 2110) in 2233 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 25) (50/76)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Removed taskresult_2110 on ip-172-31-8-43.us-east-2.compute.internal:41187 in memory (size: 15.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Finished task 54.0 in stage 118.0 (TID 2113) in 2158 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 27) (51/76)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Finished task 48.0 in stage 118.0 (TID 2108) in 2324 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 25) (52/76)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Removed taskresult_2113 on ip-172-31-4-197.us-east-2.compute.internal:37927 in memory (size: 14.9 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Removed taskresult_2108 on ip-172-31-8-43.us-east-2.compute.internal:41187 in memory (size: 15.4 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Finished task 51.0 in stage 118.0 (TID 2112) in 2238 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 25) (53/76)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Added taskresult_2114 in memory on ip-172-31-4-197.us-east-2.compute.internal:37927 (size: 14.9 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Starting task 70.0 in stage 118.0 (TID 2130, ip-172-31-4-197.us-east-2.compute.internal, executor 27, partition 70, NODE_LOCAL, 8957 bytes)\n"
     ]
    }
   ],
   "source": [
    "def get_most_pickups_per_month_window_pyspark(taxiPath, taxiLookupPath):\n",
    "    from pyspark.sql import Window\n",
    "    inputDF = get_monthly_totals_pyspark(taxiPath, taxiFilteredPath)\n",
    "    win = Window.partitionBy(\"borough\", \"pickup_month\").orderBy(f.desc(\"count\"))\n",
    "    firstDF = inputDF.withColumn(\"row_num\", f.row_number().over(win)).where(\"row_num == 1\")\n",
    "    firstDF = firstDF.orderBy(f.desc(\"count\"), f.desc(\"pickup_month\"))\n",
    "    firstDF.explain()\n",
    "    firstDF.show()\n",
    "    return firstDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a363cbeb1d408bafbf688c7a006f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 4 unexpectedly reached final status 'error'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "20/10/23 04:20:14 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 113, ip-172-31-11-79.ec2.internal, executor 13, partition 7, NODE_LOCAL, 8375 bytes)\n",
      "20/10/23 04:20:14 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on ip-172-31-8-57.ec2.internal:35637 (size: 6.1 KB, free: 5.3 GB)\n",
      "20/10/23 04:20:14 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on ip-172-31-11-79.ec2.internal:36177 (size: 6.1 KB, free: 5.3 GB)\n",
      "20/10/23 04:20:15 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on ip-172-31-11-79.ec2.internal:41257 (size: 6.1 KB, free: 5.3 GB)\n",
      "20/10/23 04:20:15 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on ip-172-31-8-57.ec2.internal:35637 (size: 28.4 KB, free: 5.3 GB)\n",
      "20/10/23 04:20:15 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on ip-172-31-11-79.ec2.internal:36177 (size: 28.4 KB, free: 5.3 GB)\n",
      "20/10/23 04:20:15 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on ip-172-31-11-79.ec2.internal:41257 (size: 28.4 KB, free: 5.3 GB)\n",
      "20/10/23 04:20:15 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 114, ip-172-31-8-57.ec2.internal, executor 11, partition 8, RACK_LOCAL, 8375 bytes)\n",
      "20/10/23 04:20:16 INFO ExecutorAllocationManager: Requesting 4 new executors because tasks are backlogged (new desired total will be 4)\n",
      "20/10/23 04:20:17 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 5)\n",
      "20/10/23 04:20:17 INFO BlockManagerInfo: Added taskresult_111 in memory on ip-172-31-8-57.ec2.internal:35637 (size: 19.5 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:17 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 115, ip-172-31-8-57.ec2.internal, executor 11, partition 9, RACK_LOCAL, 8375 bytes)\n",
      "20/10/23 04:20:17 INFO TransportClientFactory: Successfully created connection to ip-172-31-8-57.ec2.internal/172.31.8.57:35637 after 1 ms (0 ms spent in bootstraps)\n",
      "20/10/23 04:20:17 INFO TaskSetManager: Finished task 26.0 in stage 10.0 (TID 111) in 2814 ms on ip-172-31-8-57.ec2.internal (executor 11) (1/28)\n",
      "20/10/23 04:20:17 INFO BlockManagerInfo: Removed taskresult_111 on ip-172-31-8-57.ec2.internal:35637 in memory (size: 19.5 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:17 INFO BlockManagerInfo: Added taskresult_108 in memory on ip-172-31-8-57.ec2.internal:35637 (size: 20.7 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:17 INFO TaskSetManager: Starting task 10.0 in stage 10.0 (TID 116, ip-172-31-8-57.ec2.internal, executor 11, partition 10, RACK_LOCAL, 8375 bytes)\n",
      "20/10/23 04:20:17 INFO TaskSetManager: Finished task 25.0 in stage 10.0 (TID 108) in 3014 ms on ip-172-31-8-57.ec2.internal (executor 11) (2/28)\n",
      "20/10/23 04:20:17 INFO BlockManagerInfo: Removed taskresult_108 on ip-172-31-8-57.ec2.internal:35637 in memory (size: 20.7 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:18 INFO ExecutorAllocationManager: Requesting 2 new executors because tasks are backlogged (new desired total will be 7)\n",
      "20/10/23 04:20:18 INFO BlockManagerInfo: Added taskresult_105 in memory on ip-172-31-8-57.ec2.internal:35637 (size: 20.7 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:18 INFO TaskSetManager: Starting task 11.0 in stage 10.0 (TID 117, ip-172-31-8-57.ec2.internal, executor 11, partition 11, RACK_LOCAL, 8375 bytes)\n",
      "20/10/23 04:20:18 INFO TaskSetManager: Finished task 24.0 in stage 10.0 (TID 105) in 3342 ms on ip-172-31-8-57.ec2.internal (executor 11) (3/28)\n",
      "20/10/23 04:20:18 INFO BlockManagerInfo: Removed taskresult_105 on ip-172-31-8-57.ec2.internal:35637 in memory (size: 20.7 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:18 INFO BlockManagerInfo: Added taskresult_114 in memory on ip-172-31-8-57.ec2.internal:35637 (size: 20.8 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:18 INFO TaskSetManager: Starting task 12.0 in stage 10.0 (TID 118, ip-172-31-8-57.ec2.internal, executor 11, partition 12, RACK_LOCAL, 8375 bytes)\n",
      "20/10/23 04:20:18 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 114) in 3064 ms on ip-172-31-8-57.ec2.internal (executor 11) (4/28)\n",
      "20/10/23 04:20:18 INFO BlockManagerInfo: Removed taskresult_114 on ip-172-31-8-57.ec2.internal:35637 in memory (size: 20.8 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:19 INFO BlockManagerInfo: Added taskresult_104 in memory on ip-172-31-11-79.ec2.internal:36177 (size: 21.2 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:19 INFO TaskSetManager: Starting task 13.0 in stage 10.0 (TID 119, ip-172-31-11-79.ec2.internal, executor 13, partition 13, NODE_LOCAL, 8375 bytes)\n",
      "20/10/23 04:20:19 INFO TransportClientFactory: Successfully created connection to ip-172-31-11-79.ec2.internal/172.31.11.79:36177 after 1 ms (0 ms spent in bootstraps)\n",
      "20/10/23 04:20:19 INFO BlockManagerInfo: Added taskresult_106 in memory on ip-172-31-11-79.ec2.internal:41257 (size: 21.0 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:19 INFO BlockManagerInfo: Added taskresult_113 in memory on ip-172-31-11-79.ec2.internal:36177 (size: 20.8 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:19 INFO TaskSetManager: Starting task 14.0 in stage 10.0 (TID 120, ip-172-31-11-79.ec2.internal, executor 13, partition 14, NODE_LOCAL, 8375 bytes)\n",
      "20/10/23 04:20:19 INFO BlockManagerInfo: Added taskresult_112 in memory on ip-172-31-11-79.ec2.internal:41257 (size: 20.7 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:19 INFO TaskSetManager: Starting task 15.0 in stage 10.0 (TID 121, ip-172-31-11-79.ec2.internal, executor 12, partition 15, NODE_LOCAL, 8375 bytes)\n",
      "20/10/23 04:20:19 INFO TaskSetManager: Starting task 16.0 in stage 10.0 (TID 122, ip-172-31-11-79.ec2.internal, executor 12, partition 16, NODE_LOCAL, 8375 bytes)\n",
      "20/10/23 04:20:19 INFO TransportClientFactory: Successfully created connection to ip-172-31-11-79.ec2.internal/172.31.11.79:41257 after 1 ms (0 ms spent in bootstraps)\n",
      "20/10/23 04:20:19 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.8.57:46912) with ID 14\n",
      "20/10/23 04:20:19 INFO ExecutorAllocationManager: New executor 14 has registered (new total is 4)\n",
      "20/10/23 04:20:19 INFO TaskSetManager: Starting task 17.0 in stage 10.0 (TID 123, ip-172-31-8-57.ec2.internal, executor 14, partition 17, RACK_LOCAL, 8375 bytes)\n",
      "20/10/23 04:20:19 INFO TaskSetManager: Starting task 18.0 in stage 10.0 (TID 124, ip-172-31-8-57.ec2.internal, executor 14, partition 18, RACK_LOCAL, 8375 bytes)\n",
      "20/10/23 04:20:19 INFO TaskSetManager: Starting task 19.0 in stage 10.0 (TID 125, ip-172-31-8-57.ec2.internal, executor 14, partition 19, RACK_LOCAL, 8375 bytes)\n",
      "20/10/23 04:20:19 INFO TaskSetManager: Starting task 20.0 in stage 10.0 (TID 126, ip-172-31-8-57.ec2.internal, executor 14, partition 20, RACK_LOCAL, 8375 bytes)\n",
      "20/10/23 04:20:19 INFO BlockManagerInfo: Added taskresult_103 in memory on ip-172-31-11-79.ec2.internal:41257 (size: 21.2 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:19 INFO TaskSetManager: Starting task 21.0 in stage 10.0 (TID 127, ip-172-31-11-79.ec2.internal, executor 12, partition 21, NODE_LOCAL, 8375 bytes)\n",
      "20/10/23 04:20:19 INFO BlockManagerInfo: Added taskresult_107 in memory on ip-172-31-11-79.ec2.internal:36177 (size: 21.1 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:19 INFO TaskSetManager: Starting task 22.0 in stage 10.0 (TID 128, ip-172-31-11-79.ec2.internal, executor 13, partition 22, NODE_LOCAL, 8375 bytes)\n",
      "20/10/23 04:20:19 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 104) in 4806 ms on ip-172-31-11-79.ec2.internal (executor 13) (5/28)\n",
      "20/10/23 04:20:19 INFO BlockManagerInfo: Added taskresult_110 in memory on ip-172-31-11-79.ec2.internal:36177 (size: 20.7 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:19 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-8-57.ec2.internal:37689 with 5.3 GB RAM, BlockManagerId(14, ip-172-31-8-57.ec2.internal, 37689, None)\n",
      "20/10/23 04:20:19 INFO BlockManagerInfo: Removed taskresult_104 on ip-172-31-11-79.ec2.internal:36177 in memory (size: 21.2 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:19 INFO TaskSetManager: Starting task 23.0 in stage 10.0 (TID 129, ip-172-31-11-79.ec2.internal, executor 13, partition 23, NODE_LOCAL, 8375 bytes)\n",
      "20/10/23 04:20:19 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 113) in 4931 ms on ip-172-31-11-79.ec2.internal (executor 13) (6/28)\n",
      "20/10/23 04:20:19 INFO BlockManagerInfo: Added taskresult_109 in memory on ip-172-31-11-79.ec2.internal:41257 (size: 20.9 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:19 INFO TaskSetManager: Starting task 27.0 in stage 10.0 (TID 130, ip-172-31-11-79.ec2.internal, executor 12, partition 27, NODE_LOCAL, 8731 bytes)\n",
      "20/10/23 04:20:19 INFO BlockManagerInfo: Removed taskresult_113 on ip-172-31-11-79.ec2.internal:36177 in memory (size: 20.8 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:20 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 112) in 5106 ms on ip-172-31-11-79.ec2.internal (executor 12) (7/28)\n",
      "20/10/23 04:20:20 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 107) in 5121 ms on ip-172-31-11-79.ec2.internal (executor 13) (8/28)\n",
      "20/10/23 04:20:20 INFO BlockManagerInfo: Removed taskresult_112 on ip-172-31-11-79.ec2.internal:41257 in memory (size: 20.7 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:20 INFO BlockManagerInfo: Removed taskresult_107 on ip-172-31-11-79.ec2.internal:36177 in memory (size: 21.1 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:20 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 106) in 5228 ms on ip-172-31-11-79.ec2.internal (executor 12) (9/28)\n",
      "20/10/23 04:20:20 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 110) in 5257 ms on ip-172-31-11-79.ec2.internal (executor 13) (10/28)\n",
      "20/10/23 04:20:20 INFO BlockManagerInfo: Removed taskresult_106 on ip-172-31-11-79.ec2.internal:41257 in memory (size: 21.0 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:20 INFO BlockManagerInfo: Removed taskresult_110 on ip-172-31-11-79.ec2.internal:36177 in memory (size: 20.7 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:20 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 103) in 5388 ms on ip-172-31-11-79.ec2.internal (executor 12) (11/28)\n",
      "20/10/23 04:20:20 INFO BlockManagerInfo: Removed taskresult_103 on ip-172-31-11-79.ec2.internal:41257 in memory (size: 21.2 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:20 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 109) in 5460 ms on ip-172-31-11-79.ec2.internal (executor 12) (12/28)\n",
      "20/10/23 04:20:20 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on ip-172-31-8-57.ec2.internal:37689 (size: 6.1 KB, free: 5.3 GB)\n",
      "20/10/23 04:20:20 INFO BlockManagerInfo: Removed taskresult_109 on ip-172-31-11-79.ec2.internal:41257 in memory (size: 20.9 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:20 INFO BlockManagerInfo: Added taskresult_115 in memory on ip-172-31-8-57.ec2.internal:35637 (size: 21.0 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:21 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 115) in 3503 ms on ip-172-31-8-57.ec2.internal (executor 11) (13/28)\n",
      "20/10/23 04:20:21 INFO BlockManagerInfo: Removed taskresult_115 on ip-172-31-8-57.ec2.internal:35637 in memory (size: 21.0 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:21 INFO BlockManagerInfo: Added taskresult_116 in memory on ip-172-31-8-57.ec2.internal:35637 (size: 20.8 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:21 INFO TaskSetManager: Finished task 10.0 in stage 10.0 (TID 116) in 3557 ms on ip-172-31-8-57.ec2.internal (executor 11) (14/28)\n",
      "20/10/23 04:20:21 INFO BlockManagerInfo: Removed taskresult_116 on ip-172-31-8-57.ec2.internal:35637 in memory (size: 20.8 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:21 INFO BlockManagerInfo: Added taskresult_118 in memory on ip-172-31-8-57.ec2.internal:35637 (size: 20.7 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:21 INFO BlockManagerInfo: Added taskresult_117 in memory on ip-172-31-8-57.ec2.internal:35637 (size: 20.8 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:21 INFO TaskSetManager: Finished task 12.0 in stage 10.0 (TID 118) in 3340 ms on ip-172-31-8-57.ec2.internal (executor 11) (15/28)\n",
      "20/10/23 04:20:21 INFO BlockManagerInfo: Removed taskresult_118 on ip-172-31-8-57.ec2.internal:35637 in memory (size: 20.7 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:21 INFO TaskSetManager: Finished task 11.0 in stage 10.0 (TID 117) in 3646 ms on ip-172-31-8-57.ec2.internal (executor 11) (16/28)\n",
      "20/10/23 04:20:21 INFO BlockManagerInfo: Removed taskresult_117 on ip-172-31-8-57.ec2.internal:35637 in memory (size: 20.8 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:21 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on ip-172-31-8-57.ec2.internal:37689 (size: 28.4 KB, free: 5.3 GB)\n",
      "20/10/23 04:20:23 INFO BlockManagerInfo: Added taskresult_130 in memory on ip-172-31-11-79.ec2.internal:41257 (size: 19.4 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:23 INFO BlockManagerInfo: Added taskresult_122 in memory on ip-172-31-11-79.ec2.internal:41257 (size: 21.0 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:23 INFO TaskSetManager: Finished task 27.0 in stage 10.0 (TID 130) in 3380 ms on ip-172-31-11-79.ec2.internal (executor 12) (17/28)\n",
      "20/10/23 04:20:23 INFO BlockManagerInfo: Removed taskresult_130 on ip-172-31-11-79.ec2.internal:41257 in memory (size: 19.4 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:23 INFO BlockManagerInfo: Added taskresult_127 in memory on ip-172-31-11-79.ec2.internal:41257 (size: 20.9 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:23 INFO BlockManagerInfo: Added taskresult_120 in memory on ip-172-31-11-79.ec2.internal:36177 (size: 20.9 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:23 INFO TaskSetManager: Finished task 16.0 in stage 10.0 (TID 122) in 4013 ms on ip-172-31-11-79.ec2.internal (executor 12) (18/28)\n",
      "20/10/23 04:20:23 INFO BlockManagerInfo: Removed taskresult_122 on ip-172-31-11-79.ec2.internal:41257 in memory (size: 21.0 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:23 INFO BlockManagerInfo: Added taskresult_121 in memory on ip-172-31-11-79.ec2.internal:41257 (size: 21.0 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:23 INFO TaskSetManager: Finished task 14.0 in stage 10.0 (TID 120) in 4112 ms on ip-172-31-11-79.ec2.internal (executor 13) (19/28)\n",
      "20/10/23 04:20:23 INFO BlockManagerInfo: Removed taskresult_120 on ip-172-31-11-79.ec2.internal:36177 in memory (size: 20.9 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:23 INFO TaskSetManager: Finished task 21.0 in stage 10.0 (TID 127) in 3926 ms on ip-172-31-11-79.ec2.internal (executor 12) (20/28)\n",
      "20/10/23 04:20:23 INFO BlockManagerInfo: Removed taskresult_127 on ip-172-31-11-79.ec2.internal:41257 in memory (size: 20.9 MB, free: 5.3 GB)\n",
      "20/10/23 04:20:23 INFO TaskSetManager: Finished task 15.0 in stage 10.0 (TID 121) in 4214 ms on ip-172-31-11-79.ec2.internal (executor 12) (21/28)\n",
      "20/10/23 04:20:23 INFO BlockManagerInfo: Removed taskresult_121 on ip-172-31-11-79.ec2.internal:41257 in memory (size: 21.0 MB, free: 5.3 GB)\n"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_most_pickups_per_month_window_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab - 2.9 Run and time the overall pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f194bf466704c7bbfc70032201f8b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reset notebook kernel\n",
    "def ingest_main():\n",
    "    ingest_taxi_data_multi_service(\"s3://nyc-tlc/trip data/yellow_tripdata_2020-01.csv\")\n",
    "    ingest_taxi_lookup(\"s3://nyc-tlc/misc/taxi _zone_lookup.csv\")\n",
    "    get_most_pickups_per_month_window_pyspark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811c0cc089f346b69b2f026d141ee3a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name 'timer_method' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'timer_method' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"ingest_main\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
