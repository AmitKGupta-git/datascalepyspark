{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4531dc327a104a2d93357d05825c76db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Using cached https://files.pythonhosted.org/packages/f2/d9/e8a56bd0953914f60207af4c41bb3947c47ca03577b1fe26258249dd9af7/boto3-1.16.6-py2.py3-none-any.whl\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3)\n",
      "Collecting botocore<1.20.0,>=1.19.6 (from boto3)\n",
      "  Using cached https://files.pythonhosted.org/packages/15/6c/f5b074e14823f250e0a73e53714c1ed80d689d530468936d35a9d336f1dd/botocore-1.19.6-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.4.0,>=0.3.0 (from boto3)\n",
      "  Using cached https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3<1.26,>=1.25.4; python_version != \"3.4\" in /usr/local/lib/python3.7/site-packages (from botocore<1.20.0,>=1.19.6->boto3)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore<1.20.0,>=1.19.6->boto3)\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.6->boto3)\n",
      "Installing collected packages: python-dateutil, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.16.6 botocore-1.19.6 python-dateutil-2.8.1 s3transfer-0.3.3\n",
      "\n",
      "Collecting pandas\n",
      "  Using cached https://files.pythonhosted.org/packages/25/47/22fc373440e144e2111363adaa07abb09ec1f03fbc071b6d9fc0bbf65f68/pandas-1.1.3-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib64/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /mnt/tmp/1603860518087-0/lib/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.1.3\n",
      "\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests)\n",
      "\n",
      "Collecting s3fs\n",
      "  Using cached https://files.pythonhosted.org/packages/a7/58/732ea1c735d725b1cc4cf365ae6326c22569a5e88c8502d13844e91f08ef/s3fs-0.5.1-py3-none-any.whl\n",
      "Collecting aiobotocore>=1.0.1 (from s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/73/25/a81b015035012131056a6b7a339eec052f86f33e35fd91f160e961ea2a5e/aiobotocore-1.1.2-py3-none-any.whl\n",
      "Collecting fsspec>=0.8.0 (from s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/a5/8b/1df260f860f17cb08698170153ef7db672c497c1840dcc8613ce26a8a005/fsspec-0.8.4-py3-none-any.whl\n",
      "Collecting aioitertools>=0.5.1 (from aiobotocore>=1.0.1->s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/83/42/90df27c516ce54fa26964bc4a632ecaf352c7e99574b515255e48b4a7cc7/aioitertools-0.7.0-py3-none-any.whl\n",
      "Collecting botocore<1.17.45,>=1.17.44 (from aiobotocore>=1.0.1->s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/1e/6a/b6490235c01c941a24a86235e2a641e9505cf0ce4b4968d4987573d92bec/botocore-1.17.44-py2.py3-none-any.whl\n",
      "Collecting wrapt>=1.10.10 (from aiobotocore>=1.0.1->s3fs)\n",
      "Collecting aiohttp>=3.3.1 (from aiobotocore>=1.0.1->s3fs)\n",
      "Collecting typing_extensions>=3.7 (from aioitertools>=0.5.1->aiobotocore>=1.0.1->s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/60/7a/e881b5abb54db0e6e671ab088d079c57ce54e8a01a3ca443f561ccadb37e/typing_extensions-3.7.4.3-py3-none-any.whl\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /usr/local/lib/python3.7/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /mnt/tmp/1603860518087-0/lib/python3.7/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs)\n",
      "Collecting docutils<0.16,>=0.10 (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "Collecting async-timeout<4.0,>=3.0 (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "Collecting attrs>=17.3.0 (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/14/df/479736ae1ef59842f512548bacefad1abed705e400212acba43f9b0fa556/attrs-20.2.0-py2.py3-none-any.whl\n",
      "Collecting idna-ssl>=1.0 (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "Installing collected packages: typing-extensions, aioitertools, docutils, botocore, wrapt, multidict, yarl, async-timeout, attrs, idna-ssl, aiohttp, aiobotocore, fsspec, s3fs\n",
      "  Found existing installation: botocore 1.19.6\n",
      "    Uninstalling botocore-1.19.6:\n",
      "      Successfully uninstalled botocore-1.19.6\n",
      "Successfully installed aiobotocore-1.1.2 aiohttp-3.7.2 aioitertools-0.7.0 async-timeout-3.0.1 attrs-20.2.0 botocore-1.17.44 docutils-0.15.2 fsspec-0.8.4 idna-ssl-1.1.0 multidict-5.0.0 s3fs-0.5.1 typing-extensions-3.7.4.3 wrapt-1.12.1 yarl-1.6.2\n",
      "\n",
      "Requirement already satisfied: fsspec in /mnt/tmp/1603860518087-0/lib/python3.7/site-packages"
     ]
    }
   ],
   "source": [
    "# Install libraries within the notebook scope\n",
    "sc.install_pypi_package(\"boto3\")\n",
    "sc.install_pypi_package(\"pandas\")\n",
    "sc.install_pypi_package(\"requests\")\n",
    "sc.install_pypi_package(\"s3fs\")\n",
    "sc.install_pypi_package(\"fsspec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198021845b6147509a4f5dc6846f1876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import boto3\n",
    "from datetime import datetime\n",
    "import fsspec\n",
    "import pandas as pd\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql import functions as f, types as t, Window\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "import s3fs\n",
    "import subprocess\n",
    "import timeit\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Removes truncation of columns, column values in Pandas\n",
    "# by default\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "# Monkey patching the DataFrame transform method for Spark 2.4\n",
    "# This is available by default in Spark 3.0\n",
    "def transform(self, f):\n",
    "    return f(self)\n",
    "DataFrame.transform = transform\n",
    "\n",
    "# Override the timeit template to return the command's\n",
    "# return value in addition to the time\n",
    "# Reference: https://stackoverflow.com/questions/24812253/how-can-i-capture-return-value-with-python-timeit-module\n",
    "timeit.template = \"\"\"\n",
    "def inner(_it, _timer{init}):\n",
    "    {setup}\n",
    "    _t0 = _timer()\n",
    "    for _i in _it:\n",
    "        retval = {stmt}\n",
    "    _t1 = _timer()\n",
    "    return _t1 - _t0, retval\n",
    "\"\"\"\n",
    "\n",
    "def shell_cmd(cmd):\n",
    "    \"\"\"\n",
    "    Wrapper for running shell commands and printing the output\n",
    "    Some helpful recipes:\n",
    "    - List files on hdfs: shell_cmd(\"hdfs dfs -ls hdfs:///tmp/data/\")\n",
    "    - Remove files from hdfs: shell_cmd(\"hdfs dfs -rm -r hdfs:///tmp/data/test_pyspark\")\n",
    "    \"\"\"\n",
    "    for line in subprocess.check_output(cmd, shell=True).split(b'\\n'):\n",
    "        print(line)\n",
    "\n",
    "def timer_method(cmd):\n",
    "    \"\"\"\n",
    "    Wrapper for timeit that returns the value of a function and its runtime\n",
    "    To use, pass a string of the function you wish to time\n",
    "    Example: \n",
    "     run_time, result = timer_method(\"myfunction(arg1, arg2)\")\n",
    "    \"\"\"\n",
    "    # Setting globals = globals() enables the timeit function\n",
    "    # to return the value generated by cmd\n",
    "    return timeit.timeit(cmd, number=1, globals = globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set your s3 bucket name\n",
    "This should be data-scale-oreilly-{your name}   \n",
    "If you dont remember check the [S3 console](https://s3.console.aws.amazon.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1655f85238414fcea2be46afa0020e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MY_BUCKET_NAME = \"data-scale-oreilly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting from an S3 bucket - NYC Taxi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "* Taxi data \n",
    "* Data dictionaries \n",
    "* Taxi zone lookup table\n",
    "\n",
    "Data ingestion has the ultimate goal of collecting, aggregating, and surfacing data for a specific purpose; an analysis, an API, a dashboard, etc. Think about how you might use the taxi data to answer the following questions:\n",
    "\n",
    "1. Which borough is the most popular pickup or drop off spot?\n",
    "1. Are green taxis more popular for trips within the same borough vs yellow taxis?\n",
    "1. Build a recommendation engine that predicts surge pricing for a given time of day based on historical data  \n",
    "\n",
    "With this in mind, lets work through bringing this data onto the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53962a40737e492db26879b5b3067198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note, if you copy the link from the taxi data website you will see:\n",
    "# https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-01.csv\n",
    "# Two things - first, the portion of the URL following \"aws.com\" is the \n",
    "# bucket name. Second, in \"trip+data\" the \"+\" is a space\n",
    "taxi_data_path = \"s3://nyc-tlc/trip data/yellow_tripdata_2020-01.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614ca1c937ef4fbb93e74b64e8107c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  VendorID tpep_pickup_datetime tpep_dropoff_datetime passenger_count  \\\n",
      "0        1  2020-01-01 00:28:15   2020-01-01 00:33:03               1   \n",
      "1        1  2020-01-01 00:35:39   2020-01-01 00:43:04               1   \n",
      "2        1  2020-01-01 00:47:41   2020-01-01 00:53:52               1   \n",
      "3        1  2020-01-01 00:55:23   2020-01-01 01:00:14               1   \n",
      "4        2  2020-01-01 00:01:58   2020-01-01 00:04:16               1   \n",
      "\n",
      "   trip_distance RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
      "0            1.2          1                  N           238           239   \n",
      "1            1.2          1                  N           239           238   \n",
      "2            0.6          1                  N           238           238   \n",
      "3            0.8          1                  N           238           151   \n",
      "4            0.0          1                  N           193           193   \n",
      "\n",
      "  payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
      "0            1          6.0    3.0      0.5        1.47           0.0   \n",
      "1            1          7.0    3.0      0.5        1.50           0.0   \n",
      "2            1          6.0    3.0      0.5        1.00           0.0   \n",
      "3            1          5.5    0.5      0.5        1.36           0.0   \n",
      "4            2          3.5    0.5      0.5        0.00           0.0   \n",
      "\n",
      "   improvement_surcharge  total_amount  congestion_surcharge  \n",
      "0                    0.3         11.27                   2.5  \n",
      "1                    0.3         12.30                   2.5  \n",
      "2                    0.3         10.80                   2.5  \n",
      "3                    0.3          8.16                   0.0  \n",
      "4                    0.3          4.80                   0.0  \n",
      "VendorID                  object\n",
      "tpep_pickup_datetime      object\n",
      "tpep_dropoff_datetime     object\n",
      "passenger_count           object\n",
      "trip_distance            float64\n",
      "RatecodeID                object\n",
      "store_and_fwd_flag        object\n",
      "PULocationID               int64\n",
      "DOLocationID               int64\n",
      "payment_type              object\n",
      "fare_amount              float64\n",
      "extra                    float64\n",
      "mta_tax                  float64\n",
      "tip_amount               float64\n",
      "tolls_amount             float64\n",
      "improvement_surcharge    float64\n",
      "total_amount             float64\n",
      "congestion_surcharge     float64\n",
      "dtype: object"
     ]
    }
   ],
   "source": [
    "# Pandas uses s3fs to read_csv from s3:\n",
    "pd_df_taxi= pd.read_csv(taxi_data_path, keep_default_na=False)\n",
    "print(pd_df_taxi.head())\n",
    "pd_df_taxi.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8acdcc1421844a0ad3ecaa486c04aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|       1| 2020-01-01 00:28:15|  2020-01-01 00:33:03|              1|          1.2|         1|                 N|         238|         239|           1|        6.0|  3.0|    0.5|      1.47|         0.0|                  0.3|       11.27|                 2.5|\n",
      "|       1| 2020-01-01 00:35:39|  2020-01-01 00:43:04|              1|          1.2|         1|                 N|         239|         238|           1|        7.0|  3.0|    0.5|       1.5|         0.0|                  0.3|        12.3|                 2.5|\n",
      "|       1| 2020-01-01 00:47:41|  2020-01-01 00:53:52|              1|          0.6|         1|                 N|         238|         238|           1|        6.0|  3.0|    0.5|       1.0|         0.0|                  0.3|        10.8|                 2.5|\n",
      "|       1| 2020-01-01 00:55:23|  2020-01-01 01:00:14|              1|          0.8|         1|                 N|         238|         151|           1|        5.5|  0.5|    0.5|      1.36|         0.0|                  0.3|        8.16|                 0.0|\n",
      "|       2| 2020-01-01 00:01:58|  2020-01-01 00:04:16|              1|          0.0|         1|                 N|         193|         193|           2|        3.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         4.8|                 0.0|\n",
      "|       2| 2020-01-01 00:09:44|  2020-01-01 00:10:37|              1|         0.03|         1|                 N|           7|         193|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         3.8|                 0.0|\n",
      "|       2| 2020-01-01 00:39:25|  2020-01-01 00:39:29|              1|          0.0|         1|                 N|         193|         193|           1|        2.5|  0.5|    0.5|      0.01|         0.0|                  0.3|        3.81|                 0.0|\n",
      "|       2| 2019-12-18 15:27:49|  2019-12-18 15:28:59|              1|          0.0|         5|                 N|         193|         193|           1|       0.01|  0.0|    0.0|       0.0|         0.0|                  0.3|        2.81|                 2.5|\n",
      "|       2| 2019-12-18 15:30:35|  2019-12-18 15:31:35|              4|          0.0|         1|                 N|         193|         193|           1|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         6.3|                 2.5|\n",
      "|       1| 2020-01-01 00:29:01|  2020-01-01 00:40:28|              2|          0.7|         1|                 N|         246|          48|           1|        8.0|  3.0|    0.5|      2.35|         0.0|                  0.3|       14.15|                 2.5|\n",
      "|       1| 2020-01-01 00:55:11|  2020-01-01 01:12:03|              2|          2.4|         1|                 N|         246|          79|           1|       12.0|  3.0|    0.5|      1.75|         0.0|                  0.3|       17.55|                 2.5|\n",
      "|       1| 2020-01-01 00:37:15|  2020-01-01 00:51:41|              1|          0.8|         1|                 N|         163|         161|           2|        9.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        13.3|                 2.5|\n",
      "|       1| 2020-01-01 00:56:27|  2020-01-01 01:21:44|              1|          3.3|         1|                 N|         161|         144|           1|       17.0|  3.0|    0.5|      4.15|         0.0|                  0.3|       24.95|                 2.5|\n",
      "|       2| 2020-01-01 00:21:54|  2020-01-01 00:27:31|              1|         1.07|         1|                 N|          43|         239|           1|        6.0|  0.5|    0.5|      1.96|         0.0|                  0.3|       11.76|                 2.5|\n",
      "|       2| 2020-01-01 00:38:01|  2020-01-01 01:15:21|              1|         7.76|         1|                 N|         143|          25|           1|       28.5|  0.5|    0.5|      4.84|         0.0|                  0.3|       37.14|                 2.5|\n",
      "|       1| 2020-01-01 00:15:35|  2020-01-01 00:27:06|              3|          1.6|         1|                 N|         211|         234|           2|        9.0|  3.0|    0.5|       0.0|         0.0|                  0.3|        12.8|                 2.5|\n",
      "|       1| 2020-01-01 00:41:20|  2020-01-01 00:44:22|              1|          0.5|         1|                 Y|         234|          90|           1|        4.0|  3.0|    0.5|       1.0|         0.0|                  0.3|         8.8|                 2.5|\n",
      "|       1| 2020-01-01 00:56:38|  2020-01-01 01:13:34|              1|          1.7|         1|                 N|         246|         142|           2|       11.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        15.3|                 2.5|\n",
      "|       2| 2020-01-01 00:08:21|  2020-01-01 00:25:29|              1|         8.45|         1|                 N|         138|         216|           2|       24.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        25.8|                 0.0|\n",
      "|       1| 2020-01-01 00:25:39|  2020-01-01 00:27:05|              1|          0.0|         1|                 N|         170|         162|           4|        3.0|  3.0|    0.5|       0.0|         0.0|                  0.3|         6.8|                 2.5|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "# For reference, look at the Spark DataFrameReader, csv:\n",
    "# https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html\n",
    "ps_df_taxi = spark.read.option('header', True).option('inferSchema', True).csv(taxi_data_path)\n",
    "ps_df_taxi.show()\n",
    "ps_df_taxi.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abfabd2deea44663a8ce33ff9d11a6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Talk through ingest practices around retaining original data vs augmenting\n",
    "# For example, we may want to keep the data in its default format so we can\n",
    "# refer back to it if there are bugs in our data ingestion code\n",
    "ps_df_taxi.write.option(\"header\", True).csv(\"hdfs:///tmp/input/taxi_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e030bda0e7e40f78e595dae5b8196dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Found 6 items'\n",
      "b'-rw-r--r--   1 livy hadoop          0 2020-10-28 02:53 hdfs:///tmp/input/taxi_data/_SUCCESS'\n",
      "b'-rw-r--r--   1 livy hadoop  154591771 2020-10-28 02:53 hdfs:///tmp/input/taxi_data/part-00000-935fbf80-41e9-4477-8419-94a78c1e7e3d-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop  154402085 2020-10-28 02:53 hdfs:///tmp/input/taxi_data/part-00001-935fbf80-41e9-4477-8419-94a78c1e7e3d-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop  154437757 2020-10-28 02:53 hdfs:///tmp/input/taxi_data/part-00002-935fbf80-41e9-4477-8419-94a78c1e7e3d-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop  154401471 2020-10-28 02:53 hdfs:///tmp/input/taxi_data/part-00003-935fbf80-41e9-4477-8419-94a78c1e7e3d-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   65977667 2020-10-28 02:53 hdfs:///tmp/input/taxi_data/part-00004-935fbf80-41e9-4477-8419-94a78c1e7e3d-c000.csv'\n",
      "b''"
     ]
    }
   ],
   "source": [
    "# Discuss how spark writes files out\n",
    "shell_cmd(\"hdfs dfs -ls hdfs:///tmp/input/taxi_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6e7c02e31d4636be702f50e258e689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|PULocationID|DOLocationID|fare_amount|tip_amount|\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+\n",
      "| 2020-01-01 00:28:15|  2020-01-01 00:33:03|              1|          1.2|         238|         239|        6.0|      1.47|\n",
      "| 2020-01-01 00:35:39|  2020-01-01 00:43:04|              1|          1.2|         239|         238|        7.0|       1.5|\n",
      "| 2020-01-01 00:47:41|  2020-01-01 00:53:52|              1|          0.6|         238|         238|        6.0|       1.0|\n",
      "| 2020-01-01 00:55:23|  2020-01-01 01:00:14|              1|          0.8|         238|         151|        5.5|      1.36|\n",
      "| 2020-01-01 00:01:58|  2020-01-01 00:04:16|              1|          0.0|         193|         193|        3.5|       0.0|\n",
      "| 2020-01-01 00:09:44|  2020-01-01 00:10:37|              1|         0.03|           7|         193|        2.5|       0.0|\n",
      "| 2020-01-01 00:39:25|  2020-01-01 00:39:29|              1|          0.0|         193|         193|        2.5|      0.01|\n",
      "| 2019-12-18 15:27:49|  2019-12-18 15:28:59|              1|          0.0|         193|         193|       0.01|       0.0|\n",
      "| 2019-12-18 15:30:35|  2019-12-18 15:31:35|              4|          0.0|         193|         193|        2.5|       0.0|\n",
      "| 2020-01-01 00:29:01|  2020-01-01 00:40:28|              2|          0.7|         246|          48|        8.0|      2.35|\n",
      "| 2020-01-01 00:55:11|  2020-01-01 01:12:03|              2|          2.4|         246|          79|       12.0|      1.75|\n",
      "| 2020-01-01 00:37:15|  2020-01-01 00:51:41|              1|          0.8|         163|         161|        9.5|       0.0|\n",
      "| 2020-01-01 00:56:27|  2020-01-01 01:21:44|              1|          3.3|         161|         144|       17.0|      4.15|\n",
      "| 2020-01-01 00:21:54|  2020-01-01 00:27:31|              1|         1.07|          43|         239|        6.0|      1.96|\n",
      "| 2020-01-01 00:38:01|  2020-01-01 01:15:21|              1|         7.76|         143|          25|       28.5|      4.84|\n",
      "| 2020-01-01 00:15:35|  2020-01-01 00:27:06|              3|          1.6|         211|         234|        9.0|       0.0|\n",
      "| 2020-01-01 00:41:20|  2020-01-01 00:44:22|              1|          0.5|         234|          90|        4.0|       1.0|\n",
      "| 2020-01-01 00:56:38|  2020-01-01 01:13:34|              1|          1.7|         246|         142|       11.5|       0.0|\n",
      "| 2020-01-01 00:08:21|  2020-01-01 00:25:29|              1|         8.45|         138|         216|       24.5|       0.0|\n",
      "| 2020-01-01 00:25:39|  2020-01-01 00:27:05|              1|          0.0|         170|         162|        3.0|       0.0|\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "column_subset = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID', 'fare_amount', 'tip_amount']\n",
    "ps_df_taxi.select(*column_subset).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dff1be2fd904580ab1b68dad15f126b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+-----------------+------------------+------------------+\n",
      "|summary|   passenger_count|    trip_distance|      PULocationID|     DOLocationID|       fare_amount|        tip_amount|\n",
      "+-------+------------------+-----------------+------------------+-----------------+------------------+------------------+\n",
      "|  count|           6339567|          6405008|           6405008|          6405008|           6405008|           6405008|\n",
      "|   mean|1.5153326717739555|2.929643933309735|164.73225778952968|162.6626908194338|12.694108119770615|2.1893418306433965|\n",
      "| stddev| 1.151594213427813| 83.1591059732502| 65.54373944111758|69.91260629496094|12.127295340046553| 2.760028392378395|\n",
      "|    min|                 0|           -30.62|                 1|                1|           -1238.0|             -91.0|\n",
      "|    max|                 9|        210240.07|               265|              265|            4265.0|            1100.0|\n",
      "+-------+------------------+-----------------+------------------+-----------------+------------------+------------------+"
     ]
    }
   ],
   "source": [
    "ps_df_taxi.select(*column_subset).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.1 - Write an ingestion function that does the following:\n",
    "Given a file path to a taxi data csv (i.e. s3://nyc-tlc/trip data/green_tripdata_2020-01.csv) create a function that does the following:\n",
    "1. Read the file into a Spark dataframe\n",
    "1. Limit to the `column_subset` columns\n",
    "1. Write the data as json to hdfs in append mode to `hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json`\n",
    "\n",
    "Function signature:  \n",
    "`def ingest_taxi_data(file_name)`\n",
    "\n",
    "See the subsequent cell for more info on how the `ingest_taxi_data` function will be used   \n",
    "Reference: https://spark.apache.org/docs/2.4.5/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter  \n",
    "\n",
    "When you're done, run the next 2 cells to ingest several taxi data files and examine the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6fef523c0244d418d0ba0e73f4248c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ingest_taxi_data(file_name):\n",
    "    # Enclosing code in () allows multi line\n",
    "    (spark\n",
    "         .read\n",
    "         .option('header', True)\n",
    "         .option(\"inferSchema\", True)\n",
    "         .csv(file_name)\n",
    "         .select(*column_subset)           \n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d7c36aac354fab996f223f38765d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the ingest for several files\n",
    "taxi_data_prefix = \"s3://nyc-tlc/trip data\"\n",
    "taxi_data_files = [\"yellow_tripdata_2019-01.csv\", \"yellow_tripdata_2018-01.csv\"]\n",
    "for file_name in taxi_data_files: \n",
    "    taxi_data_path = f\"{taxi_data_prefix}/{file_name}\"\n",
    "    ingest_taxi_data(taxi_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f11bf2959584ac996e176aff56514a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      "\n",
      "+------------+------------+-----------+---------------+----------+------------------------+------------------------+-------------+\n",
      "|DOLocationID|PULocationID|fare_amount|passenger_count|tip_amount|tpep_dropoff_datetime   |tpep_pickup_datetime    |trip_distance|\n",
      "+------------+------------+-----------+---------------+----------+------------------------+------------------------+-------------+\n",
      "|239         |151         |7.0        |1              |1.65      |2019-01-01T00:53:20.000Z|2019-01-01T00:46:40.000Z|1.5          |\n",
      "|246         |239         |14.0       |1              |1.0       |2019-01-01T01:18:59.000Z|2019-01-01T00:59:47.000Z|2.6          |\n",
      "|236         |236         |4.5        |3              |0.0       |2018-12-21T13:52:40.000Z|2018-12-21T13:48:30.000Z|0.0          |\n",
      "|193         |193         |3.5        |5              |0.0       |2018-11-28T15:55:45.000Z|2018-11-28T15:52:25.000Z|0.0          |\n",
      "|193         |193         |52.0       |5              |0.0       |2018-11-28T15:58:33.000Z|2018-11-28T15:56:57.000Z|0.0          |\n",
      "+------------+------------+-----------+---------------+----------+------------------------+------------------------+-------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# How did the types fare in this conversion?\n",
    "# Turns out its a bug! \n",
    "# https://issues.apache.org/jira/browse/SPARK-26325\n",
    "# https://stackoverflow.com/questions/53697388/interpret-timestamp-fields-in-spark-while-reading-json\n",
    "df = spark.read.json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "df.printSchema()\n",
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming data types \n",
    "\n",
    "Available pyspark types are listed in the pyspark.sql.types module https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#module-pyspark.sql.types\n",
    "\n",
    "pyspark.types is imported as t, so to apply the IntegerType use t.IntegerType()\n",
    "\n",
    "For pandas, see the following resources on converting types https://stackoverflow.com/questions/15891038/change-column-type-in-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a623e5a6448c4f4eba18f1229acbdd6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tpep_dropoff_datetime', 'timestamp')]"
     ]
    }
   ],
   "source": [
    "# Pyspark\n",
    "(df.select(\"tpep_dropoff_datetime\")\n",
    " .withColumn(\"tpep_dropoff_datetime\", f.col(\"tpep_dropoff_datetime\").cast(t.TimestampType()))\n",
    ").dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c7b2d7f77d4ab8ac596396d7038c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid literal for int() with base 10: ''\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/1603853034423-0/lib/python3.7/site-packages/pandas/core/generic.py\", line 5531, in astype\n",
      "    col.astype(dtype=dtype[col_name], copy=copy, errors=errors)\n",
      "  File \"/tmp/1603853034423-0/lib/python3.7/site-packages/pandas/core/generic.py\", line 5546, in astype\n",
      "    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors,)\n",
      "  File \"/tmp/1603853034423-0/lib/python3.7/site-packages/pandas/core/internals/managers.py\", line 595, in astype\n",
      "    return self.apply(\"astype\", dtype=dtype, copy=copy, errors=errors)\n",
      "  File \"/tmp/1603853034423-0/lib/python3.7/site-packages/pandas/core/internals/managers.py\", line 406, in apply\n",
      "    applied = getattr(b, f)(**kwargs)\n",
      "  File \"/tmp/1603853034423-0/lib/python3.7/site-packages/pandas/core/internals/blocks.py\", line 595, in astype\n",
      "    values = astype_nansafe(vals1d, dtype, copy=True)\n",
      "  File \"/tmp/1603853034423-0/lib/python3.7/site-packages/pandas/core/dtypes/cast.py\", line 919, in astype_nansafe\n",
      "    return dtype.construct_array_type()._from_sequence(arr, dtype=dtype, copy=copy)\n",
      "  File \"/tmp/1603853034423-0/lib/python3.7/site-packages/pandas/core/arrays/integer.py\", line 378, in _from_sequence\n",
      "    return integer_array(scalars, dtype=dtype, copy=copy)\n",
      "  File \"/tmp/1603853034423-0/lib/python3.7/site-packages/pandas/core/arrays/integer.py\", line 160, in integer_array\n",
      "    values, mask = coerce_to_array(values, dtype=dtype, copy=copy)\n",
      "  File \"/tmp/1603853034423-0/lib/python3.7/site-packages/pandas/core/arrays/integer.py\", line 277, in coerce_to_array\n",
      "    values = safe_cast(values, dtype, copy=False)\n",
      "  File \"/tmp/1603853034423-0/lib/python3.7/site-packages/pandas/core/arrays/integer.py\", line 175, in safe_cast\n",
      "    casted = values.astype(dtype, copy=copy)\n",
      "ValueError: invalid literal for int() with base 10: ''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Casting pandas columns to a type - this will give an error on empty cells\n",
    "(pd_df_taxi[[*column_subset]]\n",
    "        .astype({'passenger_count': 'Int64'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d915764719bf467781332a6965e042b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Dtype()"
     ]
    }
   ],
   "source": [
    "# To convert to Integer using pandas, we have to first deal with the null values\n",
    "# to_numeric with 'coerce' will fill invalid integer values with np.NaN\n",
    "# the Int64 type in later versions of pandas will convert np.NaN to a nullable\n",
    "# integer type: https://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html\n",
    "pd.to_numeric(pd_df_taxi.passenger_count, errors='coerce').astype('Int64').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7dffd0be8a349929307784b87b00e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Modified taxi_data_ingest with transformed timestamps\n",
    "def ingest_taxi_data(file_name):\n",
    "    # Enclosing code in () allows multi line\n",
    "    (spark\n",
    "         .read\n",
    "         .option('header', True)\n",
    "         .option(\"inferSchema\", True)\n",
    "         .csv(file_name)\n",
    "         .select(*column_subset)\n",
    "         .withColumn(\"tpep_pickup_date\", f.col(\"tpep_pickup_datetime\").cast(t.DateType()))\n",
    "         .withColumn(\"tpep_dropoff_date\", f.col(\"tpep_dropoff_datetime\").cast(t.DateType()))\n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c6b07b9fd64915a06b9d02bf264b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json'\n",
      "b''"
     ]
    }
   ],
   "source": [
    "# Remove previous data\n",
    "shell_cmd(\"hdfs dfs -rm -r hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9cb3c85082465590f0286e9d6c9d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+---------------+----------+-----------------+---------------------+----------------+--------------------+-------------+\n",
      "|DOLocationID|PULocationID|fare_amount|passenger_count|tip_amount|tpep_dropoff_date|tpep_dropoff_datetime|tpep_pickup_date|tpep_pickup_datetime|trip_distance|\n",
      "+------------+------------+-----------+---------------+----------+-----------------+---------------------+----------------+--------------------+-------------+\n",
      "|          24|          41|        4.5|              1|       0.0|       2018-01-01| 2018-01-01T00:24:...|      2018-01-01|2018-01-01T00:21:...|          0.5|\n",
      "|         140|         239|       14.0|              1|       0.0|       2018-01-01| 2018-01-01T01:03:...|      2018-01-01|2018-01-01T00:44:...|          2.7|\n",
      "|         141|         262|        6.0|              2|       1.0|       2018-01-01| 2018-01-01T00:14:...|      2018-01-01|2018-01-01T00:08:...|          0.8|\n",
      "|         257|         140|       33.5|              1|       0.0|       2018-01-01| 2018-01-01T00:52:...|      2018-01-01|2018-01-01T00:20:...|         10.2|\n",
      "|         239|         246|       12.5|              2|      2.75|       2018-01-01| 2018-01-01T00:27:...|      2018-01-01|2018-01-01T00:09:...|          2.5|\n",
      "+------------+------------+-----------+---------------+----------+-----------------+---------------------+----------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tpep_dropoff_date: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- tpep_pickup_date: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "taxi_data_prefix = \"s3://nyc-tlc/trip data\"\n",
    "taxi_data_files = [\"yellow_tripdata_2019-01.csv\", \"yellow_tripdata_2018-01.csv\"]\n",
    "for file_name in taxi_data_files: \n",
    "    taxi_data_path = f\"{taxi_data_prefix}/{file_name}\"\n",
    "    ingest_taxi_data(taxi_data_path)\n",
    "    \n",
    "df = spark.read.json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing ingestion code\n",
    "\n",
    "The `ingest_taxi_data` method is not well structured for testing:\n",
    "* Writes to the file system\n",
    "* Requires an input file to test\n",
    "* What other shortcomings?\n",
    "\n",
    "To make this code more testable, split out the transformation logic so it can be unit tested.  \n",
    "Definining a transformation function that takes a dataframe and returns a dataframe provides a better interface for unit testing, and a more extensible structure in case we need to add more dataframe functions before or after the transformation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5f5f0b84824986a4ac966754702ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform_taxi_data(df):\n",
    "    return (df\n",
    "            .withColumn(\"tpep_pickup_date\", f.col(\"tpep_pickup_datetime\").cast(t.DateType()))\n",
    "            .withColumn(\"tpep_dropoff_date\", f.col(\"tpep_dropoff_datetime\").cast(t.DateType()))\n",
    "           )\n",
    "\n",
    "# Option 1\n",
    "def ingest_taxi_data_method(file_name):\n",
    "    df_input = (spark\n",
    "         .read\n",
    "         .option('header', True).csv(taxi_data_path)\n",
    "         .select(*column_subset))\n",
    "    \n",
    "    (transform_taxi_data(df_input)\n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )\n",
    "\n",
    "# Option 2\n",
    "def ingest_taxi_data_transform(file_name):\n",
    "    # Requires patching of Dataframe.transform method in Spark 2.4, but available natively\n",
    "    # in Spark 3.0 https://mungingdata.com/pyspark/chaining-dataframe-transformations/\n",
    "    df_input = (spark\n",
    "         .read\n",
    "         .option('header', True).csv(taxi_data_path)\n",
    "         .select(*column_subset)\n",
    "         .transform(transform_taxi_data)\n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ed70e87be244649d2552b218d4fee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tpep_dropoff_datetime', 'string'), ('tpep_pickup_datetime', 'string')]\n",
      "True\n",
      "root\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_pickup_date: date (nullable = true)\n",
      " |-- tpep_dropoff_date: date (nullable = true)"
     ]
    }
   ],
   "source": [
    "test_data = [\n",
    "    \"{'tpep_pickup_datetime': '2020-05-23', 'tpep_dropoff_datetime': '2020-05-23'}\",\n",
    "    \"{'tpep_pickup_datetime': '2020-10-01', 'tpep_dropoff_datetime': '2020-10-01'}\",\n",
    "    \"{'tpep_pickup_datetime': '2020-02-02', 'tpep_dropoff_datetime': '2020-02-03'}\"\n",
    "]\n",
    "expected_types = {'tpep_dropoff_date': 'date', 'tpep_pickup_date': 'date', 'tpep_pickup_datetime':'string', 'tpep_dropoff_datetime':'string'}\n",
    "\n",
    "test_df = spark.read.json(sc.parallelize(test_data))\n",
    "print(test_df.dtypes)\n",
    "test = transform_taxi_data(test_df)\n",
    "test_types = {item[0]:item[1] for item in test.dtypes}\n",
    "\n",
    "print(expected_types == test_types)\n",
    "\n",
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets try running the ingestion code on the other taxi data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b803e1f253884ee080880b936fa650e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"cannot resolve '`tpep_pickup_datetime`' given input columns: [passenger_count, mta_tax, tolls_amount, trip_distance, trip_type, payment_type, total_amount, ehail_fee, fare_amount, congestion_surcharge, lpep_dropoff_datetime, tip_amount, extra, DOLocationID, VendorID, lpep_pickup_datetime, store_and_fwd_flag, improvement_surcharge, RatecodeID, PULocationID];;\\n'Project ['tpep_pickup_datetime, 'tpep_dropoff_datetime, passenger_count#1938, trip_distance#1939, PULocationID#1936, DOLocationID#1937, fare_amount#1940, tip_amount#1943]\\n+- Relation[VendorID#1931,lpep_pickup_datetime#1932,lpep_dropoff_datetime#1933,store_and_fwd_flag#1934,RatecodeID#1935,PULocationID#1936,DOLocationID#1937,passenger_count#1938,trip_distance#1939,fare_amount#1940,extra#1941,mta_tax#1942,tip_amount#1943,tolls_amount#1944,ehail_fee#1945,improvement_surcharge#1946,total_amount#1947,payment_type#1948,trip_type#1949,congestion_surcharge#1950] csv\\n\"\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 27, in ingest_taxi_data_transform\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 1326, in select\n",
      "    jdf = self._jdf.select(self._jcols(*cols))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: \"cannot resolve '`tpep_pickup_datetime`' given input columns: [passenger_count, mta_tax, tolls_amount, trip_distance, trip_type, payment_type, total_amount, ehail_fee, fare_amount, congestion_surcharge, lpep_dropoff_datetime, tip_amount, extra, DOLocationID, VendorID, lpep_pickup_datetime, store_and_fwd_flag, improvement_surcharge, RatecodeID, PULocationID];;\\n'Project ['tpep_pickup_datetime, 'tpep_dropoff_datetime, passenger_count#1938, trip_distance#1939, PULocationID#1936, DOLocationID#1937, fare_amount#1940, tip_amount#1943]\\n+- Relation[VendorID#1931,lpep_pickup_datetime#1932,lpep_dropoff_datetime#1933,store_and_fwd_flag#1934,RatecodeID#1935,PULocationID#1936,DOLocationID#1937,passenger_count#1938,trip_distance#1939,fare_amount#1940,extra#1941,mta_tax#1942,tip_amount#1943,tolls_amount#1944,ehail_fee#1945,improvement_surcharge#1946,total_amount#1947,payment_type#1948,trip_type#1949,congestion_surcharge#1950] csv\\n\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try using the ingest code we created for yellow taxi for all the taxis\n",
    "# This will fail because the datetime fields have different names across different servcies\n",
    "\n",
    "taxi_data_prefix = \"s3://nyc-tlc/trip data/\"\n",
    "taxi_data_files = [\"green_tripdata_2020-01.csv\", \"fhv_tripdata_2020-01.csv\", \"fhvhv_tripdata_2020-01.csv\"]\n",
    "for file_name in taxi_data_files: \n",
    "    taxi_data_path = f\"{taxi_data_prefix}{file_name}\"\n",
    "    ingest_taxi_data_transform(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can we ingest all taxi services AND be able to tell them apart?\n",
    "\n",
    "Taxi file names: \n",
    "* yellow_tripdata_2020-01.csv\n",
    "* green_tripdata_2020-01.csv\n",
    "* fhv_tripdata_2020-01.csv\n",
    "* fhvhv_tripdata_2020-01.csv\n",
    "\n",
    "The file name provides information including:\n",
    "* Service type (yellow, green, etc)\n",
    "* File date\n",
    "\n",
    "We want to augment the taxi data with this information so we can refer back to it in analysis.\n",
    "\n",
    "Is there other data we might want to augment the raw data with? Some things to consider:\n",
    "* Additional fields that could help with analysis\n",
    "* Metadata, such as when the record was last updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09bb37b9500e42f48fddb29188838cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('yellow', '2020', '01')"
     ]
    }
   ],
   "source": [
    "# Using matched groups, we can extract information from the taxi file names\n",
    "# i.e. yellow_tripdata_2020-01.csv\n",
    "TAXI_DATA_PATTERN = \"(?P<service>[a-zA-Z0-9]+)_tripdata_(?P<year>[0-9]{4})-(?P<month>[0-9]{2}).csv\"\n",
    "\n",
    "def extract_file_info(file_name):\n",
    "    # Returns (service, year, month) given a taxi file name\n",
    "    m = re.match(TAXI_DATA_PATTERN, file_name)\n",
    "    if m is not None:\n",
    "        return (m.group(1), m.group(2), m.group(3))\n",
    "    \n",
    "extract_file_info(\"yellow_tripdata_2020-01.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.2 - Ingesting multiple taxi service types\n",
    "\n",
    "See the [Taxi data website](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) for reference\n",
    "\n",
    "Using the template in the next cell, create the following functions:\n",
    "* Service specific transformations to match the schema below\n",
    "* A general transformation function to apply metadata and other common transformations\n",
    "\n",
    "Schema:\n",
    "\n",
    "* pickup_datetime Timestamp\n",
    "* dropoff_datetime Timestamp\n",
    "* pickup_date Date\n",
    "* dropoff_date Date\n",
    "* passenger_count Integer\n",
    "* fare_amount Float\n",
    "* tip_amount Float\n",
    "* trip_distance Float\n",
    "* PULocationID Integer\n",
    "* DOLocationID Integer\n",
    "\n",
    "Metadata fields:  explore `f.lit` to add these columns\n",
    "* service\n",
    "* year\n",
    "* month\n",
    "\n",
    "Refer to `ingest_taxi_data_multi_service` to see how these functions will be used    \n",
    "\n",
    "You may find some helpful info here: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60c72705d3d4d45a659d023d8b6f654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform_yellow_taxi(df):\n",
    "    subset = ['pickup_datetime', 'dropoff_datetime', 'passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID', 'fare_amount', 'tip_amount']\n",
    "    return (df.withColumnRenamed(\"tpep_pickup_datetime\", \"pickup_datetime\")\n",
    "        .withColumnRenamed(\"tpep_dropoff_datetime\", \"dropoff_datetime\")\n",
    "        .select(*subset)\n",
    "        .withColumn(\"dropoff_date\", f.col(\"dropoff_datetime\").cast(t.DateType()))\n",
    "        .withColumn(\"pickup_date\", f.col(\"pickup_datetime\").cast(t.DateType()))\n",
    "\n",
    "        )\n",
    "        \n",
    "def transform_green_taxi(df):\n",
    "    subset = ['pickup_datetime', 'dropoff_datetime', 'passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID', 'fare_amount', 'tip_amount']\n",
    "    return (df.withColumnRenamed(\"lpep_pickup_datetime\", \"pickup_datetime\")\n",
    "        .withColumnRenamed(\"lpep_dropoff_datetime\", \"dropoff_datetime\")\n",
    "        .select(*subset)\n",
    "        .withColumn(\"dropoff_date\", f.col(\"dropoff_datetime\").cast(t.DateType()))\n",
    "        .withColumn(\"pickup_date\", f.col(\"pickup_datetime\").cast(t.DateType()))\n",
    "        )\n",
    "\n",
    "def transform_fhv(df):\n",
    "    return df.select(*[\"pickup_datetime\", \"dropoff_datetime\", \"PULocationID\", \"DOLocationID\"])\n",
    "\n",
    "def transform_all(df, service, year, month):\n",
    "    return (df.withColumn(\"service\", f.lit(service))\n",
    "         .withColumn(\"year\", f.lit(year))\n",
    "         .withColumn(\"month\", f.lit(month))\n",
    "         .withColumn(\"dropoff_date\", f.col(\"dropoff_datetime\").cast(t.DateType()))\n",
    "         .withColumn(\"pickup_date\", f.col(\"pickup_datetime\").cast(t.DateType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08865f95860f4fb1aaafa8fc86e82f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ingest_taxi_data_multi_service(file_name, ingested_on):\n",
    "    print(f\"Processing {file_name}\")\n",
    "    (service, year, month) = extract_file_info(Path(file_name).name)\n",
    "    input_df = spark.read.option('header', True).option('inferSchema', True).csv(file_name)\n",
    "    \n",
    "    if service == 'yellow':\n",
    "        df_transform = transform_yellow_taxi(input_df)\n",
    "    elif service == 'green':\n",
    "        df_transform = transform_green_taxi(input_df)\n",
    "    else:\n",
    "        # FHV. What happens if there are more taxi services added?\n",
    "        df_transform = transform_fhv(input_df)\n",
    "        \n",
    "    print(df_transform.dtypes)\n",
    "\n",
    "    (transform_all(df_transform, service, year, month)\n",
    "         .withColumn(\"ingested_on\", f.lit(ingest_timestamp))\n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f95588093c842479980ce792246eda5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json'\n",
      "b''"
     ]
    }
   ],
   "source": [
    "shell_cmd(\"hdfs dfs -rm -r hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9998e2e1674d7e80394f851c99e823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4ce9ce94624e0abcb1f48aafdf9ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ingest_timestamp = datetime.strftime(datetime.now(), \"%Y-%m-%d %H:%M:%S%z\")\n",
    "taxi_data_prefix = \"s3://nyc-tlc/trip data/\"\n",
    "taxi_data_files = [\"yellow_tripdata_2020-01.csv\"]#, \"green_tripdata_2020-01.csv\", \"fhv_tripdata_2020-01.csv\", \"fhvhv_tripdata_2020-01.csv\"]\n",
    "for file_name in taxi_data_files: \n",
    "    taxi_data_path = f\"{taxi_data_prefix}{file_name}\"\n",
    "    ingest_taxi_data_multi_service(taxi_data_path, ingest_timestamp)\n",
    "    \n",
    "df_taxi_output = spark.read.json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "df_taxi_output.show(5)\n",
    "df_taxi_output.groupby(\"service\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling bad data\n",
    "How to design for the inevitability of bad data  \n",
    "Reference: https://blog.knoldus.com/apache-spark-handle-corrupt-bad-records/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa6330037f2478aadcdba884d2cab48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bad_data = [\n",
    "    \"{'pickup_datetime': '2020-05-23 21:05:23', 'fare_amount': '0.05'}\",\n",
    "    \"{'pickup_datetime': '2020-05-23 08:05:23', 'fare_amount': '10.05'}\",\n",
    "    \"{'pickup_datetime': '2020-05-23 21:05:23', 'fare_amount}\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeaa8d9c97594bbaa8e1427392754663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-------------------+\n",
      "|     _corrupt_record|fare_amount|    pickup_datetime|\n",
      "+--------------------+-----------+-------------------+\n",
      "|                null|       0.05|2020-05-23 21:05:23|\n",
      "|                null|      10.05|2020-05-23 08:05:23|\n",
      "|{'pickup_datetime...|       null|               null|\n",
      "+--------------------+-----------+-------------------+"
     ]
    }
   ],
   "source": [
    "corrupt_df = spark.read.json(sc.parallelize(bad_data), mode=\"PERMISSIVE\", columnNameOfCorruptRecord=\"_corrupt_record\")\n",
    "corrupt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d164ba2ed9d43579b4d6969a72e8f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|fare_amount|    pickup_datetime|\n",
      "+-----------+-------------------+\n",
      "|       0.05|2020-05-23 21:05:23|\n",
      "|      10.05|2020-05-23 08:05:23|\n",
      "+-----------+-------------------+"
     ]
    }
   ],
   "source": [
    "corrupt_df = spark.read.json(sc.parallelize(bad_data), mode=\"DROPMALFORMED\", columnNameOfCorruptRecord=\"_corrupt_record\")\n",
    "corrupt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256ebef03b6a40deaab80f9c937a9d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o587.json.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 58.0 failed 4 times, most recent failure: Lost task 15.3 in stage 58.0 (TID 737, ip-172-31-5-146.ec2.internal, executor 1): org.apache.spark.SparkException: Malformed records are detected in schema inference. Parse Mode: FAILFAST.\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:66)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:53)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n",
      "\tat scala.collection.Iterator$class.isEmpty(Iterator.scala:331)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceLeftOption(TraversableOnce.scala:203)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceOption(TraversableOnce.scala:210)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1334)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:70)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:50)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: com.fasterxml.jackson.core.JsonParseException: Unexpected end-of-input in field name\n",
      " at [Source: [B@759c1287; line: 1, column: 113]\n",
      "\tat com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1581)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:533)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:470)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseAposName(UTF8StreamJsonParser.java:2153)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleOddName(UTF8StreamJsonParser.java:1998)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseName(UTF8StreamJsonParser.java:1650)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:740)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonUtils$.nextUntil(JacksonUtils.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.inferField(JsonInferSchema.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:57)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:55)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2619)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:55)\n",
      "\t... 24 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.infer(JsonInferSchema.scala:83)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$inferFromDataset$1.apply(JsonDataSource.scala:109)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$inferFromDataset$1.apply(JsonDataSource.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:108)\n",
      "\tat org.apache.spark.sql.DataFrameReader$$anonfun$2.apply(DataFrameReader.scala:440)\n",
      "\tat org.apache.spark.sql.DataFrameReader$$anonfun$2.apply(DataFrameReader.scala:440)\n",
      "\tat scala.Option.getOrElse(Option.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:439)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:420)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:406)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Malformed records are detected in schema inference. Parse Mode: FAILFAST.\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:66)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:53)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n",
      "\tat scala.collection.Iterator$class.isEmpty(Iterator.scala:331)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceLeftOption(TraversableOnce.scala:203)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceOption(TraversableOnce.scala:210)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1334)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:70)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:50)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: com.fasterxml.jackson.core.JsonParseException: Unexpected end-of-input in field name\n",
      " at [Source: UNKNOWN; line: 1, column: 113]\n",
      "\tat com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1581)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:533)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:470)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseAposName(UTF8StreamJsonParser.java:2153)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleOddName(UTF8StreamJsonParser.java:1998)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseName(UTF8StreamJsonParser.java:1650)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:740)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonUtils$.nextUntil(JacksonUtils.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.inferField(JsonInferSchema.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:57)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:55)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2619)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:55)\n",
      "\t... 24 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 286, in json\n",
      "    return self._df(self._jreader.json(jrdd))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o587.json.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 58.0 failed 4 times, most recent failure: Lost task 15.3 in stage 58.0 (TID 737, ip-172-31-5-146.ec2.internal, executor 1): org.apache.spark.SparkException: Malformed records are detected in schema inference. Parse Mode: FAILFAST.\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:66)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:53)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n",
      "\tat scala.collection.Iterator$class.isEmpty(Iterator.scala:331)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceLeftOption(TraversableOnce.scala:203)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceOption(TraversableOnce.scala:210)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1334)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:70)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:50)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: com.fasterxml.jackson.core.JsonParseException: Unexpected end-of-input in field name\n",
      " at [Source: [B@759c1287; line: 1, column: 113]\n",
      "\tat com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1581)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:533)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:470)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseAposName(UTF8StreamJsonParser.java:2153)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleOddName(UTF8StreamJsonParser.java:1998)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseName(UTF8StreamJsonParser.java:1650)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:740)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonUtils$.nextUntil(JacksonUtils.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.inferField(JsonInferSchema.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:57)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:55)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2619)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:55)\n",
      "\t... 24 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.infer(JsonInferSchema.scala:83)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$inferFromDataset$1.apply(JsonDataSource.scala:109)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$inferFromDataset$1.apply(JsonDataSource.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:108)\n",
      "\tat org.apache.spark.sql.DataFrameReader$$anonfun$2.apply(DataFrameReader.scala:440)\n",
      "\tat org.apache.spark.sql.DataFrameReader$$anonfun$2.apply(DataFrameReader.scala:440)\n",
      "\tat scala.Option.getOrElse(Option.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:439)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:420)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:406)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Malformed records are detected in schema inference. Parse Mode: FAILFAST.\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:66)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:53)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n",
      "\tat scala.collection.Iterator$class.isEmpty(Iterator.scala:331)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceLeftOption(TraversableOnce.scala:203)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceOption(TraversableOnce.scala:210)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1334)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:70)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:50)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: com.fasterxml.jackson.core.JsonParseException: Unexpected end-of-input in field name\n",
      " at [Source: UNKNOWN; line: 1, column: 113]\n",
      "\tat com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1581)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:533)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:470)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseAposName(UTF8StreamJsonParser.java:2153)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleOddName(UTF8StreamJsonParser.java:1998)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseName(UTF8StreamJsonParser.java:1650)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:740)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonUtils$.nextUntil(JacksonUtils.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.inferField(JsonInferSchema.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:57)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:55)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2619)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:55)\n",
      "\t... 24 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corrupt_df = spark.read.json(sc.parallelize(bad_data), mode=\"FAILFAST\", columnNameOfCorruptRecord=\"_corrupt_record\")\n",
    "corrupt_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.3 - Write an ingestion for the taxi zone lookup\n",
    "File location - Yes, there is a space between taxi and the '_'  \n",
    "\n",
    "s3://nyc-tlc/misc/taxi _zone_lookup.csv\n",
    "\n",
    "`def ingest_taxi_lookup():`\n",
    "1. Read taxi lookup data, ensuring data types are correct\n",
    "1. Add relevant metadata\n",
    "1. Save to hdfs:///tmp/data/nyc-taxi/zone-lookup/output/section2/json\n",
    "1. What write mode should be used?\n",
    "\n",
    "Refer back to Taxi Data page for more info: https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ebc100dc6541aeb3260f7b1ad19636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def taxi_zone_transform(df):\n",
    "    return df.withColumn(\"ingested_on\", f.lit(ingest_timestamp))\n",
    "\n",
    "def ingest_taxi_lookup(ingest_timestamp):\n",
    "    (spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"s3://nyc-tlc/misc/taxi _zone_lookup.csv\")\n",
    "    .transform(taxi_zone_transform)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .json(\"hdfs:///tmp/data/nyc-taxi/zone-lookup/output/section2/json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22057118583c463fa4b112646ed07cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8.81471834199965, None)"
     ]
    }
   ],
   "source": [
    "ingest_timestamp = datetime.strftime(datetime.now(), \"%Y-%m-%d %H:%M:%S%z\")\n",
    "print(timer_method(\"ingest_taxi_lookup(ingest_timestamp)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.4 - Case Study 1: Month over month, get the total count of of pickups per borough\n",
    "#### Do not blindly run hese cells, you can bork your cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821d2eb9a80546e1b5307d08f848e816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "taxiPath = \"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/\"\n",
    "taxiLookupPath = \"hdfs:///tmp/data/nyc-taxi/zone-lookup/output/section2/json/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ec180fd1614856997f92cf5c280830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'Detected implicit cartesian product for INNER join between logical plans\\nJoin Inner, ((LocationID#1692L = PULocationID#1662L) && (LocationID#1692L = DOLocationID#1661L))\\n:- Filter ((isnotnull(pickup_datetime#1668) && isnotnull(dropoff_datetime#1663)) && ((isnotnull(PULocationID#1662L) && (DOLocationID#1661L = PULocationID#1662L)) && isnotnull(DOLocationID#1661L)))\\n:  +- Relation[DOLocationID#1661L,PULocationID#1662L,dropoff_datetime#1663,fare_amount#1664,ingested_on#1665,month#1666,passenger_count#1667L,pickup_datetime#1668,service#1669,tip_amount#1670,trip_distance#1671,year#1672] json\\n+- Project [LocationID#1692L, Borough#1691 AS PUBorough#1703]\\n   +- Filter isnotnull(LocationID#1692L)\\n      +- Relation[Borough#1691,LocationID#1692L,Zone#1693,ingested_on#1694,service_zone#1695] json\\nand\\nProject [LocationID#1754L, Borough#1753 AS DOBorough#1750]\\n+- Relation[Borough#1753,LocationID#1754L,Zone#1755,ingested_on#1756,service_zone#1757] json\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 381, in show\n",
      "    print(self._jdf.showString(n, 20, vertical))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: 'Detected implicit cartesian product for INNER join between logical plans\\nJoin Inner, ((LocationID#1692L = PULocationID#1662L) && (LocationID#1692L = DOLocationID#1661L))\\n:- Filter ((isnotnull(pickup_datetime#1668) && isnotnull(dropoff_datetime#1663)) && ((isnotnull(PULocationID#1662L) && (DOLocationID#1661L = PULocationID#1662L)) && isnotnull(DOLocationID#1661L)))\\n:  +- Relation[DOLocationID#1661L,PULocationID#1662L,dropoff_datetime#1663,fare_amount#1664,ingested_on#1665,month#1666,passenger_count#1667L,pickup_datetime#1668,service#1669,tip_amount#1670,trip_distance#1671,year#1672] json\\n+- Project [LocationID#1692L, Borough#1691 AS PUBorough#1703]\\n   +- Filter isnotnull(LocationID#1692L)\\n      +- Relation[Borough#1691,LocationID#1692L,Zone#1693,ingested_on#1694,service_zone#1695] json\\nand\\nProject [LocationID#1754L, Borough#1753 AS DOBorough#1750]\\n+- Relation[Borough#1753,LocationID#1754L,Zone#1755,ingested_on#1756,service_zone#1757] json\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join boroughs\n",
    "# Expected error cartesian join. most likely a carryover bug from 2.0\n",
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", \"false\") #<-- default\n",
    "df_taxi = spark.read.json(taxiPath)\n",
    "df_taxi_lookup = spark.read.json(taxiLookupPath)\n",
    "\n",
    "taxi_filtered = (df_taxi\n",
    " .filter(df_taxi.pickup_datetime.isNotNull())\n",
    " .filter(df_taxi.dropoff_datetime.isNotNull()))\n",
    "taxi_pu = (taxi_filtered\n",
    ".join(df_taxi_lookup\n",
    "       .select(\"LocationID\", \"Borough\")\n",
    "       .withColumnRenamed(\"Borough\", \"PUBorough\"), \n",
    "       df_taxi_lookup.LocationID == df_taxi.PULocationID))\n",
    "taxi = (taxi_pu.join(df_taxi_lookup\n",
    "       .select(\"LocationID\", \"Borough\")\n",
    "       .withColumnRenamed(\"Borough\", \"DOBorough\"), \n",
    "       df_taxi_lookup.LocationID == taxi_pu.DOLocationID))\n",
    "taxi_pu.show()\n",
    "taxi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57c41571f1e4ed3a8e6d64cd3a1f5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+--------------------+-----------+-------------------+-----+---------------+-----------+--------------------+-------+----------+-------------+----+----------+---------+\n",
      "|DOLocationID|PULocationID|dropoff_date|    dropoff_datetime|fare_amount|        ingested_on|month|passenger_count|pickup_date|     pickup_datetime|service|tip_amount|trip_distance|year|LocationID|PUBorough|\n",
      "+------------+------------+------------+--------------------+-----------+-------------------+-----+---------------+-----------+--------------------+-------+----------+-------------+----+----------+---------+\n",
      "|         239|         238|  2020-01-01|2020-01-01T00:33:...|        6.0|2020-10-28 04:15:09|   01|              1| 2020-01-01|2020-01-01T00:28:...| yellow|      1.47|          1.2|2020|       238|Manhattan|\n",
      "|         238|         239|  2020-01-01|2020-01-01T00:43:...|        7.0|2020-10-28 04:15:09|   01|              1| 2020-01-01|2020-01-01T00:35:...| yellow|       1.5|          1.2|2020|       239|Manhattan|\n",
      "|         238|         238|  2020-01-01|2020-01-01T00:53:...|        6.0|2020-10-28 04:15:09|   01|              1| 2020-01-01|2020-01-01T00:47:...| yellow|       1.0|          0.6|2020|       238|Manhattan|\n",
      "|         151|         238|  2020-01-01|2020-01-01T01:00:...|        5.5|2020-10-28 04:15:09|   01|              1| 2020-01-01|2020-01-01T00:55:...| yellow|      1.36|          0.8|2020|       238|Manhattan|\n",
      "|         193|         193|  2020-01-01|2020-01-01T00:04:...|        3.5|2020-10-28 04:15:09|   01|              1| 2020-01-01|2020-01-01T00:01:...| yellow|       0.0|          0.0|2020|       193|   Queens|\n",
      "|         193|           7|  2020-01-01|2020-01-01T00:10:...|        2.5|2020-10-28 04:15:09|   01|              1| 2020-01-01|2020-01-01T00:09:...| yellow|       0.0|         0.03|2020|         7|   Queens|\n",
      "|         193|         193|  2020-01-01|2020-01-01T00:39:...|        2.5|2020-10-28 04:15:09|   01|              1| 2020-01-01|2020-01-01T00:39:...| yellow|      0.01|          0.0|2020|       193|   Queens|\n",
      "|         193|         193|  2019-12-18|2019-12-18T15:28:...|       0.01|2020-10-28 04:15:09|   01|              1| 2019-12-18|2019-12-18T15:27:...| yellow|       0.0|          0.0|2020|       193|   Queens|\n",
      "|         193|         193|  2019-12-18|2019-12-18T15:31:...|        2.5|2020-10-28 04:15:09|   01|              4| 2019-12-18|2019-12-18T15:30:...| yellow|       0.0|          0.0|2020|       193|   Queens|\n",
      "|          48|         246|  2020-01-01|2020-01-01T00:40:...|        8.0|2020-10-28 04:15:09|   01|              2| 2020-01-01|2020-01-01T00:29:...| yellow|      2.35|          0.7|2020|       246|Manhattan|\n",
      "|          79|         246|  2020-01-01|2020-01-01T01:12:...|       12.0|2020-10-28 04:15:09|   01|              2| 2020-01-01|2020-01-01T00:55:...| yellow|      1.75|          2.4|2020|       246|Manhattan|\n",
      "|         161|         163|  2020-01-01|2020-01-01T00:51:...|        9.5|2020-10-28 04:15:09|   01|              1| 2020-01-01|2020-01-01T00:37:...| yellow|       0.0|          0.8|2020|       163|Manhattan|\n",
      "|         144|         161|  2020-01-01|2020-01-01T01:21:...|       17.0|2020-10-28 04:15:09|   01|              1| 2020-01-01|2020-01-01T00:56:...| yellow|      4.15|          3.3|2020|       161|Manhattan|\n",
      "|         239|          43|  2020-01-01|2020-01-01T00:27:...|        6.0|2020-10-28 04:15:09|   01|              1| 2020-01-01|2020-01-01T00:21:...| yellow|      1.96|         1.07|2020|        43|Manhattan|\n",
      "|          25|         143|  2020-01-01|2020-01-01T01:15:...|       28.5|2020-10-28 04:15:09|   01|              1| 2020-01-01|2020-01-01T00:38:...| yellow|      4.84|         7.76|2020|       143|Manhattan|\n",
      "|         234|         211|  2020-01-01|2020-01-01T00:27:...|        9.0|2020-10-28 04:15:09|   01|              3| 2020-01-01|2020-01-01T00:15:...| yellow|       0.0|          1.6|2020|       211|Manhattan|\n",
      "|          90|         234|  2020-01-01|2020-01-01T00:44:...|        4.0|2020-10-28 04:15:09|   01|              1| 2020-01-01|2020-01-01T00:41:...| yellow|       1.0|          0.5|2020|       234|Manhattan|\n",
      "|         142|         246|  2020-01-01|2020-01-01T01:13:...|       11.5|2020-10-28 04:15:09|   01|              1| 2020-01-01|2020-01-01T00:56:...| yellow|       0.0|          1.7|2020|       246|Manhattan|\n",
      "|         216|         138|  2020-01-01|2020-01-01T00:25:...|       24.5|2020-10-28 04:15:09|   01|              1| 2020-01-01|2020-01-01T00:08:...| yellow|       0.0|         8.45|2020|       138|   Queens|\n",
      "|         162|         170|  2020-01-01|2020-01-01T00:27:...|        3.0|2020-10-28 04:15:09|   01|              1| 2020-01-01|2020-01-01T00:25:...| yellow|       0.0|          0.0|2020|       170|Manhattan|\n",
      "+------------+------------+------------+--------------------+-----------+-------------------+-----+---------------+-----------+--------------------+-------+----------+-------------+----+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan(isFinalPlan=false)\n",
      "+- BroadcastNestedLoopJoin BuildRight, Inner\n",
      "   :- BroadcastHashJoin [PULocationID#4412L, DOLocationID#4411L], [LocationID#4446L, LocationID#4446L], Inner, BuildRight\n",
      "   :  :- Project [DOLocationID#4411L, PULocationID#4412L, dropoff_date#4413, dropoff_datetime#4414, fare_amount#4415, ingested_on#4416, month#4417, passenger_count#4418L, pickup_date#4419, pickup_datetime#4420, service#4421, tip_amount#4422, trip_distance#4423, year#4424]\n",
      "   :  :  +- Filter ((((isnotnull(pickup_datetime#4420) && isnotnull(dropoff_datetime#4414)) && isnotnull(DOLocationID#4411L)) && isnotnull(PULocationID#4412L)) && (DOLocationID#4411L = PULocationID#4412L))\n",
      "   :  :     +- FileScan json [DOLocationID#4411L,PULocationID#4412L,dropoff_date#4413,dropoff_datetime#4414,fare_amount#4415,ingested_on#4416,month#4417,passenger_count#4418L,pickup_date#4419,pickup_datetime#4420,service#4421,tip_amount#4422,trip_distance#4423,year#4424] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://ip-172-31-11-187.ec2.internal:8020/tmp/data/nyc-taxi/taxi-data/output/se..., PartitionFilters: [], PushedFilters: [IsNotNull(pickup_datetime), IsNotNull(dropoff_datetime), IsNotNull(DOLocationID), IsNotNull(PULo..., ReadSchema: struct<DOLocationID:bigint,PULocationID:bigint,dropoff_date:string,dropoff_datetime:string,fare_a...\n",
      "   :  +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true], input[0, bigint, true]))\n",
      "   :     +- Project [LocationID#4446L, Borough#4445 AS PUBorough#4457]\n",
      "   :        +- Filter isnotnull(LocationID#4446L)\n",
      "   :           +- FileScan json [Borough#4445,LocationID#4446L] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://ip-172-31-11-187.ec2.internal:8020/tmp/data/nyc-taxi/zone-lookup/output/..., PartitionFilters: [], PushedFilters: [IsNotNull(LocationID)], ReadSchema: struct<Borough:string,LocationID:bigint>\n",
      "   +- BroadcastExchange IdentityBroadcastMode\n",
      "      +- Project [LocationID#4514L, Borough#4513 AS DOBorough#4510]\n",
      "         +- FileScan json [Borough#4513,LocationID#4514L] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://ip-172-31-11-187.ec2.internal:8020/tmp/data/nyc-taxi/zone-lookup/output/..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Borough:string,LocationID:bigint>"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")\n",
    "df_taxi = spark.read.json(taxiPath)\n",
    "df_taxi_lookup = spark.read.json(taxiLookupPath)\n",
    "\n",
    "taxi_filtered = (df_taxi\n",
    " .filter(df_taxi.pickup_datetime.isNotNull())\n",
    " .filter(df_taxi.dropoff_datetime.isNotNull()))\n",
    "taxi_pu = (taxi_filtered\n",
    ".join(df_taxi_lookup\n",
    "       .select(\"LocationID\", \"Borough\")\n",
    "       .withColumnRenamed(\"Borough\", \"PUBorough\"), \n",
    "       df_taxi_lookup.LocationID == df_taxi.PULocationID))\n",
    "taxi = (taxi_pu.join(df_taxi_lookup\n",
    "       .select(\"LocationID\", \"Borough\")\n",
    "       .withColumnRenamed(\"Borough\", \"DOBorough\"), \n",
    "       df_taxi_lookup.LocationID == taxi_pu.DOLocationID))\n",
    "taxi_pu.show()\n",
    "taxi.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3799b281aec4f0e9db409dab3c9e41f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_monthly_totals_pyspark(taxiPath, taxiLookupPath):\n",
    "    taxi = spark.read.json(taxiPath)\n",
    "    taxi_lookup = spark.read.json(taxiLookupPath)\n",
    "    taxi_filtered = (taxi\n",
    "     .filter(taxi.pickup_datetime.isNotNull())\n",
    "     .filter(taxi.dropoff_datetime.isNotNull()))\n",
    "                     \n",
    "    groupDF = taxi_filtered.join(taxi_lookup, taxi_filtered.PULocationID == taxi_lookup.LocationID)\n",
    "    groupDF.select(\"ingested_on\").show() # expected error\n",
    "    return groupDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96f363785f84ff5841f071a112896d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"Reference 'ingested_on' is ambiguous, could be: ingested_on, ingested_on.;\"\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 58, in timer_method\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 233, in timeit\n",
      "    return Timer(stmt, setup, timer, globals).timeit(number)\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 177, in timeit\n",
      "    timing = self.inner(it, self.timer)\n",
      "  File \"<timeit-src>\", line 6, in inner\n",
      "  File \"<stdin>\", line 9, in get_monthly_totals_pyspark\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 1326, in select\n",
      "    jdf = self._jdf.select(self._jcols(*cols))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: \"Reference 'ingested_on' is ambiguous, could be: ingested_on, ingested_on.;\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_monthly_totals_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1855ca745c034e6b880e548547832d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_monthly_totals_pandas(taxiPath, taxiLookupPath):\n",
    "    taxi = pd.read_json(taxiPath)\n",
    "    taxi_lookup = pd.read_json(taxiLookupPath)\n",
    "    taxi_filtered = tax.dropna(subset=['pickup_datetime', 'dropoff_datetime'])\n",
    "    \n",
    "    groupDF = taxi_filtered.join(taxi_lookup.set_index('LocationID'), on='PULocationID')\n",
    "    groupDF['pickup_month'] = pd.to_datetime(groupDF['pickup_datetime'], format='%m%Y')\n",
    "    groupDF = groupDF.groupby('pickup_month', 'borough').agg('count').sort_values(by=['count', 'borough'], ascending=[False, True])\n",
    "    groupDF\n",
    "    return groupDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8f0528db01467287c3b70a3d56a7cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyarrow and local java libraries required for HDFS\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 58, in timer_method\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 233, in timeit\n",
      "    return Timer(stmt, setup, timer, globals).timeit(number)\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 177, in timeit\n",
      "    timing = self.inner(it, self.timer)\n",
      "  File \"<timeit-src>\", line 6, in inner\n",
      "  File \"<stdin>\", line 2, in get_monthly_totals_pandas\n",
      "  File \"/tmp/1603838861189-0/lib/python3.7/site-packages/pandas/util/_decorators.py\", line 199, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/tmp/1603838861189-0/lib/python3.7/site-packages/pandas/util/_decorators.py\", line 296, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/tmp/1603838861189-0/lib/python3.7/site-packages/pandas/io/json/_json.py\", line 594, in read_json\n",
      "    path_or_buf, encoding=encoding, compression=compression\n",
      "  File \"/tmp/1603838861189-0/lib/python3.7/site-packages/pandas/io/common.py\", line 222, in get_filepath_or_buffer\n",
      "    filepath_or_buffer, mode=mode or \"rb\", **(storage_options or {})\n",
      "  File \"/tmp/1603838861189-0/lib/python3.7/site-packages/fsspec/core.py\", line 438, in open\n",
      "    **kwargs\n",
      "  File \"/tmp/1603838861189-0/lib/python3.7/site-packages/fsspec/core.py\", line 287, in open_files\n",
      "    expand=expand,\n",
      "  File \"/tmp/1603838861189-0/lib/python3.7/site-packages/fsspec/core.py\", line 600, in get_fs_token_paths\n",
      "    cls = get_filesystem_class(protocol)\n",
      "  File \"/tmp/1603838861189-0/lib/python3.7/site-packages/fsspec/registry.py\", line 204, in get_filesystem_class\n",
      "    raise ImportError(bit[\"err\"]) from e\n",
      "ImportError: pyarrow and local java libraries required for HDFS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_monthly_totals_pandas(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f73e28157df46369954108d45a5bbb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_monthly_totals_pandas(taxiPath, taxiLookupPath):\n",
    "    taxiPySpark = spark.read.json(taxiPath)\n",
    "    taxiLookupPySpark = spark.read.json(taxiLookupPath)\n",
    "    \n",
    "    taxi = taxiPySpark.toPandas()\n",
    "    taxiLookup = taxiLookupPySpark.toPandas()\n",
    "    taxiFiltered = taxi.dropna(subset=['pickup_datetime', 'dropoff_datetime'])\n",
    "    \n",
    "    groupDF = taxiFiltered.join(taxiLookup[[\"Borough\", \"LocationID\"]].set_index('LocationID'), on='PULocationID')\n",
    "    \n",
    "    groupDF['pickup_month'] = pd.to_datetime(groupDF['pickup_datetime']).dt.strftime('%Y%m')\n",
    "    returnGroupDF = groupDF.groupby(['pickup_month', 'Borough']).size().reset_index(name='count').sort_values(by=['pickup_month', 'count', 'Borough'], ascending=[False, False, True])\n",
    "    return returnGroupDF\n",
    "\n",
    "def get_monthly_totals_pyspark(taxiPath, taxiLookupPath):\n",
    "    taxi = spark.read.json(taxiPath)\n",
    "    taxiLookup = spark.read.json(taxiLookupPath)\n",
    "    \n",
    "    taxiFiltered = (taxi\n",
    "     .filter(taxi.pickup_datetime.isNotNull())\n",
    "     .filter(taxi.dropoff_datetime.isNotNull()))\n",
    "                     \n",
    "    groupDF = taxiFiltered.join(taxiLookup.select(\"Borough\", \"LocationID\"), taxiFiltered.PULocationID == taxiLookup.LocationID)\n",
    "    groupDF = groupDF.withColumn(\"pickup_month\", f.date_format(\"pickup_datetime\", \"yyyyMM\"))\n",
    "    groupDF = groupDF.groupBy(\"pickup_month\", \"borough\").count().orderBy(f.desc(\"pickup_month\"), f.desc(\"count\"), \"borough\")\n",
    "    groupDF.show()\n",
    "    return groupDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8739f2be1b0349bba5518f33c8db2d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(460.658796917,    pickup_month        Borough     count\n",
      "31       202101      Manhattan         3\n",
      "30       202007      Manhattan         6\n",
      "29       202006      Manhattan         1\n",
      "28       202005      Manhattan         5\n",
      "27       202004      Manhattan         1\n",
      "26       202003      Manhattan         5\n",
      "23       202002      Manhattan        34\n",
      "24       202002         Queens         9\n",
      "22       202002       Brooklyn         3\n",
      "21       202002          Bronx         1\n",
      "25       202002        Unknown         1\n",
      "17       202001      Manhattan  14817800\n",
      "15       202001       Brooklyn   5628274\n",
      "18       202001         Queens   4509457\n",
      "14       202001          Bronx   2536611\n",
      "20       202001        Unknown   1615247\n",
      "19       202001  Staten Island    260258\n",
      "16       202001            EWR      3779\n",
      "11       201912      Manhattan       129\n",
      "12       201912         Queens        17\n",
      "9        201912          Bronx         2\n",
      "10       201912       Brooklyn         1\n",
      "13       201912        Unknown         1\n",
      "8        201009         Queens         3\n",
      "5        200901      Manhattan        19\n",
      "6        200901         Queens        10\n",
      "4        200901          Bronx         1\n",
      "7        200901        Unknown         1\n",
      "1        200812      Manhattan         8\n",
      "2        200812         Queens         2\n",
      "3        200812        Unknown         1\n",
      "0        200301         Queens         1)"
     ]
    }
   ],
   "source": [
    "# Running this command with the original cluster size, will crash the cluster\n",
    "# All functions utilizing pandas from this command forward, need an upscaled driver node\n",
    "print(timer_method(\"get_monthly_totals_pandas(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e871813ea32c4f26aa0ebc1311d8130c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+--------+\n",
      "|pickup_month|      borough|   count|\n",
      "+------------+-------------+--------+\n",
      "|      202101|    Manhattan|       3|\n",
      "|      202007|    Manhattan|       6|\n",
      "|      202006|    Manhattan|       1|\n",
      "|      202005|    Manhattan|       5|\n",
      "|      202004|    Manhattan|       1|\n",
      "|      202003|    Manhattan|       5|\n",
      "|      202002|    Manhattan|      34|\n",
      "|      202002|       Queens|       9|\n",
      "|      202002|     Brooklyn|       3|\n",
      "|      202002|        Bronx|       1|\n",
      "|      202002|      Unknown|       1|\n",
      "|      202001|    Manhattan|14817800|\n",
      "|      202001|     Brooklyn| 5628274|\n",
      "|      202001|       Queens| 4509457|\n",
      "|      202001|        Bronx| 2536611|\n",
      "|      202001|      Unknown| 1615247|\n",
      "|      202001|Staten Island|  260258|\n",
      "|      202001|          EWR|    3779|\n",
      "|      201912|    Manhattan|     129|\n",
      "|      201912|       Queens|      17|\n",
      "+------------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "(36.60000383999977, DataFrame[pickup_month: string, borough: string, count: bigint])"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_monthly_totals_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Expected error for maxResultSize: This won't work. Could try the subsequent cells\n",
    "## Those restart the state of the notebook and don't work as expected\n",
    "## Need to restart the cluster and edit the Software config with: [{\"classification\":\"spark-defaults\", \"properties\":{\"spark.driver.maxResultSize\":\"5G\", \"spark.ui.killEnabled\":\"true\"}, \"configurations\":[]}]\n",
    "## Then need to reun the taxi and taxi lookup ingests\n",
    "## Run -> Run All Above Selected Cell\n",
    "## Second expected error for {\"msg\":\"requirement failed: Session isn't active.\"} and will hang. Driver node ran out of mem. Will need to go and upscale\n",
    "print(spark.conf.get('spark.driver.maxResultSize'))\n",
    "spark.conf.set(\"spark.driver.maxResultSize\", \"5G\")\n",
    "print(spark.conf.get('spark.driver.maxResultSize'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"conf\":{\"spark.driver.maxResultSize\":\"5G\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce715143cac4d49b474b78acf6185b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_monthly_totals_concat_pandas(taxiPath, taxiLookupPath):\n",
    "    taxiPySpark = spark.read.json(taxiPath)\n",
    "    taxiLookupPySpark = spark.read.json(taxiLookupPath)\n",
    "    \n",
    "    taxi = taxiPySpark.toPandas()\n",
    "    taxiLookup = taxiLookupPySpark.toPandas()\n",
    "    taxiFiltered = taxi.dropna(subset=['pickup_datetime', 'dropoff_datetime'])\n",
    "    \n",
    "    groupDF = taxiFiltered.join(taxiLookup[[\"Borough\", \"LocationID\"]].set_index('LocationID'), on='PULocationID')\n",
    "    groupDF['pickup_month'] = groupDF['year'] + groupDF['month']\n",
    "    groupDF = groupDF.groupby(['pickup_month', 'Borough']).size().reset_index(name='count').sort_values(by=['pickup_month', 'count', 'Borough'], ascending=[False, False, True])\n",
    "    return groupDF\n",
    "    \n",
    "def get_monthly_totals_concat_pyspark(taxiPath, taxiLookupPath):\n",
    "    taxi = spark.read.json(taxiPath)\n",
    "    taxiLookup = spark.read.json(taxiLookupPath)\n",
    "    taxiFiltered = (taxi\n",
    "     .filter(taxi.pickup_datetime.isNotNull())\n",
    "     .filter(taxi.dropoff_datetime.isNotNull()))\n",
    "        \n",
    "    groupDF = taxiFiltered.join(taxiLookup, taxiFiltered.PULocationID == taxiLookup.LocationID)\n",
    "    groupDF = groupDF.withColumn(\"pickup_month\", f.concat(\"year\", \"month\")).select(\"pickup_datetime\", \"borough\", \"pickup_month\")\n",
    "    groupDF = groupDF.groupBy(\"pickup_month\", \"borough\").count().orderBy(f.desc(\"pickup_month\"), f.desc(\"count\"), \"borough\")\n",
    "    groupDF.show()\n",
    "    return groupDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f6132aaab14c94a0dedd026295676e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(322.6805907979999,   pickup_month        Borough     count\n",
      "3       202001      Manhattan  14818011\n",
      "1       202001       Brooklyn   5628278\n",
      "4       202001         Queens   4509499\n",
      "0       202001          Bronx   2536615\n",
      "6       202001        Unknown   1615251\n",
      "5       202001  Staten Island    260258\n",
      "2       202001            EWR      3779)"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_monthly_totals_concat_pandas(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e95d72e9f374e54a221b5eb70980b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+--------+\n",
      "|pickup_month|      borough|   count|\n",
      "+------------+-------------+--------+\n",
      "|      202001|    Manhattan|14818011|\n",
      "|      202001|     Brooklyn| 5628278|\n",
      "|      202001|       Queens| 4509499|\n",
      "|      202001|        Bronx| 2536615|\n",
      "|      202001|      Unknown| 1615251|\n",
      "|      202001|Staten Island|  260258|\n",
      "|      202001|          EWR|    3779|\n",
      "+------------+-------------+--------+\n",
      "\n",
      "(19.413056725000388, DataFrame[pickup_month: string, borough: string, count: bigint])"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_monthly_totals_concat_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.5 - Case Study 2: Month over month, get the borough with the most amount of pickups per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150a105f1d4b40e4bd1b8fc78d27bbfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_most_pickups_per_month_pandas(taxiPath, taxiLookupPath):\n",
    "    inputDF = get_monthly_totals_pandas(taxiPath, taxiLookupPath)\n",
    "    firstDF = inputDF.groupby(\"pickup_month\").head(1).reset_index(drop=True)#.first()#sort_values(by=['pickup_month', 'count'], ascending=[True, False]).head(1).reset_index(drop=True)\n",
    "    firstDF\n",
    "    return firstDF\n",
    "\n",
    "def get_most_pickups_per_month_pyspark(taxiPath, taxiLookupPath):\n",
    "    inputDF = get_monthly_totals_pyspark(taxiPath, taxiLookupPath)\n",
    "    firstDF = inputDF.orderBy(f.desc(\"pickup_month\"), f.desc(\"count\")).groupBy(\"pickup_month\").agg(f.first(\"borough\")).orderBy(f.desc(\"pickup_month\"))\n",
    "    firstDF.explain()\n",
    "    firstDF.show()\n",
    "    return firstDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df3ed4ce4244e4588210c3b7d703b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(519.8932285199999,    pickup_month    Borough     count\n",
      "0        202101  Manhattan         3\n",
      "1        202007  Manhattan         6\n",
      "2        202006  Manhattan         1\n",
      "3        202005  Manhattan         5\n",
      "4        202004  Manhattan         1\n",
      "5        202003  Manhattan         5\n",
      "6        202002  Manhattan        34\n",
      "7        202001  Manhattan  14817800\n",
      "8        201912  Manhattan       129\n",
      "9        201009     Queens         3\n",
      "10       200901  Manhattan        19\n",
      "11       200812  Manhattan         8\n",
      "12       200301     Queens         1)"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_most_pickups_per_month_pandas(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7305763eecf94e248a45f7610053d0b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+--------+\n",
      "|pickup_month|      borough|   count|\n",
      "+------------+-------------+--------+\n",
      "|      202101|    Manhattan|       3|\n",
      "|      202007|    Manhattan|       6|\n",
      "|      202006|    Manhattan|       1|\n",
      "|      202005|    Manhattan|       5|\n",
      "|      202004|    Manhattan|       1|\n",
      "|      202003|    Manhattan|       5|\n",
      "|      202002|    Manhattan|      34|\n",
      "|      202002|       Queens|       9|\n",
      "|      202002|     Brooklyn|       3|\n",
      "|      202002|        Bronx|       1|\n",
      "|      202002|      Unknown|       1|\n",
      "|      202001|    Manhattan|14817800|\n",
      "|      202001|     Brooklyn| 5628274|\n",
      "|      202001|       Queens| 4509457|\n",
      "|      202001|        Bronx| 2536611|\n",
      "|      202001|      Unknown| 1615247|\n",
      "|      202001|Staten Island|  260258|\n",
      "|      202001|          EWR|    3779|\n",
      "|      201912|    Manhattan|     129|\n",
      "|      201912|       Queens|      17|\n",
      "+------------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan(isFinalPlan=false)\n",
      "+- Sort [pickup_month#5052 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(pickup_month#5052 DESC NULLS LAST, 1000)\n",
      "      +- SortAggregate(key=[pickup_month#5052], functions=[first(borough#4992, false)])\n",
      "         +- Sort [pickup_month#5052 ASC NULLS FIRST], false, 0\n",
      "            +- Exchange hashpartitioning(pickup_month#5052, 1000)\n",
      "               +- SortAggregate(key=[pickup_month#5052], functions=[partial_first(borough#4992, false)])\n",
      "                  +- Sort [pickup_month#5052 ASC NULLS FIRST], false, 0\n",
      "                     +- Project [pickup_month#5052, borough#4992]\n",
      "                        +- Sort [pickup_month#5052 DESC NULLS LAST, count#5088L DESC NULLS LAST, borough#4992 ASC NULLS FIRST], true, 0\n",
      "                           +- Exchange rangepartitioning(pickup_month#5052 DESC NULLS LAST, count#5088L DESC NULLS LAST, borough#4992 ASC NULLS FIRST, 1000)\n",
      "                              +- HashAggregate(keys=[pickup_month#5052, borough#4992], functions=[count(1)])\n",
      "                                 +- Exchange hashpartitioning(pickup_month#5052, borough#4992, 1000)\n",
      "                                    +- HashAggregate(keys=[pickup_month#5052, borough#4992], functions=[partial_count(1)])\n",
      "                                       +- Project [Borough#4992, date_format(cast(pickup_datetime#4967 as timestamp), yyyyMM, Some(UTC)) AS pickup_month#5052]\n",
      "                                          +- BroadcastHashJoin [PULocationID#4959L], [LocationID#4993L], Inner, BuildRight\n",
      "                                             :- Project [PULocationID#4959L, pickup_datetime#4967]\n",
      "                                             :  +- Filter ((isnotnull(pickup_datetime#4967) && isnotnull(dropoff_datetime#4961)) && isnotnull(PULocationID#4959L))\n",
      "                                             :     +- FileScan json [PULocationID#4959L,dropoff_datetime#4961,pickup_datetime#4967] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://ip-172-31-11-187.ec2.internal:8020/tmp/data/nyc-taxi/taxi-data/output/se..., PartitionFilters: [], PushedFilters: [IsNotNull(pickup_datetime), IsNotNull(dropoff_datetime), IsNotNull(PULocationID)], ReadSchema: struct<PULocationID:bigint,dropoff_datetime:string,pickup_datetime:string>\n",
      "                                             +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, bigint, true]))\n",
      "                                                +- Project [Borough#4992, LocationID#4993L]\n",
      "                                                   +- Filter isnotnull(LocationID#4993L)\n",
      "                                                      +- FileScan json [Borough#4992,LocationID#4993L] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://ip-172-31-11-187.ec2.internal:8020/tmp/data/nyc-taxi/zone-lookup/output/..., PartitionFilters: [], PushedFilters: [IsNotNull(LocationID)], ReadSchema: struct<Borough:string,LocationID:bigint>\n",
      "+------------+---------------------+\n",
      "|pickup_month|first(borough, false)|\n",
      "+------------+---------------------+\n",
      "|      202101|            Manhattan|\n",
      "|      202007|            Manhattan|\n",
      "|      202006|            Manhattan|\n",
      "|      202005|            Manhattan|\n",
      "|      202004|            Manhattan|\n",
      "|      202003|            Manhattan|\n",
      "|      202002|            Manhattan|\n",
      "|      202001|            Manhattan|\n",
      "|      201912|            Manhattan|\n",
      "|      201009|               Queens|\n",
      "|      200901|              Unknown|\n",
      "|      200812|            Manhattan|\n",
      "|      200301|               Queens|\n",
      "+------------+---------------------+\n",
      "\n",
      "(59.512433668000085, DataFrame[pickup_month: string, first(borough, false): string])"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_most_pickups_per_month_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39b3352e7644dcf9bc77a52bad04801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_most_pickups_per_month_window_pyspark(taxiPath, taxiLookupPath):\n",
    "    from pyspark.sql import Window\n",
    "    inputDF = get_monthly_totals_pyspark(taxiPath, taxiLookupPath)\n",
    "    win = Window.partitionBy(\"pickup_month\").orderBy(f.desc(\"count\"))\n",
    "    firstDF = inputDF.withColumn(\"row_num\", f.row_number().over(win)).where(\"row_num == 1\")\n",
    "    firstDF = firstDF.orderBy(f.desc(\"pickup_month\"))\n",
    "    firstDF.explain()\n",
    "    firstDF.show(firstDF.count())\n",
    "    return firstDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e97d85691e6486e8c5a707efba1d450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+--------+\n",
      "|pickup_month|      borough|   count|\n",
      "+------------+-------------+--------+\n",
      "|      202101|    Manhattan|       3|\n",
      "|      202007|    Manhattan|       6|\n",
      "|      202006|    Manhattan|       1|\n",
      "|      202005|    Manhattan|       5|\n",
      "|      202004|    Manhattan|       1|\n",
      "|      202003|    Manhattan|       5|\n",
      "|      202002|    Manhattan|      34|\n",
      "|      202002|       Queens|       9|\n",
      "|      202002|     Brooklyn|       3|\n",
      "|      202002|        Bronx|       1|\n",
      "|      202002|      Unknown|       1|\n",
      "|      202001|    Manhattan|14817800|\n",
      "|      202001|     Brooklyn| 5628274|\n",
      "|      202001|       Queens| 4509457|\n",
      "|      202001|        Bronx| 2536611|\n",
      "|      202001|      Unknown| 1615247|\n",
      "|      202001|Staten Island|  260258|\n",
      "|      202001|          EWR|    3779|\n",
      "|      201912|    Manhattan|     129|\n",
      "|      201912|       Queens|      17|\n",
      "+------------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan(isFinalPlan=false)\n",
      "+- Sort [pickup_month#5244 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(pickup_month#5244 DESC NULLS LAST, 1000)\n",
      "      +- Filter (isnotnull(row_num#5302) && (row_num#5302 = 1))\n",
      "         +- Window [row_number() windowspecdefinition(pickup_month#5244, count#5280L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#5302], [pickup_month#5244], [count#5280L DESC NULLS LAST]\n",
      "            +- Sort [pickup_month#5244 ASC NULLS FIRST, count#5280L DESC NULLS LAST], false, 0\n",
      "               +- Exchange hashpartitioning(pickup_month#5244, 1000)\n",
      "                  +- Sort [pickup_month#5244 DESC NULLS LAST, count#5280L DESC NULLS LAST, borough#5184 ASC NULLS FIRST], true, 0\n",
      "                     +- Exchange rangepartitioning(pickup_month#5244 DESC NULLS LAST, count#5280L DESC NULLS LAST, borough#5184 ASC NULLS FIRST, 1000)\n",
      "                        +- HashAggregate(keys=[pickup_month#5244, borough#5184], functions=[count(1)])\n",
      "                           +- Exchange hashpartitioning(pickup_month#5244, borough#5184, 1000)\n",
      "                              +- HashAggregate(keys=[pickup_month#5244, borough#5184], functions=[partial_count(1)])\n",
      "                                 +- Project [Borough#5184, date_format(cast(pickup_datetime#5159 as timestamp), yyyyMM, Some(UTC)) AS pickup_month#5244]\n",
      "                                    +- BroadcastHashJoin [PULocationID#5151L], [LocationID#5185L], Inner, BuildRight\n",
      "                                       :- Project [PULocationID#5151L, pickup_datetime#5159]\n",
      "                                       :  +- Filter ((isnotnull(pickup_datetime#5159) && isnotnull(dropoff_datetime#5153)) && isnotnull(PULocationID#5151L))\n",
      "                                       :     +- FileScan json [PULocationID#5151L,dropoff_datetime#5153,pickup_datetime#5159] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://ip-172-31-11-187.ec2.internal:8020/tmp/data/nyc-taxi/taxi-data/output/se..., PartitionFilters: [], PushedFilters: [IsNotNull(pickup_datetime), IsNotNull(dropoff_datetime), IsNotNull(PULocationID)], ReadSchema: struct<PULocationID:bigint,dropoff_datetime:string,pickup_datetime:string>\n",
      "                                       +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, bigint, true]))\n",
      "                                          +- Project [Borough#5184, LocationID#5185L]\n",
      "                                             +- Filter isnotnull(LocationID#5185L)\n",
      "                                                +- FileScan json [Borough#5184,LocationID#5185L] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://ip-172-31-11-187.ec2.internal:8020/tmp/data/nyc-taxi/zone-lookup/output/..., PartitionFilters: [], PushedFilters: [IsNotNull(LocationID)], ReadSchema: struct<Borough:string,LocationID:bigint>\n",
      "+------------+---------+--------+-------+\n",
      "|pickup_month|  borough|   count|row_num|\n",
      "+------------+---------+--------+-------+\n",
      "|      202101|Manhattan|       3|      1|\n",
      "|      202007|Manhattan|       6|      1|\n",
      "|      202006|Manhattan|       1|      1|\n",
      "|      202005|Manhattan|       5|      1|\n",
      "|      202004|Manhattan|       1|      1|\n",
      "|      202003|Manhattan|       5|      1|\n",
      "|      202002|Manhattan|      34|      1|\n",
      "|      202001|Manhattan|14817800|      1|\n",
      "|      201912|Manhattan|     129|      1|\n",
      "|      201009|   Queens|       3|      1|\n",
      "|      200901|Manhattan|      19|      1|\n",
      "|      200812|Manhattan|       8|      1|\n",
      "|      200301|   Queens|       1|      1|\n",
      "+------------+---------+--------+-------+\n",
      "\n",
      "(85.80998541499957, DataFrame[pickup_month: string, borough: string, count: bigint, row_num: int])"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_most_pickups_per_month_window_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab - 2.6 Run and time the overall pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset notebook kernel\n",
    "def ingest_main():\n",
    "    ingest_taxi_data_multi_service(\"s3://nyc-tlc/trip data/yellow_tripdata_2020-01.csv\")\n",
    "    ingest_taxi_lookup(\"s3://nyc-tlc/misc/taxi _zone_lookup.csv\")\n",
    "    get_most_pickups_per_month_window_pyspark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timer_method(\"ingest_main()\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
