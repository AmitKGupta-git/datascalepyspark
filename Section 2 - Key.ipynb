{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720161de5fff4aea8064b88d04c51422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1603838352695_0002</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Using cached https://files.pythonhosted.org/packages/f2/d9/e8a56bd0953914f60207af4c41bb3947c47ca03577b1fe26258249dd9af7/boto3-1.16.6-py2.py3-none-any.whl\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3)\n",
      "Collecting botocore<1.20.0,>=1.19.6 (from boto3)\n",
      "  Using cached https://files.pythonhosted.org/packages/15/6c/f5b074e14823f250e0a73e53714c1ed80d689d530468936d35a9d336f1dd/botocore-1.19.6-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.4.0,>=0.3.0 (from boto3)\n",
      "  Using cached https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3<1.26,>=1.25.4; python_version != \"3.4\" in /usr/local/lib/python3.7/site-packages (from botocore<1.20.0,>=1.19.6->boto3)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore<1.20.0,>=1.19.6->boto3)\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.6->boto3)\n",
      "Installing collected packages: python-dateutil, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.16.6 botocore-1.19.6 python-dateutil-2.8.1 s3transfer-0.3.3\n",
      "\n",
      "Collecting pandas\n",
      "  Using cached https://files.pythonhosted.org/packages/25/47/22fc373440e144e2111363adaa07abb09ec1f03fbc071b6d9fc0bbf65f68/pandas-1.1.3-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib64/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /mnt/tmp/1603838861189-0/lib/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.1.3\n",
      "\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests)\n",
      "\n",
      "Collecting fsspec\n",
      "  Using cached https://files.pythonhosted.org/packages/a5/8b/1df260f860f17cb08698170153ef7db672c497c1840dcc8613ce26a8a005/fsspec-0.8.4-py3-none-any.whl\n",
      "Installing collected packages: fsspec\n",
      "Successfully installed fsspec-0.8.4"
     ]
    }
   ],
   "source": [
    "# Install libraries within the notebook scope\n",
    "sc.install_pypi_package(\"boto3\")\n",
    "sc.install_pypi_package(\"pandas\")\n",
    "sc.install_pypi_package(\"requests\")\n",
    "#sc.install_pypi_package(\"s3fs\")\n",
    "sc.install_pypi_package(\"fsspec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a0605c3135485e8dea158eaf3ef05d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import boto3\n",
    "from datetime import datetime\n",
    "import fsspec\n",
    "import pandas as pd\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql import functions as f, types as t, Window\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "# import s3fs\n",
    "import subprocess\n",
    "import timeit\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Removes truncation of columns, column values in Pandas\n",
    "# by default\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "# Monkey patching the DataFrame transform method for Spark 2.4\n",
    "# This is available by default in Spark 3.0\n",
    "def transform(self, f):\n",
    "    return f(self)\n",
    "DataFrame.transform = transform\n",
    "\n",
    "# Override the timeit template to return the command's\n",
    "# return value in addition to the time\n",
    "# Reference: https://stackoverflow.com/questions/24812253/how-can-i-capture-return-value-with-python-timeit-module\n",
    "timeit.template = \"\"\"\n",
    "def inner(_it, _timer{init}):\n",
    "    {setup}\n",
    "    _t0 = _timer()\n",
    "    for _i in _it:\n",
    "        retval = {stmt}\n",
    "    _t1 = _timer()\n",
    "    return _t1 - _t0, retval\n",
    "\"\"\"\n",
    "\n",
    "def shell_cmd(cmd):\n",
    "    \"\"\"\n",
    "    Wrapper for running shell commands and printing the output\n",
    "    Some helpful recipes:\n",
    "    - List files on hdfs: shell_cmd(\"hdfs dfs -ls hdfs:///tmp/data/\")\n",
    "    - Remove files from hdfs: shell_cmd(\"hdfs dfs -rm -r hdfs:///tmp/data/test_pyspark\")\n",
    "    \"\"\"\n",
    "    for line in subprocess.check_output(cmd, shell=True).split(b'\\n'):\n",
    "        print(line)\n",
    "\n",
    "def timer_method(cmd):\n",
    "    \"\"\"\n",
    "    Wrapper for timeit that returns the value of a function and its runtime\n",
    "    To use, pass a string of the function you wish to time\n",
    "    Example: \n",
    "     run_time, result = timer_method(\"myfunction(arg1, arg2)\")\n",
    "    \"\"\"\n",
    "    # Setting globals = globals() enables the timeit function\n",
    "    # to return the value generated by cmd\n",
    "    return timeit.timeit(cmd, number=1, globals = globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set your s3 bucket name\n",
    "This should be data-scale-oreilly-{your name}   \n",
    "If you dont remember check the [S3 console](https://s3.console.aws.amazon.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eeb60c2780a4ebdbc8af4e173300054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MY_BUCKET_NAME = \"data-scale-oreilly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting from an S3 bucket - NYC Taxi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "* Taxi data \n",
    "* Data dictionaries \n",
    "* Taxi zone lookup table\n",
    "\n",
    "Data ingestion has the ultimate goal of collecting, aggregating, and surfacing data for a specific purpose; an analysis, an API, a dashboard, etc. Think about how you might use the taxi data to answer the following questions:\n",
    "\n",
    "1. Which borough is the most popular pickup or drop off spot?\n",
    "1. Are green taxis more popular for trips within the same borough vs yellow taxis?\n",
    "1. Build a recommendation engine that predicts surge pricing for a given time of day based on historical data  \n",
    "\n",
    "With this in mind, lets work through bringing this data onto the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c74e5728574e29bba68263e86da6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note, if you copy the link from the taxi data website you will see:\n",
    "# https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-01.csv\n",
    "# Two things - first, the portion of the URL following \"aws.com\" is the \n",
    "# bucket name. Second, in \"trip+data\" the \"+\" is a space\n",
    "taxi_data_path = \"s3://nyc-tlc/trip data/yellow_tripdata_2020-01.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b3ce355a164ac6b1fc38cb4de8e652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Install s3fs to access S3\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/1603838861189-0/lib/python3.7/site-packages/pandas/io/parsers.py\", line 686, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/tmp/1603838861189-0/lib/python3.7/site-packages/pandas/io/parsers.py\", line 435, in _read\n",
      "    filepath_or_buffer, encoding, compression\n",
      "  File \"/tmp/1603838861189-0/lib/python3.7/site-packages/pandas/io/common.py\", line 222, in get_filepath_or_buffer\n",
      "    filepath_or_buffer, mode=mode or \"rb\", **(storage_options or {})\n",
      "  File \"/tmp/1603838861189-0/lib/python3.7/site-packages/fsspec/core.py\", line 438, in open\n",
      "    **kwargs\n",
      "  File \"/tmp/1603838861189-0/lib/python3.7/site-packages/fsspec/core.py\", line 287, in open_files\n",
      "    expand=expand,\n",
      "  File \"/tmp/1603838861189-0/lib/python3.7/site-packages/fsspec/core.py\", line 600, in get_fs_token_paths\n",
      "    cls = get_filesystem_class(protocol)\n",
      "  File \"/tmp/1603838861189-0/lib/python3.7/site-packages/fsspec/registry.py\", line 204, in get_filesystem_class\n",
      "    raise ImportError(bit[\"err\"]) from e\n",
      "ImportError: Install s3fs to access S3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pandas uses s3fs to read_csv from s3:\n",
    "pd.read_csv(taxi_data_path, keep_default_na=False)\n",
    "pd_df_taxi.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5fffbe680748a9988272e41b19905e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|       1| 2020-01-01 00:28:15|  2020-01-01 00:33:03|              1|          1.2|         1|                 N|         238|         239|           1|        6.0|  3.0|    0.5|      1.47|         0.0|                  0.3|       11.27|                 2.5|\n",
      "|       1| 2020-01-01 00:35:39|  2020-01-01 00:43:04|              1|          1.2|         1|                 N|         239|         238|           1|        7.0|  3.0|    0.5|       1.5|         0.0|                  0.3|        12.3|                 2.5|\n",
      "|       1| 2020-01-01 00:47:41|  2020-01-01 00:53:52|              1|          0.6|         1|                 N|         238|         238|           1|        6.0|  3.0|    0.5|       1.0|         0.0|                  0.3|        10.8|                 2.5|\n",
      "|       1| 2020-01-01 00:55:23|  2020-01-01 01:00:14|              1|          0.8|         1|                 N|         238|         151|           1|        5.5|  0.5|    0.5|      1.36|         0.0|                  0.3|        8.16|                 0.0|\n",
      "|       2| 2020-01-01 00:01:58|  2020-01-01 00:04:16|              1|          0.0|         1|                 N|         193|         193|           2|        3.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         4.8|                 0.0|\n",
      "|       2| 2020-01-01 00:09:44|  2020-01-01 00:10:37|              1|         0.03|         1|                 N|           7|         193|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         3.8|                 0.0|\n",
      "|       2| 2020-01-01 00:39:25|  2020-01-01 00:39:29|              1|          0.0|         1|                 N|         193|         193|           1|        2.5|  0.5|    0.5|      0.01|         0.0|                  0.3|        3.81|                 0.0|\n",
      "|       2| 2019-12-18 15:27:49|  2019-12-18 15:28:59|              1|          0.0|         5|                 N|         193|         193|           1|       0.01|  0.0|    0.0|       0.0|         0.0|                  0.3|        2.81|                 2.5|\n",
      "|       2| 2019-12-18 15:30:35|  2019-12-18 15:31:35|              4|          0.0|         1|                 N|         193|         193|           1|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         6.3|                 2.5|\n",
      "|       1| 2020-01-01 00:29:01|  2020-01-01 00:40:28|              2|          0.7|         1|                 N|         246|          48|           1|        8.0|  3.0|    0.5|      2.35|         0.0|                  0.3|       14.15|                 2.5|\n",
      "|       1| 2020-01-01 00:55:11|  2020-01-01 01:12:03|              2|          2.4|         1|                 N|         246|          79|           1|       12.0|  3.0|    0.5|      1.75|         0.0|                  0.3|       17.55|                 2.5|\n",
      "|       1| 2020-01-01 00:37:15|  2020-01-01 00:51:41|              1|          0.8|         1|                 N|         163|         161|           2|        9.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        13.3|                 2.5|\n",
      "|       1| 2020-01-01 00:56:27|  2020-01-01 01:21:44|              1|          3.3|         1|                 N|         161|         144|           1|       17.0|  3.0|    0.5|      4.15|         0.0|                  0.3|       24.95|                 2.5|\n",
      "|       2| 2020-01-01 00:21:54|  2020-01-01 00:27:31|              1|         1.07|         1|                 N|          43|         239|           1|        6.0|  0.5|    0.5|      1.96|         0.0|                  0.3|       11.76|                 2.5|\n",
      "|       2| 2020-01-01 00:38:01|  2020-01-01 01:15:21|              1|         7.76|         1|                 N|         143|          25|           1|       28.5|  0.5|    0.5|      4.84|         0.0|                  0.3|       37.14|                 2.5|\n",
      "|       1| 2020-01-01 00:15:35|  2020-01-01 00:27:06|              3|          1.6|         1|                 N|         211|         234|           2|        9.0|  3.0|    0.5|       0.0|         0.0|                  0.3|        12.8|                 2.5|\n",
      "|       1| 2020-01-01 00:41:20|  2020-01-01 00:44:22|              1|          0.5|         1|                 Y|         234|          90|           1|        4.0|  3.0|    0.5|       1.0|         0.0|                  0.3|         8.8|                 2.5|\n",
      "|       1| 2020-01-01 00:56:38|  2020-01-01 01:13:34|              1|          1.7|         1|                 N|         246|         142|           2|       11.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        15.3|                 2.5|\n",
      "|       2| 2020-01-01 00:08:21|  2020-01-01 00:25:29|              1|         8.45|         1|                 N|         138|         216|           2|       24.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        25.8|                 0.0|\n",
      "|       1| 2020-01-01 00:25:39|  2020-01-01 00:27:05|              1|          0.0|         1|                 N|         170|         162|           4|        3.0|  3.0|    0.5|       0.0|         0.0|                  0.3|         6.8|                 2.5|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "# For reference, look at the Spark DataFrameReader, csv:\n",
    "# https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html\n",
    "ps_df_taxi = spark.read.option('header', True).option('inferSchema', True).csv(taxi_data_path)\n",
    "ps_df_taxi.show()\n",
    "ps_df_taxi.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c39962e9fa422394d8eb8acc4a417d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'path hdfs://ip-172-31-4-225.ec2.internal:8020/tmp/input/taxi_data already exists.;'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 932, in csv\n",
      "    self._jwrite.csv(path)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: 'path hdfs://ip-172-31-4-225.ec2.internal:8020/tmp/input/taxi_data already exists.;'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Talk through ingest practices around retaining original data vs augmenting\n",
    "# For example, we may want to keep the data in its default format so we can\n",
    "# refer back to it if there are bugs in our data ingestion code\n",
    "ps_df_taxi.write.option(\"header\", True).csv(\"hdfs:///tmp/input/taxi_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28bcecfdc0a44fba9a9633b8c99c8d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Found 17 items'\n",
      "b'-rw-r--r--   1 livy hadoop          0 2020-10-27 22:45 hdfs:///tmp/input/taxi_data/_SUCCESS'\n",
      "b'-rw-r--r--   1 livy hadoop   43084929 2020-10-27 22:45 hdfs:///tmp/input/taxi_data/part-00000-1df7029f-62bf-49e0-a704-429cd07de59a-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   43034410 2020-10-27 22:45 hdfs:///tmp/input/taxi_data/part-00001-1df7029f-62bf-49e0-a704-429cd07de59a-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   43025344 2020-10-27 22:45 hdfs:///tmp/input/taxi_data/part-00002-1df7029f-62bf-49e0-a704-429cd07de59a-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   43007863 2020-10-27 22:45 hdfs:///tmp/input/taxi_data/part-00003-1df7029f-62bf-49e0-a704-429cd07de59a-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   42953869 2020-10-27 22:45 hdfs:///tmp/input/taxi_data/part-00004-1df7029f-62bf-49e0-a704-429cd07de59a-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   42984227 2020-10-27 22:45 hdfs:///tmp/input/taxi_data/part-00005-1df7029f-62bf-49e0-a704-429cd07de59a-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   42972270 2020-10-27 22:45 hdfs:///tmp/input/taxi_data/part-00006-1df7029f-62bf-49e0-a704-429cd07de59a-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   42938718 2020-10-27 22:45 hdfs:///tmp/input/taxi_data/part-00007-1df7029f-62bf-49e0-a704-429cd07de59a-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   42998587 2020-10-27 22:45 hdfs:///tmp/input/taxi_data/part-00008-1df7029f-62bf-49e0-a704-429cd07de59a-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   43055300 2020-10-27 22:45 hdfs:///tmp/input/taxi_data/part-00009-1df7029f-62bf-49e0-a704-429cd07de59a-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   42957733 2020-10-27 22:45 hdfs:///tmp/input/taxi_data/part-00010-1df7029f-62bf-49e0-a704-429cd07de59a-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   42944532 2020-10-27 22:45 hdfs:///tmp/input/taxi_data/part-00011-1df7029f-62bf-49e0-a704-429cd07de59a-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   43007286 2020-10-27 22:45 hdfs:///tmp/input/taxi_data/part-00012-1df7029f-62bf-49e0-a704-429cd07de59a-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   42978682 2020-10-27 22:45 hdfs:///tmp/input/taxi_data/part-00013-1df7029f-62bf-49e0-a704-429cd07de59a-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   42994113 2020-10-27 22:45 hdfs:///tmp/input/taxi_data/part-00014-1df7029f-62bf-49e0-a704-429cd07de59a-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   38875715 2020-10-27 22:45 hdfs:///tmp/input/taxi_data/part-00015-1df7029f-62bf-49e0-a704-429cd07de59a-c000.csv'\n",
      "b''"
     ]
    }
   ],
   "source": [
    "# Discuss how spark writes files out\n",
    "shell_cmd(\"hdfs dfs -ls hdfs:///tmp/input/taxi_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b3ebacc37b45618b8b7628722fbcd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|PULocationID|DOLocationID|fare_amount|tip_amount|\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+\n",
      "| 2020-01-01 00:28:15|  2020-01-01 00:33:03|              1|          1.2|         238|         239|        6.0|      1.47|\n",
      "| 2020-01-01 00:35:39|  2020-01-01 00:43:04|              1|          1.2|         239|         238|        7.0|       1.5|\n",
      "| 2020-01-01 00:47:41|  2020-01-01 00:53:52|              1|          0.6|         238|         238|        6.0|       1.0|\n",
      "| 2020-01-01 00:55:23|  2020-01-01 01:00:14|              1|          0.8|         238|         151|        5.5|      1.36|\n",
      "| 2020-01-01 00:01:58|  2020-01-01 00:04:16|              1|          0.0|         193|         193|        3.5|       0.0|\n",
      "| 2020-01-01 00:09:44|  2020-01-01 00:10:37|              1|         0.03|           7|         193|        2.5|       0.0|\n",
      "| 2020-01-01 00:39:25|  2020-01-01 00:39:29|              1|          0.0|         193|         193|        2.5|      0.01|\n",
      "| 2019-12-18 15:27:49|  2019-12-18 15:28:59|              1|          0.0|         193|         193|       0.01|       0.0|\n",
      "| 2019-12-18 15:30:35|  2019-12-18 15:31:35|              4|          0.0|         193|         193|        2.5|       0.0|\n",
      "| 2020-01-01 00:29:01|  2020-01-01 00:40:28|              2|          0.7|         246|          48|        8.0|      2.35|\n",
      "| 2020-01-01 00:55:11|  2020-01-01 01:12:03|              2|          2.4|         246|          79|       12.0|      1.75|\n",
      "| 2020-01-01 00:37:15|  2020-01-01 00:51:41|              1|          0.8|         163|         161|        9.5|       0.0|\n",
      "| 2020-01-01 00:56:27|  2020-01-01 01:21:44|              1|          3.3|         161|         144|       17.0|      4.15|\n",
      "| 2020-01-01 00:21:54|  2020-01-01 00:27:31|              1|         1.07|          43|         239|        6.0|      1.96|\n",
      "| 2020-01-01 00:38:01|  2020-01-01 01:15:21|              1|         7.76|         143|          25|       28.5|      4.84|\n",
      "| 2020-01-01 00:15:35|  2020-01-01 00:27:06|              3|          1.6|         211|         234|        9.0|       0.0|\n",
      "| 2020-01-01 00:41:20|  2020-01-01 00:44:22|              1|          0.5|         234|          90|        4.0|       1.0|\n",
      "| 2020-01-01 00:56:38|  2020-01-01 01:13:34|              1|          1.7|         246|         142|       11.5|       0.0|\n",
      "| 2020-01-01 00:08:21|  2020-01-01 00:25:29|              1|         8.45|         138|         216|       24.5|       0.0|\n",
      "| 2020-01-01 00:25:39|  2020-01-01 00:27:05|              1|          0.0|         170|         162|        3.0|       0.0|\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "column_subset = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID', 'fare_amount', 'tip_amount']\n",
    "ps_df_taxi.select(*column_subset).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2dbf2e2af324c179641c535bab0e533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+-----------------+------------------+-----------------+\n",
      "|summary|   passenger_count|     trip_distance|      PULocationID|     DOLocationID|       fare_amount|       tip_amount|\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+-----------------+\n",
      "|  count|           6339567|           6405008|           6405008|          6405008|           6405008|          6405008|\n",
      "|   mean|1.5153326717739555|2.9296439333097024|164.73225778952968|162.6626908194338|12.694108119771682|2.189341830642485|\n",
      "| stddev|1.1515942134278125|  83.1591059732506| 65.54373944111781| 69.9126062949608|12.127295340046487| 2.76002839237841|\n",
      "|    min|                 0|            -30.62|                 1|                1|           -1238.0|            -91.0|\n",
      "|    max|                 9|         210240.07|               265|              265|            4265.0|           1100.0|\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+-----------------+"
     ]
    }
   ],
   "source": [
    "ps_df_taxi.select(*column_subset).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.1 - Write an ingestion function that does the following:\n",
    "Given a file path to a taxi data csv (i.e. s3://nyc-tlc/trip data/green_tripdata_2020-01.csv) create a function that does the following:\n",
    "1. Read the file into a Spark dataframe\n",
    "1. Limit to the `column_subset` columns\n",
    "1. Write the data as json to hdfs in append mode to `hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json`\n",
    "\n",
    "Function signature:  \n",
    "`def ingest_taxi_data(file_name)`\n",
    "\n",
    "See the subsequent cell for more info on how the `ingest_taxi_data` function will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698b3902e429408f80a2ccc4e4642df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ingest_taxi_data(file_name):\n",
    "    # Enclosing code in () allows multi line\n",
    "    (spark\n",
    "         .read\n",
    "         .option('header', True)\n",
    "         .option(\"inferSchema\", True)\n",
    "         .csv(file_name)\n",
    "         .select(*column_subset)           \n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73d6adfac4c412796d032dbadc9d21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the ingest for several files\n",
    "taxi_data_prefix = \"s3://nyc-tlc/trip data\"\n",
    "taxi_data_files = [\"yellow_tripdata_2019-01.csv\", \"yellow_tripdata_2018-01.csv\"]\n",
    "for file_name in taxi_data_files: \n",
    "    taxi_data_path = f\"{taxi_data_prefix}/{file_name}\"\n",
    "    ingest_taxi_data(taxi_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09bcd98091940c7a06da39326cb335a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      "\n",
      "+------------+------------+-----------+---------------+----------+------------------------+------------------------+-------------+\n",
      "|DOLocationID|PULocationID|fare_amount|passenger_count|tip_amount|tpep_dropoff_datetime   |tpep_pickup_datetime    |trip_distance|\n",
      "+------------+------------+-----------+---------------+----------+------------------------+------------------------+-------------+\n",
      "|68          |237         |19.0       |1              |5.0       |2018-01-03T11:35:06.000Z|2018-01-03T11:06:45.000Z|3.5          |\n",
      "|162         |90          |12.0       |1              |3.0       |2018-01-03T12:04:36.000Z|2018-01-03T11:47:29.000Z|1.7          |\n",
      "|140         |87          |20.0       |1              |4.15      |2018-01-03T11:22:38.000Z|2018-01-03T11:04:59.000Z|5.8          |\n",
      "|75          |140         |10.5       |1              |0.0       |2018-01-03T11:37:46.000Z|2018-01-03T11:24:57.000Z|2.1          |\n",
      "|238         |75          |7.5        |1              |2.05      |2018-01-03T11:48:21.000Z|2018-01-03T11:40:18.000Z|1.5          |\n",
      "+------------+------------+-----------+---------------+----------+------------------------+------------------------+-------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# How did the types fare in this conversion?\n",
    "# https://issues.apache.org/jira/browse/SPARK-26325\n",
    "# https://stackoverflow.com/questions/53697388/interpret-timestamp-fields-in-spark-while-reading-json\n",
    "df = spark.read.json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "df.printSchema()\n",
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming data types \n",
    "\n",
    "Available pyspark types are listed in the pyspark.sql.types module https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#module-pyspark.sql.types\n",
    "\n",
    "This is imported as t, so to apply the IntegerType use t.IntegerType()\n",
    "\n",
    "For pandas, see the following resources on converting types https://stackoverflow.com/questions/15891038/change-column-type-in-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632db148dce847f38da936728bf4a2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tpep_dropoff_datetime', 'timestamp')]"
     ]
    }
   ],
   "source": [
    "# Pyspark\n",
    "(df.select(\"tpep_dropoff_datetime\")\n",
    " .withColumn(\"tpep_dropoff_datetime\", f.col(\"tpep_dropoff_datetime\").cast(t.TimestampType()))\n",
    ").dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc68d8ebd924750b760b2fbc7027ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Casting pandas columns to a type - this will give an error on empty cells\n",
    "# (pd_df_taxi[[*column_subset]]\n",
    "#         .astype({'passenger_count': 'Int64'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8497f7a437f4251af4dc024573633ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To convert to Integer using pandas, we have to first deal with the null values\n",
    "# to_numeric with 'coerce' will fill invalid integer values with np.NaN\n",
    "# the Int64 type in later versions of pandas will convert np.NaN to a nullable\n",
    "# integer type: https://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html\n",
    "# pd.to_numeric(pd_df_taxi.passenger_count, errors='coerce').astype('Int64').unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03d3b1364b5419789bc3a27f41d9fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Modified taxi_data_ingest with transformed timestamps\n",
    "def ingest_taxi_data(file_name):\n",
    "    # Enclosing code in () allows multi line\n",
    "    (spark\n",
    "         .read\n",
    "         .option('header', True)\n",
    "         .option(\"inferSchema\", True)\n",
    "         .csv(file_name)\n",
    "         .select(*column_subset)\n",
    "         .withColumn(\"tpep_pickup_datetime\", f.col(\"tpep_pickup_datetime\").cast(t.TimestampType()))\n",
    "         .withColumn(\"tpep_dropoff_datetime\", f.col(\"tpep_dropoff_datetime\").cast(t.TimestampType()))\n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e933fbc1124248a0a499d5d4bea12c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json'\n",
      "b''"
     ]
    }
   ],
   "source": [
    "# Remove previous data\n",
    "shell_cmd(\"hdfs dfs -rm -r hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215bf67b97e04960a162e3391621f449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "taxi_data_prefix = \"s3://nyc-tlc/trip data\"\n",
    "taxi_data_files = [\"yellow_tripdata_2019-01.csv\", \"yellow_tripdata_2018-01.csv\"]\n",
    "for file_name in taxi_data_files: \n",
    "    taxi_data_path = f\"{taxi_data_prefix}/{file_name}\"\n",
    "    ingest_taxi_data(taxi_data_path)\n",
    "    \n",
    "df = spark.read.json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing ingestion code\n",
    "\n",
    "The `ingest_taxi_data` method is not well structured for testing:\n",
    "* Writes to the file system\n",
    "* Requires an input file to test\n",
    "* What other shortcomings?\n",
    "\n",
    "To make this code more testable, split out the transformation logic so it can be unit tested.  \n",
    "Definining a transformation function that takes a dataframe and returns a dataframe provides a better interface for unit testing, and a more extensible structure in case we need to add more dataframe functions before or after the transformation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23fe00ed31345858449119223fefecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform_taxi_data(df):\n",
    "    return (df\n",
    "            .withColumn(\"tpep_pickup_datetime\", f.col(\"tpep_pickup_datetime\").cast(t.TimestampType()))\n",
    "            .withColumn(\"tpep_dropoff_datetime\", f.col(\"tpep_dropoff_datetime\").cast(t.TimestampType()))\n",
    "           )\n",
    "\n",
    "# Option 1\n",
    "def ingest_taxi_data_method(file_name):\n",
    "    df_input = (spark\n",
    "         .read\n",
    "         .option('header', True).csv(taxi_data_path)\n",
    "         .select(*column_subset))\n",
    "    \n",
    "    (transform_taxi_data(df_input)\n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )\n",
    "\n",
    "# Option 2\n",
    "def ingest_taxi_data_transform(file_name):\n",
    "    # Requires patching of Dataframe.transform method in Spark 2.4, but available natively\n",
    "    # in Spark 3.0 https://mungingdata.com/pyspark/chaining-dataframe-transformations/\n",
    "    df_input = (spark\n",
    "         .read\n",
    "         .option('header', True).csv(taxi_data_path)\n",
    "         .select(*column_subset)\n",
    "         .transform(transform_taxi_data)\n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e85e60088c4420381c3c386d6d7a104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tpep_dropoff_datetime', 'string'), ('tpep_pickup_datetime', 'string')]\n",
      "True\n",
      "root\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)"
     ]
    }
   ],
   "source": [
    "test_data = [\n",
    "    \"{'tpep_pickup_datetime': '2020-05-23 21:05:23', 'tpep_dropoff_datetime': '2020-05-23 08:05:23'}\",\n",
    "    \"{'tpep_pickup_datetime': '2020-10-01 01:05:23', 'tpep_dropoff_datetime': '2020-10-01 02:05:23'}\",\n",
    "    \"{'tpep_pickup_datetime': '2020-02-02 15:22:23', 'tpep_dropoff_datetime': '2020-02-03 15:44:23'}\"\n",
    "]\n",
    "expected_types = {'tpep_dropoff_datetime': 'timestamp', 'tpep_pickup_datetime': 'timestamp'}\n",
    "\n",
    "test_df = spark.read.json(sc.parallelize(test_data))\n",
    "print(test_df.dtypes)\n",
    "test = transform_taxi_data(test_df)\n",
    "test_types = {item[0]:item[1] for item in test.dtypes}\n",
    "\n",
    "print(expected_types == test_types)\n",
    "\n",
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets try running the ingestion code on the other taxi data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0011f5cb7714aa9a491b0c2df56d061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"cannot resolve '`tpep_pickup_datetime`' given input columns: [trip_distance, ehail_fee, extra, total_amount, fare_amount, lpep_pickup_datetime, payment_type, passenger_count, mta_tax, VendorID, congestion_surcharge, store_and_fwd_flag, improvement_surcharge, PULocationID, trip_type, DOLocationID, tolls_amount, lpep_dropoff_datetime, RatecodeID, tip_amount];;\\n'Project ['tpep_pickup_datetime, 'tpep_dropoff_datetime, passenger_count#1024, trip_distance#1025, PULocationID#1022, DOLocationID#1023, fare_amount#1026, tip_amount#1029]\\n+- Relation[VendorID#1017,lpep_pickup_datetime#1018,lpep_dropoff_datetime#1019,store_and_fwd_flag#1020,RatecodeID#1021,PULocationID#1022,DOLocationID#1023,passenger_count#1024,trip_distance#1025,fare_amount#1026,extra#1027,mta_tax#1028,tip_amount#1029,tolls_amount#1030,ehail_fee#1031,improvement_surcharge#1032,total_amount#1033,payment_type#1034,trip_type#1035,congestion_surcharge#1036] csv\\n\"\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 27, in ingest_taxi_data_transform\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 1326, in select\n",
      "    jdf = self._jdf.select(self._jcols(*cols))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: \"cannot resolve '`tpep_pickup_datetime`' given input columns: [trip_distance, ehail_fee, extra, total_amount, fare_amount, lpep_pickup_datetime, payment_type, passenger_count, mta_tax, VendorID, congestion_surcharge, store_and_fwd_flag, improvement_surcharge, PULocationID, trip_type, DOLocationID, tolls_amount, lpep_dropoff_datetime, RatecodeID, tip_amount];;\\n'Project ['tpep_pickup_datetime, 'tpep_dropoff_datetime, passenger_count#1024, trip_distance#1025, PULocationID#1022, DOLocationID#1023, fare_amount#1026, tip_amount#1029]\\n+- Relation[VendorID#1017,lpep_pickup_datetime#1018,lpep_dropoff_datetime#1019,store_and_fwd_flag#1020,RatecodeID#1021,PULocationID#1022,DOLocationID#1023,passenger_count#1024,trip_distance#1025,fare_amount#1026,extra#1027,mta_tax#1028,tip_amount#1029,tolls_amount#1030,ehail_fee#1031,improvement_surcharge#1032,total_amount#1033,payment_type#1034,trip_type#1035,congestion_surcharge#1036] csv\\n\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try using the ingest code we created for yellow taxi for all the taxis\n",
    "# This will fail because the datetime fields have different names across different servcies\n",
    "\n",
    "taxi_data_prefix = \"s3://nyc-tlc/trip data/\"\n",
    "taxi_data_files = [\"green_tripdata_2020-01.csv\", \"fhv_tripdata_2020-01.csv\", \"fhvhv_tripdata_2020-01.csv\"]\n",
    "for file_name in taxi_data_files: \n",
    "    taxi_data_path = f\"{taxi_data_prefix}{file_name}\"\n",
    "    ingest_taxi_data_transform(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.2 - Write different transformation functions for each taxi service type to match the following signature and schema:\n",
    "See the [Taxi data website](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) for reference\n",
    "\n",
    "Fields / Types:\n",
    "\n",
    "* pickup_datetime Timestamp\n",
    "* dropoff_datetime Timestamp\n",
    "* passenger_count Integer\n",
    "* fare_amount Float\n",
    "* tip_amount Float\n",
    "* trip_distance Float\n",
    "* PULocationID Integer\n",
    "* DOLocationID Integer\n",
    "\n",
    "`def transform_function(dataframe):  \n",
    "    return transformed_dataframe\n",
    "`\n",
    "\n",
    "Refer to `ingest_taxi_data_multi_service` to see how these functions will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa16fa5ffb4470ab57c30ccf1c0fcef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform_yellow_taxi(df):\n",
    "    subset = ['pickup_datetime', 'dropoff_datetime', 'passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID', 'fare_amount', 'tip_amount']\n",
    "    return (df.withColumn(\"pickup_datetime\", f.col(\"tpep_pickup_datetime\").cast(t.TimestampType()))\n",
    "        .withColumn(\"dropoff_datetime\", f.col(\"tpep_dropoff_datetime\").cast(t.TimestampType()))\n",
    "        .select(*subset)\n",
    "        )\n",
    "        \n",
    "        \n",
    "def transform_green_taxi(df):\n",
    "    subset = ['pickup_datetime', 'dropoff_datetime', 'passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID', 'fare_amount', 'tip_amount']\n",
    "    return (df.withColumn(\"pickup_datetime\", f.col(\"lpep_pickup_datetime\").cast(t.TimestampType()))\n",
    "        .withColumn(\"dropoff_datetime\", f.col(\"lpep_dropoff_datetime\").cast(t.TimestampType()))\n",
    "        .select(*subset)\n",
    "        )\n",
    "\n",
    "def transform_fhv(df):\n",
    "    return df.select(*[\"pickup_datetime\", \"dropoff_datetime\", \"PULocationID\", \"DOLocationID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the ability to read all the taxi data into the same dataset, how will we be able to tell where the original data came from? The file name provides information including:\n",
    "* Service type (yellow, green, etc)\n",
    "* File date\n",
    "\n",
    "We want to augment the taxi data with this information so we can refer back to it in analysis.\n",
    "\n",
    "Is there other data we might want to augment the raw data with? Some things to consider:\n",
    "* Additional fields that could help with analysis\n",
    "* Metadata, like when the record was last updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008885bbae78473ba004ff786a33d2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using matched groups, we can extract information from the taxi file names\n",
    "# i.e. yellow_tripdata_2020-01.csv\n",
    "TAXI_DATA_PATTERN = \"(?P<service>[a-zA-Z0-9]+)_tripdata_(?P<year>[0-9]{4})-(?P<month>[0-9]{2}).csv\"\n",
    "\n",
    "def extract_file_info(file_name):\n",
    "    m = re.match(TAXI_DATA_PATTERN, file_name)\n",
    "    if m is not None:\n",
    "        return (m.group(1), m.group(2), m.group(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5b9febd65e41e1aa59568bf957b581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ingest_taxi_data_multi_service(file_name, ingested_on):\n",
    "    print(f\"Processing {file_name}\")\n",
    "    (service, year, month) = extract_file_info(Path(file_name).name)\n",
    "    input_df = spark.read.option('header', True).option('inferSchema', True).csv(file_name)\n",
    "    \n",
    "    if service == 'yellow':\n",
    "        df_transform = transform_yellow_taxi(input_df)\n",
    "    elif service == 'green':\n",
    "        df_transform = transform_green_taxi(input_df)\n",
    "    else:\n",
    "        # FHV. What happens if there are more taxi services added?\n",
    "        df_transform = transform_fhv(input_df)\n",
    "\n",
    "    (df_transform\n",
    "         .withColumn(\"service\", f.lit(service))\n",
    "         .withColumn(\"year\", f.lit(year))\n",
    "         .withColumn(\"month\", f.lit(month))\n",
    "         .withColumn(\"ingested_on\", f.lit(ingest_timestamp))\n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd6bf86b5384689a1eaacfcb54207e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json'\n",
      "b''"
     ]
    }
   ],
   "source": [
    "shell_cmd(\"hdfs dfs -rm -r hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436b4fe2494d4950b077b4a1f7b888db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing s3://nyc-tlc/trip data/yellow_tripdata_2020-01.csv\n",
      "Processing s3://nyc-tlc/trip data/green_tripdata_2020-01.csv\n",
      "Processing s3://nyc-tlc/trip data/fhv_tripdata_2020-01.csv\n",
      "Processing s3://nyc-tlc/trip data/fhvhv_tripdata_2020-01.csv\n",
      "[('DOLocationID', 'bigint'), ('PULocationID', 'bigint'), ('dropoff_datetime', 'string'), ('fare_amount', 'double'), ('ingested_on', 'string'), ('month', 'string'), ('passenger_count', 'bigint'), ('pickup_datetime', 'string'), ('service', 'string'), ('tip_amount', 'double'), ('trip_distance', 'double'), ('year', 'string')]\n",
      "+-------+--------+\n",
      "|service|   count|\n",
      "+-------+--------+\n",
      "| yellow| 6405008|\n",
      "|    fhv| 1958936|\n",
      "|  green|  447770|\n",
      "|  fhvhv|20569325|\n",
      "+-------+--------+"
     ]
    }
   ],
   "source": [
    "ingest_timestamp = datetime.strftime(datetime.now(), \"%Y-%m-%d %H:%M:%S%z\")\n",
    "taxi_data_prefix = \"s3://nyc-tlc/trip data/\"\n",
    "taxi_data_files = [\"yellow_tripdata_2020-01.csv\", \"green_tripdata_2020-01.csv\", \"fhv_tripdata_2020-01.csv\", \"fhvhv_tripdata_2020-01.csv\"]\n",
    "for file_name in taxi_data_files: \n",
    "    taxi_data_path = f\"{taxi_data_prefix}{file_name}\"\n",
    "    ingest_taxi_data_multi_service(taxi_data_path, ingest_timestamp)\n",
    "    \n",
    "df_taxi_output = spark.read.option(\"inferSchema\", True).json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "print(df_taxi_output.dtypes)\n",
    "df_taxi_output.groupby(\"service\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling bad data\n",
    "How to design for the inevitability of bad data  \n",
    "Reference: https://blog.knoldus.com/apache-spark-handle-corrupt-bad-records/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa6330037f2478aadcdba884d2cab48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bad_data = [\n",
    "    \"{'pickup_datetime': '2020-05-23 21:05:23', 'fare_amount': '0.05'}\",\n",
    "    \"{'pickup_datetime': '2020-05-23 08:05:23', 'fare_amount': '10.05'}\",\n",
    "    \"{'pickup_datetime': '2020-05-23 21:05:23', 'fare_amount}\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeaa8d9c97594bbaa8e1427392754663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-------------------+\n",
      "|     _corrupt_record|fare_amount|    pickup_datetime|\n",
      "+--------------------+-----------+-------------------+\n",
      "|                null|       0.05|2020-05-23 21:05:23|\n",
      "|                null|      10.05|2020-05-23 08:05:23|\n",
      "|{'pickup_datetime...|       null|               null|\n",
      "+--------------------+-----------+-------------------+"
     ]
    }
   ],
   "source": [
    "corrupt_df = spark.read.json(sc.parallelize(bad_data), mode=\"PERMISSIVE\", columnNameOfCorruptRecord=\"_corrupt_record\")\n",
    "corrupt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d164ba2ed9d43579b4d6969a72e8f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|fare_amount|    pickup_datetime|\n",
      "+-----------+-------------------+\n",
      "|       0.05|2020-05-23 21:05:23|\n",
      "|      10.05|2020-05-23 08:05:23|\n",
      "+-----------+-------------------+"
     ]
    }
   ],
   "source": [
    "corrupt_df = spark.read.json(sc.parallelize(bad_data), mode=\"DROPMALFORMED\", columnNameOfCorruptRecord=\"_corrupt_record\")\n",
    "corrupt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256ebef03b6a40deaab80f9c937a9d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o587.json.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 58.0 failed 4 times, most recent failure: Lost task 15.3 in stage 58.0 (TID 737, ip-172-31-5-146.ec2.internal, executor 1): org.apache.spark.SparkException: Malformed records are detected in schema inference. Parse Mode: FAILFAST.\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:66)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:53)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n",
      "\tat scala.collection.Iterator$class.isEmpty(Iterator.scala:331)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceLeftOption(TraversableOnce.scala:203)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceOption(TraversableOnce.scala:210)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1334)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:70)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:50)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: com.fasterxml.jackson.core.JsonParseException: Unexpected end-of-input in field name\n",
      " at [Source: [B@759c1287; line: 1, column: 113]\n",
      "\tat com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1581)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:533)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:470)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseAposName(UTF8StreamJsonParser.java:2153)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleOddName(UTF8StreamJsonParser.java:1998)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseName(UTF8StreamJsonParser.java:1650)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:740)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonUtils$.nextUntil(JacksonUtils.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.inferField(JsonInferSchema.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:57)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:55)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2619)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:55)\n",
      "\t... 24 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.infer(JsonInferSchema.scala:83)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$inferFromDataset$1.apply(JsonDataSource.scala:109)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$inferFromDataset$1.apply(JsonDataSource.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:108)\n",
      "\tat org.apache.spark.sql.DataFrameReader$$anonfun$2.apply(DataFrameReader.scala:440)\n",
      "\tat org.apache.spark.sql.DataFrameReader$$anonfun$2.apply(DataFrameReader.scala:440)\n",
      "\tat scala.Option.getOrElse(Option.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:439)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:420)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:406)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Malformed records are detected in schema inference. Parse Mode: FAILFAST.\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:66)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:53)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n",
      "\tat scala.collection.Iterator$class.isEmpty(Iterator.scala:331)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceLeftOption(TraversableOnce.scala:203)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceOption(TraversableOnce.scala:210)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1334)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:70)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:50)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: com.fasterxml.jackson.core.JsonParseException: Unexpected end-of-input in field name\n",
      " at [Source: UNKNOWN; line: 1, column: 113]\n",
      "\tat com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1581)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:533)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:470)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseAposName(UTF8StreamJsonParser.java:2153)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleOddName(UTF8StreamJsonParser.java:1998)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseName(UTF8StreamJsonParser.java:1650)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:740)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonUtils$.nextUntil(JacksonUtils.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.inferField(JsonInferSchema.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:57)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:55)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2619)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:55)\n",
      "\t... 24 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 286, in json\n",
      "    return self._df(self._jreader.json(jrdd))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o587.json.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 58.0 failed 4 times, most recent failure: Lost task 15.3 in stage 58.0 (TID 737, ip-172-31-5-146.ec2.internal, executor 1): org.apache.spark.SparkException: Malformed records are detected in schema inference. Parse Mode: FAILFAST.\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:66)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:53)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n",
      "\tat scala.collection.Iterator$class.isEmpty(Iterator.scala:331)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceLeftOption(TraversableOnce.scala:203)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceOption(TraversableOnce.scala:210)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1334)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:70)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:50)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: com.fasterxml.jackson.core.JsonParseException: Unexpected end-of-input in field name\n",
      " at [Source: [B@759c1287; line: 1, column: 113]\n",
      "\tat com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1581)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:533)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:470)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseAposName(UTF8StreamJsonParser.java:2153)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleOddName(UTF8StreamJsonParser.java:1998)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseName(UTF8StreamJsonParser.java:1650)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:740)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonUtils$.nextUntil(JacksonUtils.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.inferField(JsonInferSchema.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:57)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:55)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2619)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:55)\n",
      "\t... 24 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.infer(JsonInferSchema.scala:83)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$inferFromDataset$1.apply(JsonDataSource.scala:109)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$inferFromDataset$1.apply(JsonDataSource.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:108)\n",
      "\tat org.apache.spark.sql.DataFrameReader$$anonfun$2.apply(DataFrameReader.scala:440)\n",
      "\tat org.apache.spark.sql.DataFrameReader$$anonfun$2.apply(DataFrameReader.scala:440)\n",
      "\tat scala.Option.getOrElse(Option.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:439)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:420)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:406)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Malformed records are detected in schema inference. Parse Mode: FAILFAST.\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:66)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:53)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n",
      "\tat scala.collection.Iterator$class.isEmpty(Iterator.scala:331)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceLeftOption(TraversableOnce.scala:203)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceOption(TraversableOnce.scala:210)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1334)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:70)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:50)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: com.fasterxml.jackson.core.JsonParseException: Unexpected end-of-input in field name\n",
      " at [Source: UNKNOWN; line: 1, column: 113]\n",
      "\tat com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1581)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:533)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:470)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseAposName(UTF8StreamJsonParser.java:2153)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleOddName(UTF8StreamJsonParser.java:1998)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseName(UTF8StreamJsonParser.java:1650)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:740)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonUtils$.nextUntil(JacksonUtils.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.inferField(JsonInferSchema.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:57)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:55)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2619)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:55)\n",
      "\t... 24 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corrupt_df = spark.read.json(sc.parallelize(bad_data), mode=\"FAILFAST\", columnNameOfCorruptRecord=\"_corrupt_record\")\n",
    "corrupt_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.3 - Write an ingestion for the taxi zone lookup\n",
    "File location - Yes, there is a space between taxi and the '_'  \n",
    "\n",
    "s3://nyc-tlc/misc/taxi _zone_lookup.csv\n",
    "\n",
    "`def ingest_taxi_lookup():`\n",
    "1. Read taxi lookup data, ensuring data types are correct\n",
    "1. Add relevant metadata\n",
    "1. Save to hdfs:///tmp/data/nyc-taxi/zone-lookup/output/section2/json\n",
    "1. What write mode should be used?\n",
    "\n",
    "Refer back to Taxi Data page for more info: https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58462b109334e50aeaa4b286ac302e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def taxi_zone_transform(df):\n",
    "    return df.withColumn(\"ingested_on\", f.lit(ingest_timestamp))\n",
    "\n",
    "def ingest_taxi_lookup(ingest_timestamp):\n",
    "    (spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"s3://nyc-tlc/misc/taxi _zone_lookup.csv\")\n",
    "    .transform(taxi_zone_transform)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .json(\"hdfs:///tmp/data/nyc-taxi/zone-lookup/output/section2/json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dfbeb1e42dc4532aa06f802e0454383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6578351050000038, None)"
     ]
    }
   ],
   "source": [
    "ingest_timestamp = datetime.strftime(datetime.now(), \"%Y-%m-%d %H:%M:%S%z\")\n",
    "print(timer_method(\"ingest_taxi_lookup(ingest_timestamp)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.4 - Case Study 1: Month over month, get the total count of of pickups per borough\n",
    "#### Do not blindly run hese cells, you can bork your cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d471508959040f4839dbf79e008b4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "taxiPath = \"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/\"\n",
    "taxiLookupPath = \"hdfs:///tmp/data/nyc-taxi/zone-lookup/output/section2/json/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ec180fd1614856997f92cf5c280830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'Detected implicit cartesian product for INNER join between logical plans\\nJoin Inner, ((LocationID#1692L = PULocationID#1662L) && (LocationID#1692L = DOLocationID#1661L))\\n:- Filter ((isnotnull(pickup_datetime#1668) && isnotnull(dropoff_datetime#1663)) && ((isnotnull(PULocationID#1662L) && (DOLocationID#1661L = PULocationID#1662L)) && isnotnull(DOLocationID#1661L)))\\n:  +- Relation[DOLocationID#1661L,PULocationID#1662L,dropoff_datetime#1663,fare_amount#1664,ingested_on#1665,month#1666,passenger_count#1667L,pickup_datetime#1668,service#1669,tip_amount#1670,trip_distance#1671,year#1672] json\\n+- Project [LocationID#1692L, Borough#1691 AS PUBorough#1703]\\n   +- Filter isnotnull(LocationID#1692L)\\n      +- Relation[Borough#1691,LocationID#1692L,Zone#1693,ingested_on#1694,service_zone#1695] json\\nand\\nProject [LocationID#1754L, Borough#1753 AS DOBorough#1750]\\n+- Relation[Borough#1753,LocationID#1754L,Zone#1755,ingested_on#1756,service_zone#1757] json\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 381, in show\n",
      "    print(self._jdf.showString(n, 20, vertical))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: 'Detected implicit cartesian product for INNER join between logical plans\\nJoin Inner, ((LocationID#1692L = PULocationID#1662L) && (LocationID#1692L = DOLocationID#1661L))\\n:- Filter ((isnotnull(pickup_datetime#1668) && isnotnull(dropoff_datetime#1663)) && ((isnotnull(PULocationID#1662L) && (DOLocationID#1661L = PULocationID#1662L)) && isnotnull(DOLocationID#1661L)))\\n:  +- Relation[DOLocationID#1661L,PULocationID#1662L,dropoff_datetime#1663,fare_amount#1664,ingested_on#1665,month#1666,passenger_count#1667L,pickup_datetime#1668,service#1669,tip_amount#1670,trip_distance#1671,year#1672] json\\n+- Project [LocationID#1692L, Borough#1691 AS PUBorough#1703]\\n   +- Filter isnotnull(LocationID#1692L)\\n      +- Relation[Borough#1691,LocationID#1692L,Zone#1693,ingested_on#1694,service_zone#1695] json\\nand\\nProject [LocationID#1754L, Borough#1753 AS DOBorough#1750]\\n+- Relation[Borough#1753,LocationID#1754L,Zone#1755,ingested_on#1756,service_zone#1757] json\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join boroughs\n",
    "# Expected error cartesian join. most likely a carryover bug from 2.0\n",
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", \"false\") #<-- default\n",
    "df_taxi = spark.read.json(taxiPath)\n",
    "df_taxi_lookup = spark.read.json(taxiLookupPath)\n",
    "\n",
    "taxi_filtered = (df_taxi\n",
    " .filter(df_taxi.pickup_datetime.isNotNull())\n",
    " .filter(df_taxi.dropoff_datetime.isNotNull()))\n",
    "taxi_pu = (taxi_filtered\n",
    ".join(df_taxi_lookup\n",
    "       .select(\"LocationID\", \"Borough\")\n",
    "       .withColumnRenamed(\"Borough\", \"PUBorough\"), \n",
    "       df_taxi_lookup.LocationID == df_taxi.PULocationID))\n",
    "taxi = (taxi_pu.join(df_taxi_lookup\n",
    "       .select(\"LocationID\", \"Borough\")\n",
    "       .withColumnRenamed(\"Borough\", \"DOBorough\"), \n",
    "       df_taxi_lookup.LocationID == taxi_pu.DOLocationID))\n",
    "taxi_pu.show()\n",
    "taxi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51726370da594fdfa5be62f030bdd4de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+--------------------+-----------+-------------------+-----+---------------+--------------------+-------+----------+-------------+----+----------+---------+\n",
      "|DOLocationID|PULocationID|    dropoff_datetime|fare_amount|        ingested_on|month|passenger_count|     pickup_datetime|service|tip_amount|trip_distance|year|LocationID|PUBorough|\n",
      "+------------+------------+--------------------+-----------+-------------------+-----+---------------+--------------------+-------+----------+-------------+----+----------+---------+\n",
      "|          90|         148|2020-01-01T01:02:...|       null|2020-10-27 22:50:37|   01|           null|2020-01-01T00:45:...|  fhvhv|      null|         null|2020|       148|Manhattan|\n",
      "|          79|         114|2020-01-01T00:53:...|       null|2020-10-27 22:50:37|   01|           null|2020-01-01T00:47:...|  fhvhv|      null|         null|2020|       114|Manhattan|\n",
      "|         125|           4|2020-01-01T00:21:...|       null|2020-10-27 22:50:37|   01|           null|2020-01-01T00:04:...|  fhvhv|      null|         null|2020|         4|Manhattan|\n",
      "|         113|         231|2020-01-01T00:33:...|       null|2020-10-27 22:50:37|   01|           null|2020-01-01T00:26:...|  fhvhv|      null|         null|2020|       231|Manhattan|\n",
      "|         144|         114|2020-01-01T00:46:...|       null|2020-10-27 22:50:37|   01|           null|2020-01-01T00:37:...|  fhvhv|      null|         null|2020|       114|Manhattan|\n",
      "|         137|         144|2020-01-01T01:07:...|       null|2020-10-27 22:50:37|   01|           null|2020-01-01T00:49:...|  fhvhv|      null|         null|2020|       144|Manhattan|\n",
      "|         148|         249|2020-01-01T00:36:...|       null|2020-10-27 22:50:37|   01|           null|2020-01-01T00:21:...|  fhvhv|      null|         null|2020|       249|Manhattan|\n",
      "|           4|         148|2020-01-01T00:42:...|       null|2020-10-27 22:50:37|   01|           null|2020-01-01T00:38:...|  fhvhv|      null|         null|2020|       148|Manhattan|\n",
      "|           7|          79|2020-01-01T01:09:...|       null|2020-10-27 22:50:37|   01|           null|2020-01-01T00:46:...|  fhvhv|      null|         null|2020|        79|Manhattan|\n",
      "|         236|         140|2020-01-01T00:23:...|       null|2020-10-27 22:50:37|   01|           null|2020-01-01T00:15:...|  fhvhv|      null|         null|2020|       140|Manhattan|\n",
      "|         149|          75|2020-01-01T01:13:...|       null|2020-10-27 22:50:37|   01|           null|2020-01-01T00:28:...|  fhvhv|      null|         null|2020|        75|Manhattan|\n",
      "|         229|          33|2020-01-01T00:40:...|       null|2020-10-27 22:50:37|   01|           null|2020-01-01T00:14:...|  fhvhv|      null|         null|2020|        33| Brooklyn|\n",
      "|          74|         229|2020-01-01T00:59:...|       null|2020-10-27 22:50:37|   01|           null|2020-01-01T00:45:...|  fhvhv|      null|         null|2020|       229|Manhattan|\n",
      "|         238|         114|2020-01-01T00:42:...|       null|2020-10-27 22:50:37|   01|           null|2020-01-01T00:16:...|  fhvhv|      null|         null|2020|       114|Manhattan|\n",
      "|          89|          85|2020-01-01T00:45:...|       null|2020-10-27 22:50:37|   01|           null|2020-01-01T00:40:...|  fhvhv|      null|         null|2020|        85| Brooklyn|\n",
      "|         202|          61|2020-01-01T01:25:...|       null|2020-10-27 22:50:37|   01|           null|2020-01-01T00:55:...|  fhvhv|      null|         null|2020|        61| Brooklyn|\n",
      "|         265|         162|2020-01-01T01:18:...|       null|2020-10-27 22:50:37|   01|           null|2020-01-01T00:28:...|  fhvhv|      null|         null|2020|       162|Manhattan|\n",
      "|          71|          71|2020-01-01T00:22:...|       null|2020-10-27 22:50:37|   01|           null|2020-01-01T00:20:...|  fhvhv|      null|         null|2020|        71| Brooklyn|\n",
      "|          35|          71|2020-01-01T00:35:...|       null|2020-10-27 22:50:37|   01|           null|2020-01-01T00:26:...|  fhvhv|      null|         null|2020|        71| Brooklyn|\n",
      "|          76|          61|2020-01-01T01:07:...|       null|2020-10-27 22:50:37|   01|           null|2020-01-01T00:45:...|  fhvhv|      null|         null|2020|        61| Brooklyn|\n",
      "+------------+------------+--------------------+-----------+-------------------+-----+---------------+--------------------+-------+----------+-------------+----+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan(isFinalPlan=false)\n",
      "+- BroadcastNestedLoopJoin BuildRight, Inner\n",
      "   :- BroadcastHashJoin [PULocationID#1918L, DOLocationID#1917L], [LocationID#1948L, LocationID#1948L], Inner, BuildRight\n",
      "   :  :- Project [DOLocationID#1917L, PULocationID#1918L, dropoff_datetime#1919, fare_amount#1920, ingested_on#1921, month#1922, passenger_count#1923L, pickup_datetime#1924, service#1925, tip_amount#1926, trip_distance#1927, year#1928]\n",
      "   :  :  +- Filter ((((isnotnull(pickup_datetime#1924) && isnotnull(dropoff_datetime#1919)) && (DOLocationID#1917L = PULocationID#1918L)) && isnotnull(PULocationID#1918L)) && isnotnull(DOLocationID#1917L))\n",
      "   :  :     +- FileScan json [DOLocationID#1917L,PULocationID#1918L,dropoff_datetime#1919,fare_amount#1920,ingested_on#1921,month#1922,passenger_count#1923L,pickup_datetime#1924,service#1925,tip_amount#1926,trip_distance#1927,year#1928] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://ip-172-31-4-225.ec2.internal:8020/tmp/data/nyc-taxi/taxi-data/output/sec..., PartitionFilters: [], PushedFilters: [IsNotNull(pickup_datetime), IsNotNull(dropoff_datetime), IsNotNull(PULocationID), IsNotNull(DOLo..., ReadSchema: struct<DOLocationID:bigint,PULocationID:bigint,dropoff_datetime:string,fare_amount:double,ingeste...\n",
      "   :  +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true], input[0, bigint, true]))\n",
      "   :     +- Project [LocationID#1948L, Borough#1947 AS PUBorough#1959]\n",
      "   :        +- Filter isnotnull(LocationID#1948L)\n",
      "   :           +- FileScan json [Borough#1947,LocationID#1948L] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://ip-172-31-4-225.ec2.internal:8020/tmp/data/nyc-taxi/zone-lookup/output/s..., PartitionFilters: [], PushedFilters: [IsNotNull(LocationID)], ReadSchema: struct<Borough:string,LocationID:bigint>\n",
      "   +- BroadcastExchange IdentityBroadcastMode\n",
      "      +- Project [LocationID#2010L, Borough#2009 AS DOBorough#2006]\n",
      "         +- FileScan json [Borough#2009,LocationID#2010L] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://ip-172-31-4-225.ec2.internal:8020/tmp/data/nyc-taxi/zone-lookup/output/s..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Borough:string,LocationID:bigint>"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")\n",
    "df_taxi = spark.read.json(taxiPath)\n",
    "df_taxi_lookup = spark.read.json(taxiLookupPath)\n",
    "\n",
    "taxi_filtered = (df_taxi\n",
    " .filter(df_taxi.pickup_datetime.isNotNull())\n",
    " .filter(df_taxi.dropoff_datetime.isNotNull()))\n",
    "taxi_pu = (taxi_filtered\n",
    ".join(df_taxi_lookup\n",
    "       .select(\"LocationID\", \"Borough\")\n",
    "       .withColumnRenamed(\"Borough\", \"PUBorough\"), \n",
    "       df_taxi_lookup.LocationID == df_taxi.PULocationID))\n",
    "taxi = (taxi_pu.join(df_taxi_lookup\n",
    "       .select(\"LocationID\", \"Borough\")\n",
    "       .withColumnRenamed(\"Borough\", \"DOBorough\"), \n",
    "       df_taxi_lookup.LocationID == taxi_pu.DOLocationID))\n",
    "taxi_pu.show()\n",
    "taxi.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3799b281aec4f0e9db409dab3c9e41f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_monthly_totals_pyspark(taxiPath, taxiLookupPath):\n",
    "    taxi = spark.read.json(taxiPath)\n",
    "    taxi_lookup = spark.read.json(taxiLookupPath)\n",
    "    taxi_filtered = (taxi\n",
    "     .filter(taxi.pickup_datetime.isNotNull())\n",
    "     .filter(taxi.dropoff_datetime.isNotNull()))\n",
    "                     \n",
    "    groupDF = taxi_filtered.join(taxi_lookup, taxi_filtered.PULocationID == taxi_lookup.LocationID)\n",
    "    groupDF.select(\"ingested_on\").show() # expected error\n",
    "    return groupDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96f363785f84ff5841f071a112896d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"Reference 'ingested_on' is ambiguous, could be: ingested_on, ingested_on.;\"\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 58, in timer_method\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 233, in timeit\n",
      "    return Timer(stmt, setup, timer, globals).timeit(number)\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 177, in timeit\n",
      "    timing = self.inner(it, self.timer)\n",
      "  File \"<timeit-src>\", line 6, in inner\n",
      "  File \"<stdin>\", line 9, in get_monthly_totals_pyspark\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 1326, in select\n",
      "    jdf = self._jdf.select(self._jcols(*cols))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: \"Reference 'ingested_on' is ambiguous, could be: ingested_on, ingested_on.;\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_monthly_totals_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1855ca745c034e6b880e548547832d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_monthly_totals_pandas(taxiPath, taxiLookupPath):\n",
    "    taxi = pd.read_json(taxiPath)\n",
    "    taxi_lookup = pd.read_json(taxiLookupPath)\n",
    "    taxi_filtered = tax.dropna(subset=['pickup_datetime', 'dropoff_datetime'])\n",
    "    \n",
    "    groupDF = taxi_filtered.join(taxi_lookup.set_index('LocationID'), on='PULocationID')\n",
    "    groupDF['pickup_month'] = pd.to_datetime(groupDF['pickup_datetime'], format='%m%Y')\n",
    "    groupDF = groupDF.groupby('pickup_month', 'borough').agg('count').sort_values(by=['count', 'borough'], ascending=[False, True])\n",
    "    groupDF\n",
    "    return groupDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8f0528db01467287c3b70a3d56a7cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyarrow and local java libraries required for HDFS\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 58, in timer_method\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 233, in timeit\n",
      "    return Timer(stmt, setup, timer, globals).timeit(number)\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 177, in timeit\n",
      "    timing = self.inner(it, self.timer)\n",
      "  File \"<timeit-src>\", line 6, in inner\n",
      "  File \"<stdin>\", line 2, in get_monthly_totals_pandas\n",
      "  File \"/tmp/1603838861189-0/lib/python3.7/site-packages/pandas/util/_decorators.py\", line 199, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/tmp/1603838861189-0/lib/python3.7/site-packages/pandas/util/_decorators.py\", line 296, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/tmp/1603838861189-0/lib/python3.7/site-packages/pandas/io/json/_json.py\", line 594, in read_json\n",
      "    path_or_buf, encoding=encoding, compression=compression\n",
      "  File \"/tmp/1603838861189-0/lib/python3.7/site-packages/pandas/io/common.py\", line 222, in get_filepath_or_buffer\n",
      "    filepath_or_buffer, mode=mode or \"rb\", **(storage_options or {})\n",
      "  File \"/tmp/1603838861189-0/lib/python3.7/site-packages/fsspec/core.py\", line 438, in open\n",
      "    **kwargs\n",
      "  File \"/tmp/1603838861189-0/lib/python3.7/site-packages/fsspec/core.py\", line 287, in open_files\n",
      "    expand=expand,\n",
      "  File \"/tmp/1603838861189-0/lib/python3.7/site-packages/fsspec/core.py\", line 600, in get_fs_token_paths\n",
      "    cls = get_filesystem_class(protocol)\n",
      "  File \"/tmp/1603838861189-0/lib/python3.7/site-packages/fsspec/registry.py\", line 204, in get_filesystem_class\n",
      "    raise ImportError(bit[\"err\"]) from e\n",
      "ImportError: pyarrow and local java libraries required for HDFS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_monthly_totals_pandas(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbcf2e8adcbb4714966a35d022d260fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_monthly_totals_pandas(taxiPath, taxiLookupPath):\n",
    "    taxiPySpark = spark.read.json(taxiPath)\n",
    "    taxiLookupPySpark = spark.read.json(taxiLookupPath)\n",
    "    \n",
    "    taxi = taxiPySpark.toPandas()\n",
    "    taxiLookup = taxiLookupPySpark.toPandas()\n",
    "    taxiFiltered = taxi.dropna(subset=['pickup_datetime', 'dropoff_datetime'])\n",
    "    \n",
    "    groupDF = taxiFiltered.join(taxiLookup[[\"Borough\", \"LocationID\"]].set_index('LocationID'), on='PULocationID')\n",
    "    \n",
    "    groupDF['pickup_month'] = pd.to_datetime(groupDF['pickup_datetime']).dt.strftime('%Y%m')\n",
    "    returnGroupDF = groupDF.groupby(['pickup_month', 'Borough']).size().reset_index(name='count').sort_values(by=['pickup_month', 'count', 'Borough'], ascending=[False, False, True])\n",
    "    return returnGroupDF\n",
    "\n",
    "def get_monthly_totals_pyspark(taxiPath, taxiLookupPath):\n",
    "    taxi = spark.read.json(taxiPath)\n",
    "    taxiLookup = spark.read.json(taxiLookupPath)\n",
    "    \n",
    "    taxiFiltered = (taxi\n",
    "     .filter(taxi.pickup_datetime.isNotNull())\n",
    "     .filter(taxi.dropoff_datetime.isNotNull()))\n",
    "                     \n",
    "    groupDF = taxiFiltered.join(taxiLookup.select(\"Borough\", \"LocationID\"), taxiFiltered.PULocationID == taxiLookup.LocationID)\n",
    "    groupDF = groupDF.withColumn(\"pickup_month\", f.date_format(\"pickup_datetime\", \"yyyyMM\"))\n",
    "    groupDF = groupDF.groupBy(\"pickup_month\", \"borough\").count().orderBy(f.desc(\"pickup_month\"), f.desc(\"count\"), \"borough\")\n",
    "    groupDF.show()\n",
    "    return groupDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8739f2be1b0349bba5518f33c8db2d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(460.658796917,    pickup_month        Borough     count\n",
      "31       202101      Manhattan         3\n",
      "30       202007      Manhattan         6\n",
      "29       202006      Manhattan         1\n",
      "28       202005      Manhattan         5\n",
      "27       202004      Manhattan         1\n",
      "26       202003      Manhattan         5\n",
      "23       202002      Manhattan        34\n",
      "24       202002         Queens         9\n",
      "22       202002       Brooklyn         3\n",
      "21       202002          Bronx         1\n",
      "25       202002        Unknown         1\n",
      "17       202001      Manhattan  14817800\n",
      "15       202001       Brooklyn   5628274\n",
      "18       202001         Queens   4509457\n",
      "14       202001          Bronx   2536611\n",
      "20       202001        Unknown   1615247\n",
      "19       202001  Staten Island    260258\n",
      "16       202001            EWR      3779\n",
      "11       201912      Manhattan       129\n",
      "12       201912         Queens        17\n",
      "9        201912          Bronx         2\n",
      "10       201912       Brooklyn         1\n",
      "13       201912        Unknown         1\n",
      "8        201009         Queens         3\n",
      "5        200901      Manhattan        19\n",
      "6        200901         Queens        10\n",
      "4        200901          Bronx         1\n",
      "7        200901        Unknown         1\n",
      "1        200812      Manhattan         8\n",
      "2        200812         Queens         2\n",
      "3        200812        Unknown         1\n",
      "0        200301         Queens         1)"
     ]
    }
   ],
   "source": [
    "# Running this command with the original cluster size, will crash the cluster\n",
    "# All functions utilizing pandas from this command forward, need an upscaled driver node\n",
    "print(timer_method(\"get_monthly_totals_pandas(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7eedd87b5894e06a0c2c769658ba4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+--------+\n",
      "|pickup_month|      borough|   count|\n",
      "+------------+-------------+--------+\n",
      "|      202101|    Manhattan|       3|\n",
      "|      202007|    Manhattan|       6|\n",
      "|      202006|    Manhattan|       1|\n",
      "|      202005|    Manhattan|       5|\n",
      "|      202004|    Manhattan|       1|\n",
      "|      202003|    Manhattan|       5|\n",
      "|      202002|    Manhattan|      34|\n",
      "|      202002|       Queens|       9|\n",
      "|      202002|     Brooklyn|       3|\n",
      "|      202002|        Bronx|       1|\n",
      "|      202002|      Unknown|       1|\n",
      "|      202001|    Manhattan|14817800|\n",
      "|      202001|     Brooklyn| 5628274|\n",
      "|      202001|       Queens| 4509457|\n",
      "|      202001|        Bronx| 2536611|\n",
      "|      202001|      Unknown| 1615247|\n",
      "|      202001|Staten Island|  260258|\n",
      "|      202001|          EWR|    3779|\n",
      "|      201912|    Manhattan|     129|\n",
      "|      201912|       Queens|      17|\n",
      "+------------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "(36.985134702000096, DataFrame[pickup_month: string, borough: string, count: bigint])"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_monthly_totals_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Expected error for maxResultSize: This won't work. Could try the subsequent cells\n",
    "## Those restart the state of the notebook and don't work as expected\n",
    "## Need to restart the cluster and edit the Software config with: [{\"classification\":\"spark-defaults\", \"properties\":{\"spark.driver.maxResultSize\":\"5G\", \"spark.ui.killEnabled\":\"true\"}, \"configurations\":[]}]\n",
    "## Then need to reun the taxi and taxi lookup ingests\n",
    "## Run -> Run All Above Selected Cell\n",
    "## Second expected error for {\"msg\":\"requirement failed: Session isn't active.\"} and will hang. Driver node ran out of mem. Will need to go and upscale\n",
    "print(spark.conf.get('spark.driver.maxResultSize'))\n",
    "spark.conf.set(\"spark.driver.maxResultSize\", \"5G\")\n",
    "print(spark.conf.get('spark.driver.maxResultSize'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"conf\":{\"spark.driver.maxResultSize\":\"5G\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb28e6e68b554461bcf34c41d8b3facb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_monthly_totals_concat_pandas(taxiPath, taxiLookupPath):\n",
    "    taxiPySpark = spark.read.json(taxiPath)\n",
    "    taxiLookupPySpark = spark.read.json(taxiLookupPath)\n",
    "    \n",
    "    taxi = taxiPySpark.toPandas()\n",
    "    taxiLookup = taxiLookupPySpark.toPandas()\n",
    "    taxiFiltered = taxi.dropna(subset=['pickup_datetime', 'dropoff_datetime'])\n",
    "    \n",
    "    groupDF = taxiFiltered.join(taxiLookup[[\"Borough\", \"LocationID\"]].set_index('LocationID'), on='PULocationID')\n",
    "    groupDF['pickup_month'] = groupDF['year'] + groupDF['month']\n",
    "    groupDF = groupDF.groupby(['pickup_month', 'Borough']).size().reset_index(name='count').sort_values(by=['pickup_month', 'count', 'Borough'], ascending=[False, False, True])\n",
    "    return groupDF\n",
    "    \n",
    "def get_monthly_totals_concat_pyspark(taxiPath, taxiLookupPath):\n",
    "    taxi = spark.read.json(taxiPath)\n",
    "    taxiLookup = spark.read.json(taxiLookupPath)\n",
    "    taxiFiltered = (taxi\n",
    "     .filter(taxi.pickup_datetime.isNotNull())\n",
    "     .filter(taxi.dropoff_datetime.isNotNull()))\n",
    "        \n",
    "    groupDF = taxiFiltered.join(taxiLookup, taxiFiltered.PULocationID == taxiLookup.LocationID)\n",
    "    groupDF = groupDF.withColumn(\"pickup_month\", f.concat(\"year\", \"month\")).select(\"pickup_datetime\", \"borough\", \"pickup_month\")\n",
    "    groupDF = groupDF.groupBy(\"pickup_month\", \"borough\").count().orderBy(f.desc(\"pickup_month\"), f.desc(\"count\"), \"borough\")\n",
    "    groupDF.show()\n",
    "    return groupDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f6132aaab14c94a0dedd026295676e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(322.6805907979999,   pickup_month        Borough     count\n",
      "3       202001      Manhattan  14818011\n",
      "1       202001       Brooklyn   5628278\n",
      "4       202001         Queens   4509499\n",
      "0       202001          Bronx   2536615\n",
      "6       202001        Unknown   1615251\n",
      "5       202001  Staten Island    260258\n",
      "2       202001            EWR      3779)"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_monthly_totals_concat_pandas(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc7a62b615d43b087c6dba535ca1284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+--------+\n",
      "|pickup_month|      borough|   count|\n",
      "+------------+-------------+--------+\n",
      "|      202001|    Manhattan|14818011|\n",
      "|      202001|     Brooklyn| 5628278|\n",
      "|      202001|       Queens| 4509499|\n",
      "|      202001|        Bronx| 2536615|\n",
      "|      202001|      Unknown| 1615251|\n",
      "|      202001|Staten Island|  260258|\n",
      "|      202001|          EWR|    3779|\n",
      "+------------+-------------+--------+\n",
      "\n",
      "(22.389204961999894, DataFrame[pickup_month: string, borough: string, count: bigint])"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_monthly_totals_concat_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.5 - Case Study 2: Month over month, get the borough with the most amount of pickups per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1348aa54ac244285bf035185439b42c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_most_pickups_per_month_pandas(taxiPath, taxiLookupPath):\n",
    "    inputDF = get_monthly_totals_pandas(taxiPath, taxiLookupPath)\n",
    "    firstDF = inputDF.groupby(\"pickup_month\").head(1).reset_index(drop=True)#.first()#sort_values(by=['pickup_month', 'count'], ascending=[True, False]).head(1).reset_index(drop=True)\n",
    "    firstDF\n",
    "    return firstDF\n",
    "\n",
    "def get_most_pickups_per_month_pyspark(taxiPath, taxiLookupPath):\n",
    "    inputDF = get_monthly_totals_pyspark(taxiPath, taxiLookupPath)\n",
    "    firstDF = inputDF.orderBy(f.desc(\"pickup_month\"), f.desc(\"count\")).groupBy(\"pickup_month\").agg(f.first(\"borough\")).orderBy(f.desc(\"pickup_month\"))\n",
    "    firstDF.explain()\n",
    "    firstDF.show()\n",
    "    return firstDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df3ed4ce4244e4588210c3b7d703b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(519.8932285199999,    pickup_month    Borough     count\n",
      "0        202101  Manhattan         3\n",
      "1        202007  Manhattan         6\n",
      "2        202006  Manhattan         1\n",
      "3        202005  Manhattan         5\n",
      "4        202004  Manhattan         1\n",
      "5        202003  Manhattan         5\n",
      "6        202002  Manhattan        34\n",
      "7        202001  Manhattan  14817800\n",
      "8        201912  Manhattan       129\n",
      "9        201009     Queens         3\n",
      "10       200901  Manhattan        19\n",
      "11       200812  Manhattan         8\n",
      "12       200301     Queens         1)"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_most_pickups_per_month_pandas(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8359ca3e250444e0b8f356ff4f088ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+--------+\n",
      "|pickup_month|      borough|   count|\n",
      "+------------+-------------+--------+\n",
      "|      202101|    Manhattan|       3|\n",
      "|      202007|    Manhattan|       6|\n",
      "|      202006|    Manhattan|       1|\n",
      "|      202005|    Manhattan|       5|\n",
      "|      202004|    Manhattan|       1|\n",
      "|      202003|    Manhattan|       5|\n",
      "|      202002|    Manhattan|      34|\n",
      "|      202002|       Queens|       9|\n",
      "|      202002|     Brooklyn|       3|\n",
      "|      202002|        Bronx|       1|\n",
      "|      202002|      Unknown|       1|\n",
      "|      202001|    Manhattan|14817800|\n",
      "|      202001|     Brooklyn| 5628274|\n",
      "|      202001|       Queens| 4509457|\n",
      "|      202001|        Bronx| 2536611|\n",
      "|      202001|      Unknown| 1615247|\n",
      "|      202001|Staten Island|  260258|\n",
      "|      202001|          EWR|    3779|\n",
      "|      201912|    Manhattan|     129|\n",
      "|      201912|       Queens|      17|\n",
      "+------------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan(isFinalPlan=false)\n",
      "+- Sort [pickup_month#2784 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(pickup_month#2784 DESC NULLS LAST, 1000)\n",
      "      +- SortAggregate(key=[pickup_month#2784], functions=[first(borough#2730, false)])\n",
      "         +- Sort [pickup_month#2784 ASC NULLS FIRST], false, 0\n",
      "            +- Exchange hashpartitioning(pickup_month#2784, 1000)\n",
      "               +- SortAggregate(key=[pickup_month#2784], functions=[partial_first(borough#2730, false)])\n",
      "                  +- Sort [pickup_month#2784 ASC NULLS FIRST], false, 0\n",
      "                     +- Project [pickup_month#2784, borough#2730]\n",
      "                        +- Sort [pickup_month#2784 DESC NULLS LAST, count#2816L DESC NULLS LAST, borough#2730 ASC NULLS FIRST], true, 0\n",
      "                           +- Exchange rangepartitioning(pickup_month#2784 DESC NULLS LAST, count#2816L DESC NULLS LAST, borough#2730 ASC NULLS FIRST, 1000)\n",
      "                              +- HashAggregate(keys=[pickup_month#2784, borough#2730], functions=[count(1)])\n",
      "                                 +- Exchange hashpartitioning(pickup_month#2784, borough#2730, 1000)\n",
      "                                    +- HashAggregate(keys=[pickup_month#2784, borough#2730], functions=[partial_count(1)])\n",
      "                                       +- Project [Borough#2730, date_format(cast(pickup_datetime#2707 as timestamp), yyyyMM, Some(UTC)) AS pickup_month#2784]\n",
      "                                          +- BroadcastHashJoin [PULocationID#2701L], [LocationID#2731L], Inner, BuildRight\n",
      "                                             :- Project [PULocationID#2701L, pickup_datetime#2707]\n",
      "                                             :  +- Filter ((isnotnull(pickup_datetime#2707) && isnotnull(dropoff_datetime#2702)) && isnotnull(PULocationID#2701L))\n",
      "                                             :     +- FileScan json [PULocationID#2701L,dropoff_datetime#2702,pickup_datetime#2707] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://ip-172-31-4-225.ec2.internal:8020/tmp/data/nyc-taxi/taxi-data/output/sec..., PartitionFilters: [], PushedFilters: [IsNotNull(pickup_datetime), IsNotNull(dropoff_datetime), IsNotNull(PULocationID)], ReadSchema: struct<PULocationID:bigint,dropoff_datetime:string,pickup_datetime:string>\n",
      "                                             +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, bigint, true]))\n",
      "                                                +- Project [Borough#2730, LocationID#2731L]\n",
      "                                                   +- Filter isnotnull(LocationID#2731L)\n",
      "                                                      +- FileScan json [Borough#2730,LocationID#2731L] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://ip-172-31-4-225.ec2.internal:8020/tmp/data/nyc-taxi/zone-lookup/output/s..., PartitionFilters: [], PushedFilters: [IsNotNull(LocationID)], ReadSchema: struct<Borough:string,LocationID:bigint>\n",
      "+------------+---------------------+\n",
      "|pickup_month|first(borough, false)|\n",
      "+------------+---------------------+\n",
      "|      202101|            Manhattan|\n",
      "|      202007|            Manhattan|\n",
      "|      202006|            Manhattan|\n",
      "|      202005|            Manhattan|\n",
      "|      202004|            Manhattan|\n",
      "|      202003|            Manhattan|\n",
      "|      202002|            Manhattan|\n",
      "|      202001|            Manhattan|\n",
      "|      201912|                Bronx|\n",
      "|      201009|               Queens|\n",
      "|      200901|               Queens|\n",
      "|      200812|               Queens|\n",
      "|      200301|               Queens|\n",
      "+------------+---------------------+\n",
      "\n",
      "(63.58880115300008, DataFrame[pickup_month: string, first(borough, false): string])"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_most_pickups_per_month_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1ef376c0ca436787f94727d86dceff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_most_pickups_per_month_window_pyspark(taxiPath, taxiLookupPath):\n",
    "    from pyspark.sql import Window\n",
    "    inputDF = get_monthly_totals_pyspark(taxiPath, taxiLookupPath)\n",
    "    win = Window.partitionBy(\"pickup_month\").orderBy(f.desc(\"count\"))\n",
    "    firstDF = inputDF.withColumn(\"row_num\", f.row_number().over(win)).where(\"row_num == 1\")\n",
    "    firstDF = firstDF.orderBy(f.desc(\"pickup_month\"))\n",
    "    firstDF.explain()\n",
    "    firstDF.show(firstDF.count())\n",
    "    return firstDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1a5bc3e3fc4dd98f3de921f7273462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+--------+\n",
      "|pickup_month|      borough|   count|\n",
      "+------------+-------------+--------+\n",
      "|      202101|    Manhattan|       3|\n",
      "|      202007|    Manhattan|       6|\n",
      "|      202006|    Manhattan|       1|\n",
      "|      202005|    Manhattan|       5|\n",
      "|      202004|    Manhattan|       1|\n",
      "|      202003|    Manhattan|       5|\n",
      "|      202002|    Manhattan|      34|\n",
      "|      202002|       Queens|       9|\n",
      "|      202002|     Brooklyn|       3|\n",
      "|      202002|        Bronx|       1|\n",
      "|      202002|      Unknown|       1|\n",
      "|      202001|    Manhattan|14817800|\n",
      "|      202001|     Brooklyn| 5628274|\n",
      "|      202001|       Queens| 4509457|\n",
      "|      202001|        Bronx| 2536611|\n",
      "|      202001|      Unknown| 1615247|\n",
      "|      202001|Staten Island|  260258|\n",
      "|      202001|          EWR|    3779|\n",
      "|      201912|    Manhattan|     129|\n",
      "|      201912|       Queens|      17|\n",
      "+------------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan(isFinalPlan=false)\n",
      "+- Sort [pickup_month#2962 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(pickup_month#2962 DESC NULLS LAST, 1000)\n",
      "      +- Filter (isnotnull(row_num#3016) && (row_num#3016 = 1))\n",
      "         +- Window [row_number() windowspecdefinition(pickup_month#2962, count#2994L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#3016], [pickup_month#2962], [count#2994L DESC NULLS LAST]\n",
      "            +- Sort [pickup_month#2962 ASC NULLS FIRST, count#2994L DESC NULLS LAST], false, 0\n",
      "               +- Exchange hashpartitioning(pickup_month#2962, 1000)\n",
      "                  +- Sort [pickup_month#2962 DESC NULLS LAST, count#2994L DESC NULLS LAST, borough#2908 ASC NULLS FIRST], true, 0\n",
      "                     +- Exchange rangepartitioning(pickup_month#2962 DESC NULLS LAST, count#2994L DESC NULLS LAST, borough#2908 ASC NULLS FIRST, 1000)\n",
      "                        +- HashAggregate(keys=[pickup_month#2962, borough#2908], functions=[count(1)])\n",
      "                           +- Exchange hashpartitioning(pickup_month#2962, borough#2908, 1000)\n",
      "                              +- HashAggregate(keys=[pickup_month#2962, borough#2908], functions=[partial_count(1)])\n",
      "                                 +- Project [Borough#2908, date_format(cast(pickup_datetime#2885 as timestamp), yyyyMM, Some(UTC)) AS pickup_month#2962]\n",
      "                                    +- BroadcastHashJoin [PULocationID#2879L], [LocationID#2909L], Inner, BuildRight\n",
      "                                       :- Project [PULocationID#2879L, pickup_datetime#2885]\n",
      "                                       :  +- Filter ((isnotnull(pickup_datetime#2885) && isnotnull(dropoff_datetime#2880)) && isnotnull(PULocationID#2879L))\n",
      "                                       :     +- FileScan json [PULocationID#2879L,dropoff_datetime#2880,pickup_datetime#2885] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://ip-172-31-4-225.ec2.internal:8020/tmp/data/nyc-taxi/taxi-data/output/sec..., PartitionFilters: [], PushedFilters: [IsNotNull(pickup_datetime), IsNotNull(dropoff_datetime), IsNotNull(PULocationID)], ReadSchema: struct<PULocationID:bigint,dropoff_datetime:string,pickup_datetime:string>\n",
      "                                       +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, bigint, true]))\n",
      "                                          +- Project [Borough#2908, LocationID#2909L]\n",
      "                                             +- Filter isnotnull(LocationID#2909L)\n",
      "                                                +- FileScan json [Borough#2908,LocationID#2909L] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://ip-172-31-4-225.ec2.internal:8020/tmp/data/nyc-taxi/zone-lookup/output/s..., PartitionFilters: [], PushedFilters: [IsNotNull(LocationID)], ReadSchema: struct<Borough:string,LocationID:bigint>\n",
      "+------------+---------+--------+-------+\n",
      "|pickup_month|  borough|   count|row_num|\n",
      "+------------+---------+--------+-------+\n",
      "|      202101|Manhattan|       3|      1|\n",
      "|      202007|Manhattan|       6|      1|\n",
      "|      202006|Manhattan|       1|      1|\n",
      "|      202005|Manhattan|       5|      1|\n",
      "|      202004|Manhattan|       1|      1|\n",
      "|      202003|Manhattan|       5|      1|\n",
      "|      202002|Manhattan|      34|      1|\n",
      "|      202001|Manhattan|14817800|      1|\n",
      "|      201912|Manhattan|     129|      1|\n",
      "|      201009|   Queens|       3|      1|\n",
      "|      200901|Manhattan|      19|      1|\n",
      "|      200812|Manhattan|       8|      1|\n",
      "|      200301|   Queens|       1|      1|\n",
      "+------------+---------+--------+-------+\n",
      "\n",
      "(85.6818935350002, DataFrame[pickup_month: string, borough: string, count: bigint, row_num: int])"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_most_pickups_per_month_window_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab - 2.6 Run and time the overall pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset notebook kernel\n",
    "def ingest_main():\n",
    "    ingest_taxi_data_multi_service(\"s3://nyc-tlc/trip data/yellow_tripdata_2020-01.csv\")\n",
    "    ingest_taxi_lookup(\"s3://nyc-tlc/misc/taxi _zone_lookup.csv\")\n",
    "    get_most_pickups_per_month_window_pyspark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timer_method(\"ingest_main\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
