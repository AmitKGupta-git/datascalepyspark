{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b79533cb7b5483f97a3a5343c000cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1603063175185_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-6-49.us-east-2.compute.internal:20888/proxy/application_1603063175185_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-8-43.us-east-2.compute.internal:8042/node/containerlogs/container_1603063175185_0003_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Using cached https://files.pythonhosted.org/packages/2f/08/f1ff665147a5d75b871bbe5ba76916f6490419c52a33e588385c4b69281b/boto3-1.15.18-py2.py3-none-any.whl\n",
      "Collecting botocore<1.19.0,>=1.18.18 (from boto3)\n",
      "  Using cached https://files.pythonhosted.org/packages/2d/72/984ac8f33b5c8df5ff63f323a8724f65b4d0f8956968b942b77d35d3a1ef/botocore-1.18.18-py2.py3-none-any.whl\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0 (from boto3)\n",
      "  Using cached https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl\n",
      "Collecting urllib3<1.26,>=1.20; python_version != \"3.4\" (from botocore<1.19.0,>=1.18.18->boto3)\n",
      "  Using cached https://files.pythonhosted.org/packages/9f/f0/a391d1463ebb1b233795cabfc0ef38d3db4442339de68f847026199e69d7/urllib3-1.25.10-py2.py3-none-any.whl\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore<1.19.0,>=1.18.18->boto3)\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.19.0,>=1.18.18->boto3)\n",
      "Installing collected packages: urllib3, python-dateutil, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.15.18 botocore-1.18.18 python-dateutil-2.8.1 s3transfer-0.3.3 urllib3-1.25.10\n",
      "\n",
      "Collecting pandas\n",
      "  Using cached https://files.pythonhosted.org/packages/25/47/22fc373440e144e2111363adaa07abb09ec1f03fbc071b6d9fc0bbf65f68/pandas-1.1.3-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib64/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /mnt/tmp/1603064188681-0/lib/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.1.3\n",
      "\n",
      "Collecting requests\n",
      "  Using cached https://files.pythonhosted.org/packages/45/1e/0c169c6a5381e241ba7404532c16a21d86ab872c9bed8bdcd4c423954103/requests-2.24.0-py2.py3-none-any.whl\n",
      "Collecting chardet<4,>=3.0.2 (from requests)\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\n",
      "Collecting idna<3,>=2.5 (from requests)\n",
      "  Using cached https://files.pythonhosted.org/packages/a2/38/928ddce2273eaa564f6f50de919327bf3a00f091b5baba8dfa9460f3a8a8/idna-2.10-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /mnt/tmp/1603064188681-0/lib/python3.7/site-packages (from requests)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached https://files.pythonhosted.org/packages/5e/c4/6c4fe722df5343c33226f0b4e0bb042e4dc13483228b4718baf286f86d87/certifi-2020.6.20-py2.py3-none-any.whl\n",
      "Installing collected packages: chardet, idna, certifi, requests\n",
      "Successfully installed certifi-2020.6.20 chardet-3.0.4 idna-2.10 requests-2.24.0\n",
      "\n",
      "Collecting s3fs\n",
      "  Using cached https://files.pythonhosted.org/packages/a7/58/732ea1c735d725b1cc4cf365ae6326c22569a5e88c8502d13844e91f08ef/s3fs-0.5.1-py3-none-any.whl\n",
      "Collecting aiobotocore>=1.0.1 (from s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/73/25/a81b015035012131056a6b7a339eec052f86f33e35fd91f160e961ea2a5e/aiobotocore-1.1.2-py3-none-any.whl\n",
      "Collecting fsspec>=0.8.0 (from s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/a5/8b/1df260f860f17cb08698170153ef7db672c497c1840dcc8613ce26a8a005/fsspec-0.8.4-py3-none-any.whl\n",
      "Collecting aioitertools>=0.5.1 (from aiobotocore>=1.0.1->s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/83/42/90df27c516ce54fa26964bc4a632ecaf352c7e99574b515255e48b4a7cc7/aioitertools-0.7.0-py3-none-any.whl\n",
      "Collecting botocore<1.17.45,>=1.17.44 (from aiobotocore>=1.0.1->s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/1e/6a/b6490235c01c941a24a86235e2a641e9505cf0ce4b4968d4987573d92bec/botocore-1.17.44-py2.py3-none-any.whl\n",
      "Collecting wrapt>=1.10.10 (from aiobotocore>=1.0.1->s3fs)\n",
      "Collecting aiohttp>=3.3.1 (from aiobotocore>=1.0.1->s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/12/a2/ca3ba17c50ebeb3e7473330d8d1ce08fb83506a9bc985bcc0716354d2018/aiohttp-3.6.3-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting typing_extensions>=3.7 (from aioitertools>=0.5.1->aiobotocore>=1.0.1->s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/60/7a/e881b5abb54db0e6e671ab088d079c57ce54e8a01a3ca443f561ccadb37e/typing_extensions-3.7.4.3-py3-none-any.whl\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /mnt/tmp/1603064188681-0/lib/python3.7/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /mnt/tmp/1603064188681-0/lib/python3.7/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs)\n",
      "Collecting docutils<0.16,>=0.10 (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /mnt/tmp/1603064188681-0/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "Collecting async-timeout<4.0,>=3.0 (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
      "Collecting attrs>=17.3.0 (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/14/df/479736ae1ef59842f512548bacefad1abed705e400212acba43f9b0fa556/attrs-20.2.0-py2.py3-none-any.whl\n",
      "Collecting yarl<1.6.0,>=1.0 (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/5b/1c/556b99a3a11916e05cd2128367f507dc330fc30ed1f5991e1ffe4dabf635/yarl-1.5.1-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting multidict<5.0,>=4.5 (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/85/b8/a9fe777dab4c6aa067b516a34fe995213707e490ea1e72f823949a830a6a/multidict-4.7.6-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs)\n",
      "Requirement already satisfied: idna>=2.0 in /mnt/tmp/1603064188681-0/lib/python3.7/site-packages (from yarl<1.6.0,>=1.0->aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "Installing collected packages: typing-extensions, aioitertools, docutils, botocore, wrapt, async-timeout, attrs, multidict, yarl, aiohttp, aiobotocore, fsspec, s3fs\n",
      "  Found existing installation: botocore 1.18.18\n",
      "    Uninstalling botocore-1.18.18:\n",
      "      Successfully uninstalled botocore-1.18.18\n",
      "Successfully installed aiobotocore-1.1.2 aiohttp-3.6.3 aioitertools-0.7.0 async-timeout-3.0.1 attrs-20.2.0 botocore-1.17.44 docutils-0.15.2 fsspec-0.8.4 multidict-4.7.6 s3fs-0.5.1 typing-extensions-3.7.4.3 wrapt-1.12.1 yarl-1.5.1"
     ]
    }
   ],
   "source": [
    "# Install libraries within the notebook scope\n",
    "sc.install_pypi_package(\"boto3\")\n",
    "sc.install_pypi_package(\"pandas\")\n",
    "sc.install_pypi_package(\"requests\")\n",
    "sc.install_pypi_package(\"s3fs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af7fcf74cac4a7bad2a4a662cc6cd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import boto3\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql import functions as f, types as t\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "import s3fs\n",
    "import subprocess\n",
    "import timeit\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Removes truncation of columns, column values in Pandas\n",
    "# by default\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "# Monkey patching the DataFrame transform method for Spark 2.4\n",
    "# This is available by default in Spark 3.0\n",
    "def transform(self, f):\n",
    "    return f(self)\n",
    "DataFrame.transform = transform\n",
    "\n",
    "# Override the timeit template to return the command's\n",
    "# return value in addition to the time\n",
    "# Reference: https://stackoverflow.com/questions/24812253/how-can-i-capture-return-value-with-python-timeit-module\n",
    "timeit.template = \"\"\"\n",
    "def inner(_it, _timer{init}):\n",
    "    {setup}\n",
    "    _t0 = _timer()\n",
    "    for _i in _it:\n",
    "        retval = {stmt}\n",
    "    _t1 = _timer()\n",
    "    return _t1 - _t0, retval\n",
    "\"\"\"\n",
    "\n",
    "def shell_cmd(cmd):\n",
    "    \"\"\"\n",
    "    Wrapper for running shell commands and printing the output\n",
    "    Some helpful recipes:\n",
    "    - List files on hdfs: shell_cmd(\"hdfs dfs -ls hdfs:///tmp/data/\")\n",
    "    - Remove files from hdfs: shell_cmd(\"hdfs dfs -rm -r hdfs:///tmp/data/test_pyspark\")\n",
    "    \"\"\"\n",
    "    for line in subprocess.check_output(cmd, shell=True).split(b'\\n'):\n",
    "        print(line)\n",
    "\n",
    "def timer_method(cmd):\n",
    "    \"\"\"\n",
    "    Wrapper for timeit that returns the value of a function and its runtime\n",
    "    To use, pass a string of the function you wish to time\n",
    "    Example: \n",
    "     run_time, result = timer_method(\"myfunction(arg1, arg2)\")\n",
    "    \"\"\"\n",
    "    # Setting globals = globals() enables the timeit function\n",
    "    # to return the value generated by cmd\n",
    "    return timeit.timeit(cmd, number=1, globals = globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set your s3 bucket name\n",
    "This should be data-scale-oreilly-{your name}   \n",
    "If you dont remember check the [S3 console](https://s3.console.aws.amazon.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e53de75f424a38bc7fa138828c7279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MY_BUCKET_NAME = \"data-scale-oreilly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting from an S3 bucket - NYC Taxi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "* Taxi data \n",
    "* Data dictionaries \n",
    "* Taxi zone lookup table\n",
    "\n",
    "Take a few minutes to look through the Data Dictionaries and Metadata and the Taxi Zone Maps and Lookup Tables. What are some things you notice about the data?\n",
    "\n",
    "Data ingestion has the ultimate goal of collecting, aggregating, and surfacing data for a specific purpose; an analysis, an API, a dashboard, etc. Think about how you might use the taxi data to answer the following questions:\n",
    "\n",
    "1. Which borough is the most popular pickup or drop off spot?\n",
    "1. Are green taxis more popular for trips within the same borough vs yellow taxis?\n",
    "1. Build a recommendation engine that predicts surge pricing for a given time of day based on historical data  \n",
    "\n",
    "With this in mind, lets work through bringing this data onto the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15b1a53f0d04a87bab3194313b4009a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: 20.69798012299998\n",
      "<string>:45: DtypeWarning: Columns (0,3,5,9) have mixed types.Specify dtype option on import or set low_memory=False."
     ]
    }
   ],
   "source": [
    "# Note, if you copy the link from the taxi data website you will see:\n",
    "# https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-01.csv\n",
    "# Two things - first, the portion of the URL following \"aws.com\" is the \n",
    "# bucket name. Second, in \"trip+data\" the \"+\" is a space\n",
    "taxi_data_path = \"s3://nyc-tlc/trip data/yellow_tripdata_2020-01.csv\"\n",
    "\n",
    "# When working with big data it can be challenging to view the data. How would you \n",
    "# go about getting a sample of this data? (download it, use requests, pandas, etc)\n",
    "\n",
    "# Pandas uses s3fs to read_csv from s3:\n",
    "pd_run_time, pd_df_taxi = timer_method(\"pd.read_csv(taxi_data_path, keep_default_na=False)\")\n",
    "print(f\"runtime: {pd_run_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ba13f64e2f4b318ca2a7954f2b3c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VendorID                  object\n",
      "tpep_pickup_datetime      object\n",
      "tpep_dropoff_datetime     object\n",
      "passenger_count           object\n",
      "trip_distance            float64\n",
      "RatecodeID                object\n",
      "store_and_fwd_flag        object\n",
      "PULocationID               int64\n",
      "DOLocationID               int64\n",
      "payment_type              object\n",
      "fare_amount              float64\n",
      "extra                    float64\n",
      "mta_tax                  float64\n",
      "tip_amount               float64\n",
      "tolls_amount             float64\n",
      "improvement_surcharge    float64\n",
      "total_amount             float64\n",
      "congestion_surcharge     float64\n",
      "dtype: object"
     ]
    }
   ],
   "source": [
    "# Take a look at the data. Notice how pandas will try to assign types. Is this desirable?\n",
    "# Why or why not?\n",
    "# Since we have column names it also seems this data has a header\n",
    "pd_df_taxi.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f6bbf3a9ab4848b3f4c7a5acfdcb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: 5.77777310700003"
     ]
    }
   ],
   "source": [
    "# For reference, look at the Spark DataFrameReader, csv:\n",
    "# https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html\n",
    "ps_run_time, ps_df_taxi = timer_method(\"spark.read.option('header', True).csv(taxi_data_path)\")\n",
    "print(f\"runtime: {ps_run_time}\")\n",
    "\n",
    "# Talk through the spark UI here, partcularly note that the cell number will show up in the SparkUI\n",
    "# Job list next to the Job id. For example. this would be Job 2 (cell number)\n",
    "# Look at runtime and talk about lazy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1cde4ae65e141828da6643469f9753a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|1       |2020-01-01 00:28:15 |2020-01-01 00:33:03  |1              |1.20         |1         |N                 |238         |239         |1           |6          |3    |0.5    |1.47      |0           |0.3                  |11.27       |2.5                 |\n",
      "|1       |2020-01-01 00:35:39 |2020-01-01 00:43:04  |1              |1.20         |1         |N                 |239         |238         |1           |7          |3    |0.5    |1.5       |0           |0.3                  |12.3        |2.5                 |\n",
      "|1       |2020-01-01 00:47:41 |2020-01-01 00:53:52  |1              |.60          |1         |N                 |238         |238         |1           |6          |3    |0.5    |1         |0           |0.3                  |10.8        |2.5                 |\n",
      "|1       |2020-01-01 00:55:23 |2020-01-01 01:00:14  |1              |.80          |1         |N                 |238         |151         |1           |5.5        |0.5  |0.5    |1.36      |0           |0.3                  |8.16        |0                   |\n",
      "|2       |2020-01-01 00:01:58 |2020-01-01 00:04:16  |1              |.00          |1         |N                 |193         |193         |2           |3.5        |0.5  |0.5    |0         |0           |0.3                  |4.8         |0                   |\n",
      "|2       |2020-01-01 00:09:44 |2020-01-01 00:10:37  |1              |.03          |1         |N                 |7           |193         |2           |2.5        |0.5  |0.5    |0         |0           |0.3                  |3.8         |0                   |\n",
      "|2       |2020-01-01 00:39:25 |2020-01-01 00:39:29  |1              |.00          |1         |N                 |193         |193         |1           |2.5        |0.5  |0.5    |0.01      |0           |0.3                  |3.81        |0                   |\n",
      "|2       |2019-12-18 15:27:49 |2019-12-18 15:28:59  |1              |.00          |5         |N                 |193         |193         |1           |0.01       |0    |0      |0         |0           |0.3                  |2.81        |2.5                 |\n",
      "|2       |2019-12-18 15:30:35 |2019-12-18 15:31:35  |4              |.00          |1         |N                 |193         |193         |1           |2.5        |0.5  |0.5    |0         |0           |0.3                  |6.3         |2.5                 |\n",
      "|1       |2020-01-01 00:29:01 |2020-01-01 00:40:28  |2              |.70          |1         |N                 |246         |48          |1           |8          |3    |0.5    |2.35      |0           |0.3                  |14.15       |2.5                 |\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "ps_df_taxi.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895bc69774a94508b6437c4e13c1d1d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- congestion_surcharge: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "# Lets see how the spark dataframe reader interpreted the data\n",
    "# Talk about nullable vs non nullable and maybe a small bit about data schemas\n",
    "ps_df_taxi.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2319e1d62f6643a081865a179c196a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'path hdfs://ip-172-31-6-49.us-east-2.compute.internal:8020/tmp/input/taxi_data already exists.;'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 932, in csv\n",
      "    self._jwrite.csv(path)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: 'path hdfs://ip-172-31-6-49.us-east-2.compute.internal:8020/tmp/input/taxi_data already exists.;'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Talk through ingest practices around retaining original data vs augmenting\n",
    "# For example, we may want to keep the data in its default format so we can\n",
    "# refer back to it if there are bugs in our data ingestion code\n",
    "ps_df_taxi.write.option(\"header\", True).csv(\"hdfs:///tmp/input/taxi_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c504059d67b74027820b2df10457ceab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Found 9 items'\n",
      "b'-rw-r--r--   1 livy hadoop          0 2020-10-18 23:28 hdfs:///tmp/input/taxi_data/_SUCCESS'\n",
      "b'-rw-r--r--   1 livy hadoop   73917327 2020-10-18 23:28 hdfs:///tmp/input/taxi_data/part-00000-4ec8700e-82dd-454f-a072-d3a2943c8375-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   73919562 2020-10-18 23:28 hdfs:///tmp/input/taxi_data/part-00001-4ec8700e-82dd-454f-a072-d3a2943c8375-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   73920337 2020-10-18 23:28 hdfs:///tmp/input/taxi_data/part-00002-4ec8700e-82dd-454f-a072-d3a2943c8375-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   73921216 2020-10-18 23:28 hdfs:///tmp/input/taxi_data/part-00003-4ec8700e-82dd-454f-a072-d3a2943c8375-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   73918682 2020-10-18 23:28 hdfs:///tmp/input/taxi_data/part-00004-4ec8700e-82dd-454f-a072-d3a2943c8375-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   73921304 2020-10-18 23:28 hdfs:///tmp/input/taxi_data/part-00005-4ec8700e-82dd-454f-a072-d3a2943c8375-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   73919745 2020-10-18 23:28 hdfs:///tmp/input/taxi_data/part-00006-4ec8700e-82dd-454f-a072-d3a2943c8375-c000.csv'\n",
      "b'-rw-r--r--   1 livy hadoop   70423763 2020-10-18 23:28 hdfs:///tmp/input/taxi_data/part-00007-4ec8700e-82dd-454f-a072-d3a2943c8375-c000.csv'\n",
      "b''"
     ]
    }
   ],
   "source": [
    "# Discuss how spark writes files out\n",
    "shell_cmd(\"hdfs dfs -ls hdfs:///tmp/input/taxi_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8fd64ae17ee4a91905b654baea6fb15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|PULocationID|DOLocationID|fare_amount|tip_amount|\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+\n",
      "|2020-01-01 00:28:15 |2020-01-01 00:33:03  |1              |1.20         |238         |239         |6          |1.47      |\n",
      "|2020-01-01 00:35:39 |2020-01-01 00:43:04  |1              |1.20         |239         |238         |7          |1.5       |\n",
      "|2020-01-01 00:47:41 |2020-01-01 00:53:52  |1              |.60          |238         |238         |6          |1         |\n",
      "|2020-01-01 00:55:23 |2020-01-01 01:00:14  |1              |.80          |238         |151         |5.5        |1.36      |\n",
      "|2020-01-01 00:01:58 |2020-01-01 00:04:16  |1              |.00          |193         |193         |3.5        |0         |\n",
      "|2020-01-01 00:09:44 |2020-01-01 00:10:37  |1              |.03          |7           |193         |2.5        |0         |\n",
      "|2020-01-01 00:39:25 |2020-01-01 00:39:29  |1              |.00          |193         |193         |2.5        |0.01      |\n",
      "|2019-12-18 15:27:49 |2019-12-18 15:28:59  |1              |.00          |193         |193         |0.01       |0         |\n",
      "|2019-12-18 15:30:35 |2019-12-18 15:31:35  |4              |.00          |193         |193         |2.5        |0         |\n",
      "|2020-01-01 00:29:01 |2020-01-01 00:40:28  |2              |.70          |246         |48          |8          |2.35      |\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "# Examine the data, what do you notice?\n",
    "column_subset = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID', 'fare_amount', 'tip_amount']\n",
    "ps_df_taxi.select(*column_subset).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d15aa1d04143f78f2d0e60c830dccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---------------------+------------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "|summary|tpep_pickup_datetime|tpep_dropoff_datetime|   passenger_count|     trip_distance|      PULocationID|     DOLocationID|       fare_amount|        tip_amount|\n",
      "+-------+--------------------+---------------------+------------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "|  count|             6405008|              6405008|           6339567|           6405008|           6405008|          6405008|           6405008|           6405008|\n",
      "|   mean|                null|                 null|1.5153326717739555|2.9296439333096704|164.73225778952968|162.6626908194338|  12.6941081197704|2.1893418306424284|\n",
      "| stddev|                null|                 null|1.1515942134278123|  83.1591059732504| 65.54373944111667| 69.9126062949612|12.127295340046542| 2.760028392378421|\n",
      "|    min| 2003-01-01 00:07:17|  2003-01-01 14:16:59|                 0|              -.01|                 1|                1|             -0.01|             -0.01|\n",
      "|    max| 2021-01-02 01:12:10|  2021-01-02 01:25:01|                 9|             99.03|                99|               99|             99.99|             99.99|\n",
      "+-------+--------------------+---------------------+------------------+------------------+------------------+-----------------+------------------+------------------+"
     ]
    }
   ],
   "source": [
    "# Keep in mind the datatype when considering these results\n",
    "ps_df_taxi.select(*column_subset).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277dd5bfb85744efa89281cb2188b2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|passenger_count|\n",
      "+---------------+\n",
      "|           null|\n",
      "|              1|\n",
      "|              6|\n",
      "|              3|\n",
      "|              4|\n",
      "|              8|\n",
      "|              5|\n",
      "|              2|\n",
      "|              7|\n",
      "|              0|\n",
      "|              9|\n",
      "+---------------+"
     ]
    }
   ],
   "source": [
    "# Casting columns to a type\n",
    "(ps_df_taxi.select(\"passenger_count\")\n",
    " .withColumn(\"passenger_count\", f.col(\"passenger_count\").cast(t.IntegerType()))\n",
    ").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0189d1e67e045fc8e64d01dd101a5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid literal for int() with base 10: ''\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/pandas/core/generic.py\", line 5531, in astype\n",
      "    col.astype(dtype=dtype[col_name], copy=copy, errors=errors)\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/pandas/core/generic.py\", line 5546, in astype\n",
      "    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors,)\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/pandas/core/internals/managers.py\", line 595, in astype\n",
      "    return self.apply(\"astype\", dtype=dtype, copy=copy, errors=errors)\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/pandas/core/internals/managers.py\", line 406, in apply\n",
      "    applied = getattr(b, f)(**kwargs)\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/pandas/core/internals/blocks.py\", line 595, in astype\n",
      "    values = astype_nansafe(vals1d, dtype, copy=True)\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/pandas/core/dtypes/cast.py\", line 919, in astype_nansafe\n",
      "    return dtype.construct_array_type()._from_sequence(arr, dtype=dtype, copy=copy)\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/pandas/core/arrays/integer.py\", line 378, in _from_sequence\n",
      "    return integer_array(scalars, dtype=dtype, copy=copy)\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/pandas/core/arrays/integer.py\", line 160, in integer_array\n",
      "    values, mask = coerce_to_array(values, dtype=dtype, copy=copy)\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/pandas/core/arrays/integer.py\", line 277, in coerce_to_array\n",
      "    values = safe_cast(values, dtype, copy=False)\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/pandas/core/arrays/integer.py\", line 175, in safe_cast\n",
      "    casted = values.astype(dtype, copy=copy)\n",
      "ValueError: invalid literal for int() with base 10: ''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Casting pandas columns to a type - this will give an error on empty cells\n",
    "(pd_df_taxi[[*column_subset]]\n",
    "         .astype({'passenger_count': 'Int64'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ed290f28994f06b87dae5ed27ca4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IntegerArray>\n",
      "[1, 4, 2, 3, 6, 5, 0, 8, 7, 9, <NA>]\n",
      "Length: 11, dtype: Int64"
     ]
    }
   ],
   "source": [
    "# To convert to Integer using pandas, we have to first deal with the null values\n",
    "# to_numeric with 'coerce' will fill invalid integer values with np.NaN\n",
    "# the Int64 type in later versions of pandas will convert np.NaN to a nullable\n",
    "# integer type: https://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html\n",
    "pd.to_numeric(pd_df_taxi.passenger_count, errors='coerce').astype('Int64').unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.1 - Transform the column_subset of taxi data to data types that accurately represent the data\n",
    "\n",
    "Result: a transformed_taxi dataframe with the column_subset columns cast to an appropriate type\n",
    "\n",
    "Available types are listed in the pyspark.sql.types module https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#module-pyspark.sql.types  \n",
    "\n",
    "This is imported as `t`, so to apply the IntegerType use `t.IntegerType()`\n",
    "\n",
    "For pandas, see the following resources on converting types\n",
    "https://stackoverflow.com/questions/15891038/change-column-type-in-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6f3b99e91e446c9b03e5d500f73fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Talk about transforming\n",
    "# the data into a type schema to surface for analytical operations - i.e. pay the penalty of time\n",
    "# on the ingest to transform strings to timestamps so the analysis side can use datetime methods\n",
    "# without having to remember to cast\n",
    "\n",
    "# Any gotchyas on transforming types we should discuss? Perhaps datetime casting\n",
    "def transform_taxi_ps(ps_df_taxi):\n",
    "    return (ps_df_taxi\n",
    "            .select(*column_subset)\n",
    "            .withColumn(\"tpep_pickup_datetime\", f.col(\"tpep_pickup_datetime\").cast(t.TimestampType()))\n",
    "            .withColumn(\"tpep_dropoff_datetime\", f.col(\"tpep_dropoff_datetime\").cast(t.TimestampType()))\n",
    "            .withColumn(\"passenger_count\", f.col(\"passenger_count\").cast(t.IntegerType()))\n",
    "            .withColumn(\"trip_distance\", f.col(\"trip_distance\").cast(t.FloatType()))\n",
    "            .withColumn(\"PULocationID\", f.col(\"PULocationID\").cast(t.IntegerType()))\n",
    "            .withColumn(\"DOLocationID\", f.col(\"DOLocationID\").cast(t.IntegerType()))\n",
    "            .withColumn(\"fare_amount\", f.col(\"fare_amount\").cast(t.FloatType()))\n",
    "            .withColumn(\"tip_amount\", f.col(\"tip_amount\").cast(t.FloatType()))             \n",
    "           )\n",
    "\n",
    "def transform_taxi_pd(pd_df_taxi):\n",
    "    res = pd_df_taxi[[*column_subset]].copy()\n",
    "    res.tpep_pickup_datetime = pd.to_datetime(res.tpep_pickup_datetime)\n",
    "    res.tpep_dropoff_datetime = pd.to_datetime(res.tpep_dropoff_datetime)\n",
    "    res.passenger_count = pd.to_numeric(res.passenger_count, errors='coerce').astype('Int64')\n",
    "    res.trip_distance = pd.to_numeric(res.trip_distance, errors='coerce')\n",
    "    res.PULocationID = pd.to_numeric(res.PULocationID, errors='coerce').astype('Int64')\n",
    "    res.DOLocationID = pd.to_numeric(res.DOLocationID, errors='coerce').astype('Int64')\n",
    "    res.fare_amount = pd.to_numeric(res.fare_amount, errors='coerce')\n",
    "    res.tip_amount = pd.to_numeric(res.tip_amount, errors='coerce')\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b994153f67b4ea8a38eb92dd7a0aa94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark runtime: 0.12057788799984337 pandas runtime 4.160019910999836"
     ]
    }
   ],
   "source": [
    "ps_run_time, ps_transform_taxi = timer_method(\"transform_taxi_ps(ps_df_taxi)\")\n",
    "pd_run_time, pd_transform_taxi = timer_method(\"transform_taxi_pd(pd_df_taxi)\")\n",
    "print(f\"pyspark runtime: {ps_run_time} pandas runtime {pd_run_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d61e56a62964b8790884d6602ceee04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  \\\n",
      "0  2020-01-01 00:28:15   2020-01-01 00:33:03                1            1.2   \n",
      "1  2020-01-01 00:35:39   2020-01-01 00:43:04                1            1.2   \n",
      "2  2020-01-01 00:47:41   2020-01-01 00:53:52                1            0.6   \n",
      "3  2020-01-01 00:55:23   2020-01-01 01:00:14                1            0.8   \n",
      "4  2020-01-01 00:01:58   2020-01-01 00:04:16                1            0.0   \n",
      "\n",
      "   PULocationID  DOLocationID  fare_amount  tip_amount  \n",
      "0           238           239          6.0        1.47  \n",
      "1           239           238          7.0        1.50  \n",
      "2           238           238          6.0        1.00  \n",
      "3           238           151          5.5        1.36  \n",
      "4           193           193          3.5        0.00"
     ]
    }
   ],
   "source": [
    "pd_transform_taxi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3bd4d9c713c477399b2d11346bae5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|PULocationID|DOLocationID|fare_amount|tip_amount|\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+\n",
      "| 2020-01-01 00:28:15|  2020-01-01 00:33:03|              1|          1.2|         238|         239|        6.0|      1.47|\n",
      "| 2020-01-01 00:35:39|  2020-01-01 00:43:04|              1|          1.2|         239|         238|        7.0|       1.5|\n",
      "| 2020-01-01 00:47:41|  2020-01-01 00:53:52|              1|          0.6|         238|         238|        6.0|       1.0|\n",
      "| 2020-01-01 00:55:23|  2020-01-01 01:00:14|              1|          0.8|         238|         151|        5.5|      1.36|\n",
      "| 2020-01-01 00:01:58|  2020-01-01 00:04:16|              1|          0.0|         193|         193|        3.5|       0.0|\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "ps_transform_taxi.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa16837e097a49e09c27bf1fbc17ef71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "|summary|   passenger_count|     trip_distance|      PULocationID|     DOLocationID|       fare_amount|        tip_amount|\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "|  count|           6339567|           6405008|           6405008|          6405008|           6405008|           6405008|\n",
      "|   mean|1.5153326717739555|2.9296439317743554|164.73225778952968|162.6626908194338|12.694108121822374|2.1893418282101753|\n",
      "| stddev| 1.151594213427813| 83.15910301291039| 65.54373944111757|69.91260629496095|12.127295342892479|2.7600283861848287|\n",
      "|    min|                 0|            -30.62|                 1|                1|           -1238.0|             -91.0|\n",
      "|    max|                 9|         210240.06|               265|              265|            4265.0|            1100.0|\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+------------------+"
     ]
    }
   ],
   "source": [
    "# Note that this only shows results for numeric and string columns - now that the data has been cast to \n",
    "# types, we can explore it a bit more\n",
    "ps_transform_taxi.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6bf59827ba408aa807c9e4dd2e7e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|tpep_pickup_datetime|\n",
      "+--------------------+\n",
      "|2003-01-01 00:07:17 |\n",
      "|2008-12-31 23:02:40 |\n",
      "|2008-12-31 23:02:50 |\n",
      "|2008-12-31 23:03:44 |\n",
      "|2008-12-31 23:03:48 |\n",
      "|2008-12-31 23:06:13 |\n",
      "|2008-12-31 23:17:15 |\n",
      "|2008-12-31 23:24:11 |\n",
      "|2008-12-31 23:34:13 |\n",
      "|2008-12-31 23:35:00 |\n",
      "+--------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "# Take a look at the date ranges...\n",
    "ps_transform_taxi.select(\"tpep_pickup_datetime\").sort(f.asc(\"tpep_pickup_datetime\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47b0160f34f4f809ef506c7d737a809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write out transformed data to EBS\n",
    "ps_transform_taxi.write.mode(\"append\").json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.2 - Write an ingestion function that does the following:\n",
    "Given a file path to a taxi data csv (i.e. s3://nyc-tlc/trip data/green_tripdata_2020-01.csv)\n",
    "1. Read the file into a Spark dataframe\n",
    "2. Transform the column_subset\n",
    "3. Write the data as json to hdfs in append mode to `hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json`\n",
    "\n",
    "Function signature:  \n",
    "`def ingest_taxi_data(file_name)`\n",
    "\n",
    "Inputs can be created from:  \n",
    "`taxi_data_prefix = \"s3://nyc-tlc/trip data/\"\n",
    "taxi_data_files = [\"yellow_tripdata_2019-01.csv\", \"yellow_tripdata_2018-01.csv\", \"yellow_tripdata_2017-01.csv\"]  `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb952e5789f74780bca8d3ad4de10ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ingest_taxi_data(file_name):\n",
    "    (spark\n",
    "         .read\n",
    "         .option('header', True).csv(file_name)\n",
    "         .select(*column_subset)\n",
    "         .withColumn(\"tpep_pickup_datetime\", f.col(\"tpep_pickup_datetime\").cast(t.TimestampType()))\n",
    "         .withColumn(\"tpep_dropoff_datetime\", f.col(\"tpep_dropoff_datetime\").cast(t.TimestampType()))\n",
    "         .withColumn(\"passenger_count\", f.col(\"passenger_count\").cast(t.IntegerType()))\n",
    "         .withColumn(\"trip_distance\", f.col(\"trip_distance\").cast(t.FloatType()))\n",
    "         .withColumn(\"PULocationID\", f.col(\"PULocationID\").cast(t.IntegerType()))\n",
    "         .withColumn(\"DOLocationID\", f.col(\"DOLocationID\").cast(t.IntegerType()))\n",
    "         .withColumn(\"fare_amount\", f.col(\"fare_amount\").cast(t.FloatType()))\n",
    "         .withColumn(\"tip_amount\", f.col(\"tip_amount\").cast(t.FloatType()))             \n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a513ceb7f7b943538f9f9e8410350102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the ingest for several files\n",
    "taxi_data_prefix = \"s3://nyc-tlc/trip data/\"\n",
    "taxi_data_files = [\"yellow_tripdata_2019-01.csv\", \"yellow_tripdata_2018-01.csv\", \"yellow_tripdata_2017-01.csv\"]\n",
    "for file_name in taxi_data_files: \n",
    "    taxi_data_path = f\"{taxi_data_prefix}{file_name}\"\n",
    "    ingest_taxi_data(taxi_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838d4f5c2aa346f1ad329f26a79d600e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- congestion_surcharge: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- lpep_dropoff_datetime: string (nullable = true)\n",
      " |-- lpep_pickup_datetime: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- trip_type: string (nullable = true)\n",
      " |-- year: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "df.printSchema()\n",
    "\n",
    "# Talk through how the read.json interpreted the Integers as Longs, set stage for using schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing ingestion code\n",
    "\n",
    "The `ingest_taxi_data` method is not well structured for testing:\n",
    "* Writes to the file system\n",
    "* Requires an input file to test\n",
    "* What other shortcomings?\n",
    "\n",
    "To make this code more testable, split out the transformation logic so it can be unit tested.  \n",
    "Definining a transformation function that takes a dataframe and returns a dataframe provides a better interface for unit testing, and a more extensible structure in case we need to add more dataframe functions before or after the transformation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f434d6d8e9d54f428337c17416923678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform_taxi_data(df):\n",
    "    return (df.withColumn(\"tpep_pickup_datetime\", f.col(\"tpep_pickup_datetime\").cast(t.TimestampType()))\n",
    "     .withColumn(\"tpep_dropoff_datetime\", f.col(\"tpep_dropoff_datetime\").cast(t.TimestampType()))\n",
    "     .withColumn(\"passenger_count\", f.col(\"passenger_count\").cast(t.IntegerType()))\n",
    "     .withColumn(\"trip_distance\", f.col(\"trip_distance\").cast(t.FloatType()))\n",
    "     .withColumn(\"PULocationID\", f.col(\"PULocationID\").cast(t.IntegerType()))\n",
    "     .withColumn(\"DOLocationID\", f.col(\"DOLocationID\").cast(t.IntegerType()))\n",
    "     .withColumn(\"fare_amount\", f.col(\"fare_amount\").cast(t.FloatType()))\n",
    "     .withColumn(\"tip_amount\", f.col(\"tip_amount\").cast(t.FloatType())))\n",
    "    \n",
    "def ingest_taxi_data_transform(file_name):\n",
    "    # Requires patching of Dataframe.transform method in Spark 2.4, but available natively\n",
    "    # in Spark 3.0 https://mungingdata.com/pyspark/chaining-dataframe-transformations/\n",
    "    df_input = (spark\n",
    "         .read\n",
    "         .option('header', True).csv(taxi_data_path)\n",
    "         .select(*column_subset)\n",
    "         .transform(transform_taxi_data)\n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )\n",
    "    \n",
    "def ingest_taxi_data_method(file_name):\n",
    "    # Equivalent code without using the monkey-patched transform method for DataFrame\n",
    "    df_input = (spark\n",
    "         .read\n",
    "         .option('header', True).csv(taxi_data_path)\n",
    "         .select(*column_subset))\n",
    "    \n",
    "    (transform_taxi_data(df_input)\n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad97c0e2d4714f7d9dc28f65ceb67563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = [\n",
    "    \"{'tpep_pickup_datetime': '2020-05-23 21:05:23', 'tpep_dropoff_datetime': '2020-05-23 08:05:23', 'passenger_count': 0, 'trip_distance': 10.5, 'PULocationID': 1, 'DOLocationID': 254, 'fare_amount': 0.05, 'tip_amount': 1.00}\",\n",
    "    \"{'tpep_pickup_datetime': '2020-10-01 01:05:23', 'tpep_dropoff_datetime': '2020-10-01 02:05:23', 'passenger_count': 1, 'trip_distance': 0.1, 'PULocationID': 45, 'DOLocationID': 3, 'fare_amount': 10.0, 'tip_amount': 5.00}\",\n",
    "    \"{'tpep_pickup_datetime': '2020-02-02 15:22:23', 'tpep_dropoff_datetime': '2020-02-03 15:44:23', 'passenger_count': 3, 'trip_distance': 3.25, 'PULocationID': 10, 'DOLocationID': 24, 'fare_amount': 5.05, 'tip_amount': 1.00}\"\n",
    "]\n",
    "expected_types = {'DOLocationID': 'int', 'PULocationID': 'int', 'fare_amount': 'float', 'passenger_count': 'int', 'tip_amount': 'float', 'tpep_dropoff_datetime': 'timestamp', 'tpep_pickup_datetime': 'timestamp', 'trip_distance': 'float'}\n",
    "\n",
    "test_df = spark.read.json(sc.parallelize(test_data))\n",
    "test = transform_taxi_data(test_df)\n",
    "test_types = {item[0]:item[1] for item in test.dtypes}\n",
    "\n",
    "assert expected_types == test_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff85e65fd54e4c308d1dc857f88e559c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/_SUCCESS'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00000-3cdb1396-1992-4d07-80d3-1531a3742fd6-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00000-4809f966-f1d4-4ee8-b666-ef305d15087c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00000-4c7becf7-98c4-4522-95e9-260e3620bef3-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00000-5562d860-816e-4355-b99f-bd032b323ffc-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00000-761a0aa1-fb3e-4281-b78f-bdb9d3ef48b4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00000-a70ed2c1-6de6-4828-84a4-96fdfd462c5c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00000-b68e4320-1025-44d2-a400-22de6ab6c2de-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00000-f6da1fd9-a705-487c-869c-ace12a6b9ca4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00001-3cdb1396-1992-4d07-80d3-1531a3742fd6-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00001-4809f966-f1d4-4ee8-b666-ef305d15087c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00001-4c7becf7-98c4-4522-95e9-260e3620bef3-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00001-5562d860-816e-4355-b99f-bd032b323ffc-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00001-761a0aa1-fb3e-4281-b78f-bdb9d3ef48b4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00001-a70ed2c1-6de6-4828-84a4-96fdfd462c5c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00001-b68e4320-1025-44d2-a400-22de6ab6c2de-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00001-f6da1fd9-a705-487c-869c-ace12a6b9ca4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00002-3cdb1396-1992-4d07-80d3-1531a3742fd6-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00002-4809f966-f1d4-4ee8-b666-ef305d15087c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00002-4c7becf7-98c4-4522-95e9-260e3620bef3-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00002-5562d860-816e-4355-b99f-bd032b323ffc-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00002-761a0aa1-fb3e-4281-b78f-bdb9d3ef48b4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00002-a70ed2c1-6de6-4828-84a4-96fdfd462c5c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00002-b68e4320-1025-44d2-a400-22de6ab6c2de-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00002-f6da1fd9-a705-487c-869c-ace12a6b9ca4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00003-3cdb1396-1992-4d07-80d3-1531a3742fd6-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00003-4809f966-f1d4-4ee8-b666-ef305d15087c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00003-4c7becf7-98c4-4522-95e9-260e3620bef3-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00003-5562d860-816e-4355-b99f-bd032b323ffc-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00003-761a0aa1-fb3e-4281-b78f-bdb9d3ef48b4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00003-a70ed2c1-6de6-4828-84a4-96fdfd462c5c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00003-b68e4320-1025-44d2-a400-22de6ab6c2de-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00003-f6da1fd9-a705-487c-869c-ace12a6b9ca4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00004-3cdb1396-1992-4d07-80d3-1531a3742fd6-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00004-4809f966-f1d4-4ee8-b666-ef305d15087c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00004-4c7becf7-98c4-4522-95e9-260e3620bef3-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00004-5562d860-816e-4355-b99f-bd032b323ffc-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00004-761a0aa1-fb3e-4281-b78f-bdb9d3ef48b4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00004-a70ed2c1-6de6-4828-84a4-96fdfd462c5c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00004-b68e4320-1025-44d2-a400-22de6ab6c2de-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00004-f6da1fd9-a705-487c-869c-ace12a6b9ca4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00005-3cdb1396-1992-4d07-80d3-1531a3742fd6-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00005-4809f966-f1d4-4ee8-b666-ef305d15087c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00005-4c7becf7-98c4-4522-95e9-260e3620bef3-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00005-5562d860-816e-4355-b99f-bd032b323ffc-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00005-761a0aa1-fb3e-4281-b78f-bdb9d3ef48b4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00005-a70ed2c1-6de6-4828-84a4-96fdfd462c5c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00005-b68e4320-1025-44d2-a400-22de6ab6c2de-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00005-f6da1fd9-a705-487c-869c-ace12a6b9ca4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00006-3cdb1396-1992-4d07-80d3-1531a3742fd6-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00006-4809f966-f1d4-4ee8-b666-ef305d15087c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00006-4c7becf7-98c4-4522-95e9-260e3620bef3-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00006-5562d860-816e-4355-b99f-bd032b323ffc-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00006-761a0aa1-fb3e-4281-b78f-bdb9d3ef48b4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00006-a70ed2c1-6de6-4828-84a4-96fdfd462c5c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00006-b68e4320-1025-44d2-a400-22de6ab6c2de-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00006-f6da1fd9-a705-487c-869c-ace12a6b9ca4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00007-3cdb1396-1992-4d07-80d3-1531a3742fd6-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00007-4809f966-f1d4-4ee8-b666-ef305d15087c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00007-4c7becf7-98c4-4522-95e9-260e3620bef3-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00007-5562d860-816e-4355-b99f-bd032b323ffc-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00007-761a0aa1-fb3e-4281-b78f-bdb9d3ef48b4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00007-a70ed2c1-6de6-4828-84a4-96fdfd462c5c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00007-b68e4320-1025-44d2-a400-22de6ab6c2de-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00007-f6da1fd9-a705-487c-869c-ace12a6b9ca4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00008-4c7becf7-98c4-4522-95e9-260e3620bef3-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00008-761a0aa1-fb3e-4281-b78f-bdb9d3ef48b4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00008-a70ed2c1-6de6-4828-84a4-96fdfd462c5c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00008-f6da1fd9-a705-487c-869c-ace12a6b9ca4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00009-4c7becf7-98c4-4522-95e9-260e3620bef3-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00009-761a0aa1-fb3e-4281-b78f-bdb9d3ef48b4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00009-a70ed2c1-6de6-4828-84a4-96fdfd462c5c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00009-f6da1fd9-a705-487c-869c-ace12a6b9ca4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00010-761a0aa1-fb3e-4281-b78f-bdb9d3ef48b4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00010-a70ed2c1-6de6-4828-84a4-96fdfd462c5c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00010-f6da1fd9-a705-487c-869c-ace12a6b9ca4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00011-761a0aa1-fb3e-4281-b78f-bdb9d3ef48b4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00011-a70ed2c1-6de6-4828-84a4-96fdfd462c5c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00011-f6da1fd9-a705-487c-869c-ace12a6b9ca4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00012-761a0aa1-fb3e-4281-b78f-bdb9d3ef48b4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00012-a70ed2c1-6de6-4828-84a4-96fdfd462c5c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00012-f6da1fd9-a705-487c-869c-ace12a6b9ca4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00013-761a0aa1-fb3e-4281-b78f-bdb9d3ef48b4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00013-a70ed2c1-6de6-4828-84a4-96fdfd462c5c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00013-f6da1fd9-a705-487c-869c-ace12a6b9ca4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00014-761a0aa1-fb3e-4281-b78f-bdb9d3ef48b4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00014-a70ed2c1-6de6-4828-84a4-96fdfd462c5c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00014-f6da1fd9-a705-487c-869c-ace12a6b9ca4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00015-761a0aa1-fb3e-4281-b78f-bdb9d3ef48b4-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00015-a70ed2c1-6de6-4828-84a4-96fdfd462c5c-c000.json'\n",
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/part-00015-f6da1fd9-a705-487c-869c-ace12a6b9ca4-c000.json'\n",
      "b''"
     ]
    }
   ],
   "source": [
    "shell_cmd(\"hdfs dfs -rm hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18273738da540d093ee460fe9da8eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Re run the ingestion using the functions with the transformation broken out\n",
    "taxi_data_prefix = \"s3://nyc-tlc/trip data/\"\n",
    "taxi_data_files = [\"yellow_tripdata_2019-01.csv\", \"yellow_tripdata_2018-01.csv\", \"yellow_tripdata_2017-01.csv\"]\n",
    "for file_name in taxi_data_files: \n",
    "    taxi_data_path = f\"{taxi_data_prefix}{file_name}\"\n",
    "    ingest_taxi_data_method(file_name)\n",
    "    #ingest_taxi_data_transform(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95cd43e32c6245368366514b06842be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26137790"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets try running the ingestion code on the other taxi data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a330a89579442a8a998dc78789a675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"cannot resolve '`tpep_pickup_datetime`' given input columns: [tolls_amount, congestion_surcharge, payment_type, extra, mta_tax, store_and_fwd_flag, lpep_dropoff_datetime, trip_type, PULocationID, improvement_surcharge, fare_amount, total_amount, tip_amount, ehail_fee, DOLocationID, lpep_pickup_datetime, RatecodeID, VendorID, trip_distance, passenger_count];;\\n'Project ['tpep_pickup_datetime, 'tpep_dropoff_datetime, passenger_count#2458, trip_distance#2459, PULocationID#2456, DOLocationID#2457, fare_amount#2460, tip_amount#2463]\\n+- Relation[VendorID#2451,lpep_pickup_datetime#2452,lpep_dropoff_datetime#2453,store_and_fwd_flag#2454,RatecodeID#2455,PULocationID#2456,DOLocationID#2457,passenger_count#2458,trip_distance#2459,fare_amount#2460,extra#2461,mta_tax#2462,tip_amount#2463,tolls_amount#2464,ehail_fee#2465,improvement_surcharge#2466,total_amount#2467,payment_type#2468,trip_type#2469,congestion_surcharge#2470] csv\\n\"\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 17, in ingest_taxi_data_transform\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 1327, in select\n",
      "    jdf = self._jdf.select(self._jcols(*cols))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: \"cannot resolve '`tpep_pickup_datetime`' given input columns: [tolls_amount, congestion_surcharge, payment_type, extra, mta_tax, store_and_fwd_flag, lpep_dropoff_datetime, trip_type, PULocationID, improvement_surcharge, fare_amount, total_amount, tip_amount, ehail_fee, DOLocationID, lpep_pickup_datetime, RatecodeID, VendorID, trip_distance, passenger_count];;\\n'Project ['tpep_pickup_datetime, 'tpep_dropoff_datetime, passenger_count#2458, trip_distance#2459, PULocationID#2456, DOLocationID#2457, fare_amount#2460, tip_amount#2463]\\n+- Relation[VendorID#2451,lpep_pickup_datetime#2452,lpep_dropoff_datetime#2453,store_and_fwd_flag#2454,RatecodeID#2455,PULocationID#2456,DOLocationID#2457,passenger_count#2458,trip_distance#2459,fare_amount#2460,extra#2461,mta_tax#2462,tip_amount#2463,tolls_amount#2464,ehail_fee#2465,improvement_surcharge#2466,total_amount#2467,payment_type#2468,trip_type#2469,congestion_surcharge#2470] csv\\n\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try using the ingest code we created for yellow taxi for all the taxis\n",
    "# This will fail because the datetime fields have different names across different servcies\n",
    "\n",
    "taxi_data_prefix = \"s3://nyc-tlc/trip data/\"\n",
    "taxi_data_files = [\"green_tripdata_2020-01.csv\", \"fhv_tripdata_2020-01.csv\", \"fhvhv_tripdata_2020-01.csv\"]\n",
    "for file_name in taxi_data_files: \n",
    "    taxi_data_path = f\"{taxi_data_prefix}{file_name}\"\n",
    "    ingest_taxi_data_transform(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data modeling\n",
    "Create a model for all taxi data, given that there are differences across the services in the kind of data collected\n",
    "\n",
    "Commmon fields across all services:\n",
    "* PULocationID\n",
    "* DOLocationID\n",
    "\n",
    "Fields we want to normalize across all services - this data is in all services but is named differently\n",
    "* pickup datetime\n",
    "* drop off datetime\n",
    "\n",
    "Service specific fields. These are only in green or yellow data\n",
    "* Passenger_count\n",
    "* Trip_distance\n",
    "* Fare_amount\n",
    "* Tip_amount\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.3 - Write different transformation functions for each taxi service type to match the following signature and schema:\n",
    "Fields / Types:\n",
    "\n",
    "* pickup_datetime Timestamp\n",
    "* dropoff_datetime Timestamp\n",
    "* passenger_count Integer\n",
    "* fare_amount Float\n",
    "* tip_amount Float\n",
    "* PULocationID Integer\n",
    "* DOLocationID Integer\n",
    "\n",
    "`def transform_function(dataframe):  \n",
    "    return transformed_dataframe\n",
    "`\n",
    "\n",
    "Once the transformation functions are done, rewrite `ingest_taxi_data` to use these new functions depending on the file being processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78d7fc7fba043a283a65567b6e0ca80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform_yellow_taxi(df):\n",
    "    return (df.withColumn(\"pickup_datetime\", f.col(\"tpep_pickup_datetime\").cast(t.TimestampType()))\n",
    "        .withColumn(\"dropoff_datetime\", f.col(\"tpep_dropoff_datetime\").cast(t.TimestampType()))\n",
    "        .withColumn(\"passenger_count\", f.col(\"passenger_count\").cast(t.IntegerType()))\n",
    "        .withColumn(\"trip_distance\", f.col(\"trip_distance\").cast(t.FloatType()))\n",
    "        .withColumn(\"fare_amount\", f.col(\"fare_amount\").cast(t.FloatType()))\n",
    "        .withColumn(\"tip_amount\", f.col(\"tip_amount\").cast(t.FloatType()))\n",
    "        .withColumn(\"PULocationID\", f.col(\"PULocationID\").cast(t.IntegerType()))\n",
    "        .withColumn(\"DOLocationID\", f.col(\"DOLocationID\").cast(t.IntegerType())))\n",
    "        \n",
    "        \n",
    "def transform_green_taxi(df):\n",
    "    return (df.withColumn(\"pickup_datetime\", f.col(\"lpep_pickup_datetime\").cast(t.TimestampType()))\n",
    "        .withColumn(\"dropoff_datetime\", f.col(\"lpep_dropoff_datetime\").cast(t.TimestampType()))\n",
    "        .withColumn(\"passenger_count\", f.col(\"passenger_count\").cast(t.IntegerType()))\n",
    "        .withColumn(\"trip_distance\", f.col(\"trip_distance\").cast(t.FloatType()))\n",
    "        .withColumn(\"fare_amount\", f.col(\"fare_amount\").cast(t.FloatType()))\n",
    "        .withColumn(\"tip_amount\", f.col(\"tip_amount\").cast(t.FloatType()))\n",
    "        .withColumn(\"PULocationID\", f.col(\"PULocationID\").cast(t.IntegerType()))\n",
    "        .withColumn(\"DOLocationID\", f.col(\"DOLocationID\").cast(t.IntegerType())))\n",
    " \n",
    "\n",
    "def transform_fhv(df):\n",
    "    return (df.withColumn(\"pickup_datetime\", f.col(\"pickup_datetime\").cast(t.TimestampType()))\n",
    "        .withColumn(\"dropoff_datetime\", f.col(\"dropoff_datetime\").cast(t.TimestampType()))\n",
    "        .withColumn(\"PULocationID\", f.col(\"PULocationID\").cast(t.IntegerType()))\n",
    "        .withColumn(\"DOLocationID\", f.col(\"DOLocationID\").cast(t.IntegerType())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the ability to read all the taxi data into the same dataset, how will we be able to tell where the original data came from? The file name provides information including:\n",
    "* Service type (yellow, green, etc)\n",
    "* File date\n",
    "\n",
    "We want to augment the taxi data with this information so we can refer back to it in analysis.\n",
    "\n",
    "Is there other data we might want to augment the raw data with? Some things to consider:\n",
    "* Additional fields that could help with analysis\n",
    "* Metadata, like when the record was last updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03a8fd84ea7499e9324d24ee399d7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using matched groups, we can extract information from the taxi file names\n",
    "TAXI_DATA_PATTERN = \"(?P<service>[a-zA-Z0-9]+)_tripdata_(?P<year>[0-9]{4})-(?P<month>[0-9]{2}).csv\"\n",
    "\n",
    "def extract_file_info(file_name):\n",
    "    m = re.match(TAXI_DATA_PATTERN, file_name)\n",
    "    if m is not None:\n",
    "        return (m.group(1), m.group(2), m.group(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade1b6701e434767bb111816bb514b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ingest_taxi_data_multi_service(file_name):\n",
    "    print(f\"Processing {file_name}\")\n",
    "    (service, year, month) = extract_file_info(Path(file_name).name)\n",
    "    input_df = spark.read.option('header', True).csv(file_name)\n",
    "    \n",
    "    if service == 'yellow':\n",
    "        df_transform = transform_yellow_taxi(input_df)\n",
    "    elif service == 'green':\n",
    "        df_transform = transform_green_taxi(input_df)\n",
    "    else:\n",
    "        # FHV. What happens if there are more taxi services added?\n",
    "        df_transform = transform_fhv(input_df)\n",
    "\n",
    "    (df_transform\n",
    "         .withColumn(\"service\", f.lit(service))\n",
    "         .withColumn(\"year\", f.lit(year))\n",
    "         .withColumn(\"month\", f.lit(month))\n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a7b1b03b7c44ebcbabb3e21d17e8dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Deleted hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json'\n",
      "b''"
     ]
    }
   ],
   "source": [
    "shell_cmd(\"hdfs dfs -rm -r hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ff21752c164da9956a12cfee4bf9a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing s3://nyc-tlc/trip data/yellow_tripdata_2020-01.csv\n",
      "Processing s3://nyc-tlc/trip data/green_tripdata_2020-01.csv\n",
      "Processing s3://nyc-tlc/trip data/fhv_tripdata_2020-01.csv\n",
      "Processing s3://nyc-tlc/trip data/fhvhv_tripdata_2020-01.csv"
     ]
    }
   ],
   "source": [
    "taxi_data_prefix = \"s3://nyc-tlc/trip data/\"\n",
    "taxi_data_files = [\"yellow_tripdata_2020-01.csv\", \"green_tripdata_2020-01.csv\", \"fhv_tripdata_2020-01.csv\", \"fhvhv_tripdata_2020-01.csv\"]\n",
    "for file_name in taxi_data_files: \n",
    "    taxi_data_path = f\"{taxi_data_prefix}{file_name}\"\n",
    "    ingest_taxi_data_multi_service(taxi_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a654c97b99934a11abcd21b5e68a07dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_taxi_output = spark.read.json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ed34adcaf74c878b3539c2a98a97c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|service|   count|\n",
      "+-------+--------+\n",
      "| yellow| 6405008|\n",
      "|    fhv| 1958936|\n",
      "|  green|  447770|\n",
      "|  fhvhv|20569325|\n",
      "+-------+--------+"
     ]
    }
   ],
   "source": [
    "df_taxi_output.groupby(\"service\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling bad data\n",
    "How to design for the inevitability of bad data  \n",
    "Reference: https://blog.knoldus.com/apache-spark-handle-corrupt-bad-records/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbaec4e88b804afc8cfad6bc63e99084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bad_data = [\n",
    "    \"{'pickup_datetime': '2020-05-23 21:05:23', 'fare_amount': '0.05'}\",\n",
    "    \"{'pickup_datetime': '2020-05-23 08:05:23', 'fare_amount': '10.05'}\",\n",
    "    \"{'pickup_datetime': '2020-05-23 21:05:23', 'fare_amount}\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa6f3d16dfe484eb06c2b9e5d08b27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-------------------+\n",
      "|     _corrupt_record|fare_amount|    pickup_datetime|\n",
      "+--------------------+-----------+-------------------+\n",
      "|                null|       0.05|2020-05-23 21:05:23|\n",
      "|                null|      10.05|2020-05-23 08:05:23|\n",
      "|{'pickup_datetime...|       null|               null|\n",
      "+--------------------+-----------+-------------------+"
     ]
    }
   ],
   "source": [
    "corrupt_df = spark.read.json(sc.parallelize(bad_data), mode=\"PERMISSIVE\", columnNameOfCorruptRecord=\"_corrupt_record\")\n",
    "corrupt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ea22048a2d4cf7a09b4bac20911d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|fare_amount|    pickup_datetime|\n",
      "+-----------+-------------------+\n",
      "|       0.05|2020-05-23 21:05:23|\n",
      "|      10.05|2020-05-23 08:05:23|\n",
      "+-----------+-------------------+"
     ]
    }
   ],
   "source": [
    "corrupt_df = spark.read.json(sc.parallelize(bad_data), mode=\"DROPMALFORMED\", columnNameOfCorruptRecord=\"_corrupt_record\")\n",
    "corrupt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ba8eb63d27440aa85c68d9d0c2b868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o900.json.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 69.0 failed 4 times, most recent failure: Lost task 15.3 in stage 69.0 (TID 903, ip-172-31-8-43.us-east-2.compute.internal, executor 6): org.apache.spark.SparkException: Malformed records are detected in schema inference. Parse Mode: FAILFAST.\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:66)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:53)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n",
      "\tat scala.collection.Iterator$class.isEmpty(Iterator.scala:331)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceLeftOption(TraversableOnce.scala:203)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceOption(TraversableOnce.scala:210)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1334)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:70)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:50)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: com.fasterxml.jackson.core.JsonParseException: Unexpected end-of-input in field name\n",
      " at [Source: [B@50fb5101; line: 1, column: 113]\n",
      "\tat com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1581)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:533)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:470)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseAposName(UTF8StreamJsonParser.java:2153)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleOddName(UTF8StreamJsonParser.java:1998)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseName(UTF8StreamJsonParser.java:1650)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:740)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonUtils$.nextUntil(JacksonUtils.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.inferField(JsonInferSchema.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:57)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:55)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2619)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:55)\n",
      "\t... 24 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.infer(JsonInferSchema.scala:83)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$inferFromDataset$1.apply(JsonDataSource.scala:109)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$inferFromDataset$1.apply(JsonDataSource.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:108)\n",
      "\tat org.apache.spark.sql.DataFrameReader$$anonfun$2.apply(DataFrameReader.scala:440)\n",
      "\tat org.apache.spark.sql.DataFrameReader$$anonfun$2.apply(DataFrameReader.scala:440)\n",
      "\tat scala.Option.getOrElse(Option.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:439)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:420)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:406)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Malformed records are detected in schema inference. Parse Mode: FAILFAST.\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:66)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:53)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n",
      "\tat scala.collection.Iterator$class.isEmpty(Iterator.scala:331)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceLeftOption(TraversableOnce.scala:203)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceOption(TraversableOnce.scala:210)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1334)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:70)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:50)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: com.fasterxml.jackson.core.JsonParseException: Unexpected end-of-input in field name\n",
      " at [Source: UNKNOWN; line: 1, column: 113]\n",
      "\tat com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1581)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:533)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:470)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseAposName(UTF8StreamJsonParser.java:2153)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleOddName(UTF8StreamJsonParser.java:1998)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseName(UTF8StreamJsonParser.java:1650)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:740)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonUtils$.nextUntil(JacksonUtils.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.inferField(JsonInferSchema.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:57)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:55)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2619)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:55)\n",
      "\t... 24 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 286, in json\n",
      "    return self._df(self._jreader.json(jrdd))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o900.json.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 69.0 failed 4 times, most recent failure: Lost task 15.3 in stage 69.0 (TID 903, ip-172-31-8-43.us-east-2.compute.internal, executor 6): org.apache.spark.SparkException: Malformed records are detected in schema inference. Parse Mode: FAILFAST.\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:66)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:53)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n",
      "\tat scala.collection.Iterator$class.isEmpty(Iterator.scala:331)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceLeftOption(TraversableOnce.scala:203)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceOption(TraversableOnce.scala:210)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1334)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:70)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:50)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: com.fasterxml.jackson.core.JsonParseException: Unexpected end-of-input in field name\n",
      " at [Source: [B@50fb5101; line: 1, column: 113]\n",
      "\tat com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1581)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:533)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:470)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseAposName(UTF8StreamJsonParser.java:2153)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleOddName(UTF8StreamJsonParser.java:1998)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseName(UTF8StreamJsonParser.java:1650)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:740)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonUtils$.nextUntil(JacksonUtils.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.inferField(JsonInferSchema.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:57)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:55)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2619)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:55)\n",
      "\t... 24 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.infer(JsonInferSchema.scala:83)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$inferFromDataset$1.apply(JsonDataSource.scala:109)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$inferFromDataset$1.apply(JsonDataSource.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:108)\n",
      "\tat org.apache.spark.sql.DataFrameReader$$anonfun$2.apply(DataFrameReader.scala:440)\n",
      "\tat org.apache.spark.sql.DataFrameReader$$anonfun$2.apply(DataFrameReader.scala:440)\n",
      "\tat scala.Option.getOrElse(Option.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:439)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:420)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:406)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Malformed records are detected in schema inference. Parse Mode: FAILFAST.\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:66)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:53)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n",
      "\tat scala.collection.Iterator$class.isEmpty(Iterator.scala:331)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceLeftOption(TraversableOnce.scala:203)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.reduceOption(TraversableOnce.scala:210)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1334)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:70)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1.apply(JsonInferSchema.scala:50)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: com.fasterxml.jackson.core.JsonParseException: Unexpected end-of-input in field name\n",
      " at [Source: UNKNOWN; line: 1, column: 113]\n",
      "\tat com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1581)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:533)\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:470)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseAposName(UTF8StreamJsonParser.java:2153)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleOddName(UTF8StreamJsonParser.java:1998)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseName(UTF8StreamJsonParser.java:1650)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:740)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonUtils$.nextUntil(JacksonUtils.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.inferField(JsonInferSchema.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:57)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3.apply(JsonInferSchema.scala:55)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2619)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1.apply(JsonInferSchema.scala:55)\n",
      "\t... 24 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corrupt_df = spark.read.json(sc.parallelize(bad_data), mode=\"FAILFAST\", columnNameOfCorruptRecord=\"_corrupt_record\")\n",
    "corrupt_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.4 - Write an ingestion for the taxi zone lookup\n",
    "File location - Yes, there is a space between taxi and the '_'  \n",
    "\n",
    "s3://nyc-tlc/misc/taxi _zone_lookup.csv\n",
    "\n",
    "`def ingest_taxi_lookup():`\n",
    "1. Read taxi data\n",
    "1. Cast to correct data types\n",
    "1. Save to hdfs:///tmp/data/nyc-taxi/zone-lookup/output/section2/json\n",
    "\n",
    "Refer back to Taxi Data page for more info: https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a1e90b29ef4b5981f8c5050a0104a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def taxi_zone_transform(df):\n",
    "    return (df.withColumn(\"LocationID\", f.col(\"LocationID\").cast(t.IntegerType()))\n",
    "            .withColumn(\"Borough\", f.col(\"Borough\").cast(t.StringType()))\n",
    "            .withColumn(\"Zone\", f.col(\"Zone\").cast(t.StringType()))\n",
    "            .withColumn(\"service_zone\", f.col(\"service_zone\").cast(t.StringType())))\n",
    "\n",
    "def ingest_taxi_lookup():\n",
    "    (spark.read\n",
    "    .option(\"header\", True)\n",
    "    .csv(\"s3://nyc-tlc/misc/taxi _zone_lookup.csv\")\n",
    "    .transform(taxi_zone_transform)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .json(\"hdfs:///tmp/data/nyc-taxi/zone-lookup/output/section2/json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a204e313911b42c298a20ed71360ef71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.2848559850001493, None)"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"ingest_taxi_lookup()\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather data ingestion\n",
    "\n",
    "NOAA GHCND dataset  \n",
    "https://docs.opendata.aws/noaa-ghcn-pds/readme.html  \n",
    "\n",
    "Scroll down to 'FORMAT OF “ghcnd-stations.txt” file' for the schema of the fixed-width stations data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a889ac52e224b389d90211cad60d0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ghcnd_stations_path = \"https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt\"\n",
    "stations_s3 = f\"s3://{MY_BUCKET_NAME}/data/ghcnd/stations/input/ghcnd_stations.txt\"\n",
    "stations_local = \"hdfs:///tmp/data/ghcnd/stations/input/ghcnd-stations.txt\"\n",
    "stations_output = \"hdfs:///tmp/data/ghcnd/stations/output/ghcnd-stations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3496d63c0a4fbbab139a6ea5275775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Workaround reading HTTPS -> HDFS, HTTPS -> S3 -> HDFS\n",
    "# Spark cant read data directly from HTTP, so copy the file to S3 and read into a dataframe from there\n",
    "# Then save the file to HDFS for further processing\n",
    "ingest_timestamp = datetime.strftime(datetime.now(), \"%Y-%m-%d %H:%M:%S%z\")\n",
    "resp = requests.get(ghcnd_stations_path)\n",
    "if resp.status_code != 200:\n",
    "    print(\"Couldn't get station data\")\n",
    "else:\n",
    "    s3 = boto3.client('s3')\n",
    "    res = s3.put_object(Body=resp.content, Bucket=MY_BUCKET_NAME, Key=f\"data/ghcnd/stations/input/ghcnd_stations.txt\")\n",
    "    if res['ResponseMetadata']['HTTPStatusCode'] != 200:\n",
    "        print(f\"Unable to create ghcnd_stations.txt in s3, response {res['ResponseMetadata']['HTTPStatusCode']}\")\n",
    "    else:\n",
    "        (spark\n",
    "         .read\n",
    "         .text(stations_s3)\n",
    "         .write\n",
    "         .format(\"text\")\n",
    "         .mode(\"overwrite\")\n",
    "         .save(stations_local))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33aa825201d481aa793e9ed65536878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------+\n",
      "|value                                                                                |\n",
      "+-------------------------------------------------------------------------------------+\n",
      "|ACW00011604  17.1167  -61.7833   10.1    ST JOHNS COOLIDGE FLD                       |\n",
      "|ACW00011647  17.1333  -61.7833   19.2    ST JOHNS                                    |\n",
      "|AE000041196  25.3330   55.5170   34.0    SHARJAH INTER. AIRP            GSN     41196|\n",
      "|AEM00041194  25.2550   55.3640   10.4    DUBAI INTL                             41194|\n",
      "|AEM00041217  24.4330   54.6510   26.8    ABU DHABI INTL                         41217|\n",
      "|AEM00041218  24.2620   55.6090  264.9    AL AIN INTL                            41218|\n",
      "|AF000040930  35.3170   69.0170 3366.0    NORTH-SALANG                   GSN     40930|\n",
      "|AFM00040938  34.2100   62.2280  977.2    HERAT                                  40938|\n",
      "|AFM00040948  34.5660   69.2120 1791.3    KABUL INTL                             40948|\n",
      "|AFM00040990  31.5000   65.8500 1010.0    KANDAHAR AIRPORT                       40990|\n",
      "+-------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "# Take a look at the stations file we just saved to HDFS\n",
    "stations = spark.read.text(stations_local)\n",
    "stations.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20caaa972c41478786793ce2b2f03edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|id         |\n",
      "+-----------+\n",
      "|ACW00011604|\n",
      "|ACW00011647|\n",
      "|AE000041196|\n",
      "|AEM00041194|\n",
      "|AEM00041217|\n",
      "|AEM00041218|\n",
      "|AF000040930|\n",
      "|AFM00040938|\n",
      "|AFM00040948|\n",
      "|AFM00040990|\n",
      "+-----------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "# Example of doing a substring transformation\n",
    "(stations\n",
    "    .withColumn(\"id\", f.col(\"value\").substr(0, 11))\n",
    "    .drop(\"value\")\n",
    ").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.5 - Build the ingestion for the weather stations data\n",
    "\n",
    "Reference the fixed width schema provided under **FORMAT OF “ghcnd-stations.txt” file**   \n",
    "https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt  \n",
    "#### Important note - FORMAT OF is incorrect for: elevation - should be 7 char not 6\n",
    "\n",
    "Create transformation code to convert the value column into the following schema:\n",
    "* id String\n",
    "* lat Float\n",
    "* long Float\n",
    "* elevation Float\n",
    "* state String\n",
    "* name String\n",
    "\n",
    "Drop the value column, save the data in JSON format to s3://data-scale-oreilly/data/ghcnd/stations/output/section2_json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb99673427c4995b0456b7af9211405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform_stations_ps(stations_local):\n",
    "    return (spark.read.text(stations_local)\n",
    "     .withColumn(\"id\", f.col(\"value\").substr(0, 11))\n",
    "     .withColumn(\"lat\", f.col(\"value\").substr(13, 8))\n",
    "     .withColumn(\"long\", f.col(\"value\").substr(22, 9))\n",
    "     .withColumn(\"elevation\", f.col(\"value\").substr(32, 7))\n",
    "     .withColumn(\"state\", f.col(\"value\").substr(39, 2))\n",
    "     .withColumn(\"name\", f.col(\"value\").substr(42, 30))\n",
    "     .drop(\"value\")\n",
    "    )\n",
    "\n",
    "def transform_stations_pd(stations_s3):\n",
    "    return pd.read_fwf(stations_s3, \n",
    "                     [(0,10), (13, 20), (22, 30), (32, 38), (39, 40), (41, 71)],\n",
    "                    names=[\"id\", \"lat\", \"long\", \"elevation\", \"state\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9f060f53f14a059fc2efb877914877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark runtime: 0.06883469100012007 pandas runtime 0.9708598619999975"
     ]
    }
   ],
   "source": [
    "ps_run_time, stations_df = timer_method(\"transform_stations_ps(stations_local)\")\n",
    "pd_run_time, pd_stations_df = timer_method(\"transform_stations_pd(stations_s3)\")\n",
    "print(f\"pyspark runtime: {ps_run_time} pandas runtime {pd_run_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A key aspect of designing scalable systems is to be judicious about the data being stored and processed. \n",
    "#### The GHCND stations file contains data on stations across the US, but we are only interested in data near NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178ed09e33a642b790e043eb78e812ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115084"
     ]
    }
   ],
   "source": [
    "stations_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a46989b5043415397f42248377cad31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets look at some performance tradeoffs between pyspark and pandas\n",
    "# The .toPandas() function in pyspark can be convenient if you are familiar with pandas manipulations\n",
    "# but this can quickly become very expensive as it collects all data on the driver to do the conversion.\n",
    "\n",
    "def filter_ny_stations_pandas(stations_df):\n",
    "    ny_stations = stations_df.filter(\"state == 'NY'\")\n",
    "\n",
    "    # filter down to just stations in NY in NYC. Lat of south Yonkers ~40.9124\n",
    "    ny_pandas = ny_stations.toPandas()\n",
    "    ny_pandas[ny_pandas.columns] = ny_pandas.apply(lambda x: x.str.strip())\n",
    "    nyc_stations = ny_pandas[ny_pandas['lat'].apply(lambda x: float(x)) < 40.9124]\n",
    "    return spark.createDataFrame(nyc_stations)\n",
    "\n",
    "def filter_ny_stations_pyspark(stations_df):\n",
    "    print(\"Filtering stations to NY only\")\n",
    "    ny_stations = stations_df.filter(\"state == 'NY'\")\n",
    "    return (ny_stations\n",
    "            .withColumn(\"lat\", f.col(\"lat\").cast(t.FloatType()))\n",
    "            .filter(\"lat < 40.9124\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ea261dc2d049e5a01acee26b1d0000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime 0.5828636930000357"
     ]
    }
   ],
   "source": [
    "pd_run_time, pd_ny_stations = timer_method(\"filter_ny_stations_pandas(stations_df)\")\n",
    "print(f\"runtime {pd_run_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a74644dfec040c79b34c7c9e025f4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering stations to NY only\n",
      "runtime 0.026030307999917568"
     ]
    }
   ],
   "source": [
    "# Are these runtimes really comprable with lazy eval? I think we need to do some combined actions, \n",
    "# like filter and aggregate, to look at performance tradeoffs\n",
    "ps_run_time, ps_ny_stations = timer_method(\"filter_ny_stations_pyspark(stations_df)\")\n",
    "print(f\"runtime {ps_run_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Lab 2.6 - Write two ingest functions for the stations data, one using filter_ny_stations_pandas and the other using filter_ny_stations_pyspark. What do you notice about the differences?\n",
    "\n",
    "The functions should:\n",
    "* Read the station data from local or s3\n",
    "* Transform the station data into columns from the fixed width format\n",
    "* Use the above filter functions\n",
    "* Write the output to stations_output as json in overwrite mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea51fbc8bf746a8b229bb3a9f5dd3d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform_station(stations):\n",
    "    return (stations\n",
    "     .withColumn(\"id\", f.col(\"value\").substr(0, 11))\n",
    "     .withColumn(\"lat\", f.col(\"value\").substr(13, 8))\n",
    "     .withColumn(\"long\", f.col(\"value\").substr(22, 9))\n",
    "     .withColumn(\"elevation\", f.col(\"value\").substr(32, 7))\n",
    "     .withColumn(\"state\", f.col(\"value\").substr(39, 2))\n",
    "     .withColumn(\"name\", f.col(\"value\").substr(42, 30))\n",
    "     .drop(\"value\")\n",
    "    )\n",
    "\n",
    "def ingest_station_pandas():\n",
    "    stations = spark.read.text(stations_local)\n",
    "    (stations.transform(transform_station)\n",
    "     .transform(filter_ny_stations_pandas)\n",
    "     .coalesce(1)\n",
    "     .write\n",
    "     .mode(\"overwrite\")\n",
    "     .json(stations_output)\n",
    "    )\n",
    "    \n",
    "def ingest_station_pyspark():\n",
    "    stations = spark.read.text(stations_s3)\n",
    "    (stations.transform(transform_station)\n",
    "     .transform(filter_ny_stations_pyspark)\n",
    "     .coalesce(1)\n",
    "     .write\n",
    "     .mode(\"overwrite\")\n",
    "     .json(stations_output)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0b208734124957a78d64c6948b35cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#shell_cmd(\"hdfs dfs -rm -r hdfs:///tmp/data/test_pyspark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238a8d2ca7cb4459990a9b3ff2de2d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering stations to NY only\n",
      "pandas: (13.97231917199997, None) pyspark: (0.804971101999854, None)"
     ]
    }
   ],
   "source": [
    "ps_result = timer_method(\"ingest_station_pyspark()\")\n",
    "pd_result = timer_method(\"ingest_station_pandas()\")\n",
    "print(f\"pandas: {pd_result} pyspark: {ps_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.7 - Case Study 1: Month over month, get the total count of of pickups per borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88fe3732e944a56a36b537a0ca4588a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "taxiPath = \"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/\"\n",
    "taxiLookupPath = \"hdfs:///tmp/data/nyc-taxi/zone-lookup/output/section2/json/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8e78cbc56a4df4aa9a4ebd6664e7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'Detected implicit cartesian product for INNER join between logical plans\\nJoin Inner, ((LocationID#3838L = PULocationID#3774L) && (LocationID#3838L = DOLocationID#3773L))\\n:- Filter ((isnotnull(pickup_datetime#3791) && isnotnull(dropoff_datetime#3780)) && (((DOLocationID#3773L = PULocationID#3774L) && isnotnull(PULocationID#3774L)) && isnotnull(DOLocationID#3773L)))\\n:  +- Relation[DOLocationID#3773L,PULocationID#3774L,RatecodeID#3775,SR_Flag#3776,VendorID#3777,congestion_surcharge#3778,dispatching_base_num#3779,dropoff_datetime#3780,extra#3781,fare_amount#3782,hvfhs_license_num#3783,improvement_surcharge#3784,lpep_dropoff_datetime#3785,lpep_pickup_datetime#3786,month#3787,mta_tax#3788,passenger_count#3789L,payment_type#3790,pickup_datetime#3791,service#3792,store_and_fwd_flag#3793,tip_amount#3794,tolls_amount#3795,total_amount#3796,... 5 more fields] json\\n+- Project [LocationID#3838L, Borough#3837 AS PUBorough#3847]\\n   +- Filter isnotnull(LocationID#3838L)\\n      +- Relation[Borough#3837,LocationID#3838L,Zone#3839,service_zone#3840] json\\nand\\nProject [LocationID#3949L, Borough#3948 AS DOBorough#3945]\\n+- Relation[Borough#3948,LocationID#3949L,Zone#3950,service_zone#3951] json\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 382, in show\n",
      "    print(self._jdf.showString(n, 20, vertical))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: 'Detected implicit cartesian product for INNER join between logical plans\\nJoin Inner, ((LocationID#3838L = PULocationID#3774L) && (LocationID#3838L = DOLocationID#3773L))\\n:- Filter ((isnotnull(pickup_datetime#3791) && isnotnull(dropoff_datetime#3780)) && (((DOLocationID#3773L = PULocationID#3774L) && isnotnull(PULocationID#3774L)) && isnotnull(DOLocationID#3773L)))\\n:  +- Relation[DOLocationID#3773L,PULocationID#3774L,RatecodeID#3775,SR_Flag#3776,VendorID#3777,congestion_surcharge#3778,dispatching_base_num#3779,dropoff_datetime#3780,extra#3781,fare_amount#3782,hvfhs_license_num#3783,improvement_surcharge#3784,lpep_dropoff_datetime#3785,lpep_pickup_datetime#3786,month#3787,mta_tax#3788,passenger_count#3789L,payment_type#3790,pickup_datetime#3791,service#3792,store_and_fwd_flag#3793,tip_amount#3794,tolls_amount#3795,total_amount#3796,... 5 more fields] json\\n+- Project [LocationID#3838L, Borough#3837 AS PUBorough#3847]\\n   +- Filter isnotnull(LocationID#3838L)\\n      +- Relation[Borough#3837,LocationID#3838L,Zone#3839,service_zone#3840] json\\nand\\nProject [LocationID#3949L, Borough#3948 AS DOBorough#3945]\\n+- Relation[Borough#3948,LocationID#3949L,Zone#3950,service_zone#3951] json\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join boroughs\n",
    "# Expected error cartesian join. most likely a carryover bug from 2.0\n",
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", \"false\") #<-- default\n",
    "df_taxi = spark.read.json(taxiPath)\n",
    "df_taxi_lookup = spark.read.json(taxiLookupPath)\n",
    "\n",
    "taxi_filtered = (df_taxi\n",
    " .filter(df_taxi.pickup_datetime.isNotNull())\n",
    " .filter(df_taxi.dropoff_datetime.isNotNull()))\n",
    "taxi_pu = (taxi_filtered\n",
    ".join(df_taxi_lookup\n",
    "       .select(\"LocationID\", \"Borough\")\n",
    "       .withColumnRenamed(\"Borough\", \"PUBorough\"), \n",
    "       df_taxi_lookup.LocationID == df_taxi.PULocationID))\n",
    "taxi = (taxi_pu.join(df_taxi_lookup\n",
    "       .select(\"LocationID\", \"Borough\")\n",
    "       .withColumnRenamed(\"Borough\", \"DOBorough\"), \n",
    "       df_taxi_lookup.LocationID == taxi_pu.DOLocationID))\n",
    "taxi_pu.show()\n",
    "taxi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8797b0d2e46f4d05920ac7c0abe7072f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan(isFinalPlan=false)\n",
      "+- BroadcastHashJoin [PULocationID#4282L], [LocationID#4346L], Inner, BuildRight\n",
      "   :- Project [DOLocationID#4281L, PULocationID#4282L, RatecodeID#4283, SR_Flag#4284, VendorID#4285, congestion_surcharge#4286, dispatching_base_num#4287, dropoff_datetime#4288, extra#4289, fare_amount#4290, hvfhs_license_num#4291, improvement_surcharge#4292, lpep_dropoff_datetime#4293, lpep_pickup_datetime#4294, month#4295, mta_tax#4296, passenger_count#4297L, payment_type#4298, pickup_datetime#4299, service#4300, store_and_fwd_flag#4301, tip_amount#4302, tolls_amount#4303, total_amount#4304, ... 5 more fields]\n",
      "   :  +- Filter ((isnotnull(pickup_datetime#4299) && isnotnull(dropoff_datetime#4288)) && isnotnull(PULocationID#4282L))\n",
      "   :     +- FileScan json [DOLocationID#4281L,PULocationID#4282L,RatecodeID#4283,SR_Flag#4284,VendorID#4285,congestion_surcharge#4286,dispatching_base_num#4287,dropoff_datetime#4288,extra#4289,fare_amount#4290,hvfhs_license_num#4291,improvement_surcharge#4292,lpep_dropoff_datetime#4293,lpep_pickup_datetime#4294,month#4295,mta_tax#4296,passenger_count#4297L,payment_type#4298,pickup_datetime#4299,service#4300,store_and_fwd_flag#4301,tip_amount#4302,tolls_amount#4303,total_amount#4304,... 5 more fields] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://ip-172-31-6-49.us-east-2.compute.internal:8020/tmp/data/nyc-taxi/taxi-da..., PartitionFilters: [], PushedFilters: [IsNotNull(pickup_datetime), IsNotNull(dropoff_datetime), IsNotNull(PULocationID)], ReadSchema: struct<DOLocationID:bigint,PULocationID:bigint,RatecodeID:string,SR_Flag:string,VendorID:string,c...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]))\n",
      "      +- Project [LocationID#4346L, Borough#4345 AS PUBorough#4355]\n",
      "         +- Filter isnotnull(LocationID#4346L)\n",
      "            +- FileScan json [Borough#4345,LocationID#4346L] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://ip-172-31-6-49.us-east-2.compute.internal:8020/tmp/data/nyc-taxi/zone-lo..., PartitionFilters: [], PushedFilters: [IsNotNull(LocationID)], ReadSchema: struct<Borough:string,LocationID:bigint>\n",
      "+------------+------------+----------+-------+--------+--------------------+--------------------+--------------------+-----+-----------+-----------------+---------------------+---------------------+--------------------+-----+-------+---------------+------------+--------------------+-------+------------------+----------+------------+------------+---------------------+--------------------+-------------+---------+----+----------+---------+\n",
      "|DOLocationID|PULocationID|RatecodeID|SR_Flag|VendorID|congestion_surcharge|dispatching_base_num|    dropoff_datetime|extra|fare_amount|hvfhs_license_num|improvement_surcharge|lpep_dropoff_datetime|lpep_pickup_datetime|month|mta_tax|passenger_count|payment_type|     pickup_datetime|service|store_and_fwd_flag|tip_amount|tolls_amount|total_amount|tpep_dropoff_datetime|tpep_pickup_datetime|trip_distance|trip_type|year|LocationID|PUBorough|\n",
      "+------------+------------+----------+-------+--------+--------------------+--------------------+--------------------+-----+-----------+-----------------+---------------------+---------------------+--------------------+-----+-------+---------------+------------+--------------------+-------+------------------+----------+------------+------------+---------------------+--------------------+-------------+---------+----+----------+---------+\n",
      "|          90|         148|      null|   null|    null|                null|              B02864|2020-01-01T01:02:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:45:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|       148|Manhattan|\n",
      "|          79|         114|      null|   null|    null|                null|              B02682|2020-01-01T00:53:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:47:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|       114|Manhattan|\n",
      "|         125|           4|      null|   null|    null|                null|              B02764|2020-01-01T00:21:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:04:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|         4|Manhattan|\n",
      "|         113|         231|      null|   null|    null|                null|              B02764|2020-01-01T00:33:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:26:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|       231|Manhattan|\n",
      "|         144|         114|      null|   null|    null|                null|              B02764|2020-01-01T00:46:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:37:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|       114|Manhattan|\n",
      "|         137|         144|      null|   null|    null|                null|              B02764|2020-01-01T01:07:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:49:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|       144|Manhattan|\n",
      "|         148|         249|      null|   null|    null|                null|              B02870|2020-01-01T00:36:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:21:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|       249|Manhattan|\n",
      "|           4|         148|      null|   null|    null|                null|              B02870|2020-01-01T00:42:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:38:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|       148|Manhattan|\n",
      "|           7|          79|      null|   null|    null|                null|              B02870|2020-01-01T01:09:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:46:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|        79|Manhattan|\n",
      "|         236|         140|      null|   null|    null|                null|              B02836|2020-01-01T00:23:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:15:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|       140|Manhattan|\n",
      "|         149|          75|      null|   null|    null|                null|              B02836|2020-01-01T01:13:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:28:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|        75|Manhattan|\n",
      "|         229|          33|      null|   null|    null|                null|              B02875|2020-01-01T00:40:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:14:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|        33| Brooklyn|\n",
      "|          74|         229|      null|   null|    null|                null|              B02875|2020-01-01T00:59:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:45:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|       229|Manhattan|\n",
      "|         238|         114|      null|   null|    null|                null|              B02764|2020-01-01T00:42:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:16:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|       114|Manhattan|\n",
      "|          89|          85|      null|   null|    null|                null|              B02617|2020-01-01T00:45:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:40:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|        85| Brooklyn|\n",
      "|         202|          61|      null|   null|    null|                null|              B02617|2020-01-01T01:25:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:55:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|        61| Brooklyn|\n",
      "|         265|         162|      null|   null|    null|                null|              B02883|2020-01-01T01:18:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:28:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|       162|Manhattan|\n",
      "|          71|          71|      null|   null|    null|                null|              B02872|2020-01-01T00:22:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:20:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|        71| Brooklyn|\n",
      "|          35|          71|      null|   null|    null|                null|              B02872|2020-01-01T00:35:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:26:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|        71| Brooklyn|\n",
      "|          76|          61|      null|   null|    null|                null|              B02872|2020-01-01T01:07:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:45:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|        61| Brooklyn|\n",
      "+------------+------------+----------+-------+--------+--------------------+--------------------+--------------------+-----+-----------+-----------------+---------------------+---------------------+--------------------+-----+-------+---------------+------------+--------------------+-------+------------------+----------+------------+------------+---------------------+--------------------+-------------+---------+----+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan(isFinalPlan=false)\n",
      "+- BroadcastNestedLoopJoin BuildRight, Inner\n",
      "   :- BroadcastHashJoin [PULocationID#4282L, DOLocationID#4281L], [LocationID#4346L, LocationID#4346L], Inner, BuildRight\n",
      "   :  :- Project [DOLocationID#4281L, PULocationID#4282L, RatecodeID#4283, SR_Flag#4284, VendorID#4285, congestion_surcharge#4286, dispatching_base_num#4287, dropoff_datetime#4288, extra#4289, fare_amount#4290, hvfhs_license_num#4291, improvement_surcharge#4292, lpep_dropoff_datetime#4293, lpep_pickup_datetime#4294, month#4295, mta_tax#4296, passenger_count#4297L, payment_type#4298, pickup_datetime#4299, service#4300, store_and_fwd_flag#4301, tip_amount#4302, tolls_amount#4303, total_amount#4304, ... 5 more fields]\n",
      "   :  :  +- Filter ((((isnotnull(pickup_datetime#4299) && isnotnull(dropoff_datetime#4288)) && (DOLocationID#4281L = PULocationID#4282L)) && isnotnull(DOLocationID#4281L)) && isnotnull(PULocationID#4282L))\n",
      "   :  :     +- FileScan json [DOLocationID#4281L,PULocationID#4282L,RatecodeID#4283,SR_Flag#4284,VendorID#4285,congestion_surcharge#4286,dispatching_base_num#4287,dropoff_datetime#4288,extra#4289,fare_amount#4290,hvfhs_license_num#4291,improvement_surcharge#4292,lpep_dropoff_datetime#4293,lpep_pickup_datetime#4294,month#4295,mta_tax#4296,passenger_count#4297L,payment_type#4298,pickup_datetime#4299,service#4300,store_and_fwd_flag#4301,tip_amount#4302,tolls_amount#4303,total_amount#4304,... 5 more fields] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://ip-172-31-6-49.us-east-2.compute.internal:8020/tmp/data/nyc-taxi/taxi-da..., PartitionFilters: [], PushedFilters: [IsNotNull(pickup_datetime), IsNotNull(dropoff_datetime), IsNotNull(DOLocationID), IsNotNull(PULo..., ReadSchema: struct<DOLocationID:bigint,PULocationID:bigint,RatecodeID:string,SR_Flag:string,VendorID:string,c...\n",
      "   :  +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true], input[0, bigint, true]))\n",
      "   :     +- Project [LocationID#4346L, Borough#4345 AS PUBorough#4355]\n",
      "   :        +- Filter isnotnull(LocationID#4346L)\n",
      "   :           +- FileScan json [Borough#4345,LocationID#4346L] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://ip-172-31-6-49.us-east-2.compute.internal:8020/tmp/data/nyc-taxi/zone-lo..., PartitionFilters: [], PushedFilters: [IsNotNull(LocationID)], ReadSchema: struct<Borough:string,LocationID:bigint>\n",
      "   +- BroadcastExchange IdentityBroadcastMode\n",
      "      +- Project [LocationID#4457L, Borough#4456 AS DOBorough#4453]\n",
      "         +- FileScan json [Borough#4456,LocationID#4457L] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://ip-172-31-6-49.us-east-2.compute.internal:8020/tmp/data/nyc-taxi/zone-lo..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Borough:string,LocationID:bigint>"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")\n",
    "df_taxi = spark.read.json(taxiPath)\n",
    "df_taxi_lookup = spark.read.json(taxiLookupPath)\n",
    "\n",
    "taxi_filtered = (df_taxi\n",
    " .filter(df_taxi.pickup_datetime.isNotNull())\n",
    " .filter(df_taxi.dropoff_datetime.isNotNull()))\n",
    "taxi_pu = (taxi_filtered\n",
    ".join(df_taxi_lookup\n",
    "       .select(\"LocationID\", \"Borough\")\n",
    "       .withColumnRenamed(\"Borough\", \"PUBorough\"), \n",
    "       df_taxi_lookup.LocationID == df_taxi.PULocationID))\n",
    "taxi = (taxi_pu.join(df_taxi_lookup\n",
    "       .select(\"LocationID\", \"Borough\")\n",
    "       .withColumnRenamed(\"Borough\", \"DOBorough\"), \n",
    "       df_taxi_lookup.LocationID == taxi_pu.DOLocationID))\n",
    "taxi_pu.explain()\n",
    "taxi_pu.show()\n",
    "taxi.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f4e1e045f14decad3bf0214d1d6b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_monthly_totals_pyspark(taxiPath, taxiLookupPath):\n",
    "    taxi = spark.read.json(taxiPath)\n",
    "    taxi_lookup = spark.read.json(taxiLookupPath)\n",
    "    taxi_filtered = (taxi\n",
    "     .filter(taxi.pickup_datetime.isNotNull())\n",
    "     .filter(taxi.dropoff_datetime.isNotNull()))\n",
    "                     \n",
    "    groupDF = taxi_filtered.join(taxi_lookup, taxi_filtered.PULocationID == taxi_lookup.LocationID)\n",
    "#     groupDF.select(\"ingested_on\").show() # expected error\n",
    "    groupDF.show()\n",
    "    return groupDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d6d21e996b4a88a98ed6c894498f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+----------+-------+--------+--------------------+--------------------+--------------------+-----+-----------+-----------------+---------------------+---------------------+--------------------+-----+-------+---------------+------------+--------------------+-------+------------------+----------+------------+------------+---------------------+--------------------+-------------+---------+----+---------+----------+--------------------+------------+\n",
      "|DOLocationID|PULocationID|RatecodeID|SR_Flag|VendorID|congestion_surcharge|dispatching_base_num|    dropoff_datetime|extra|fare_amount|hvfhs_license_num|improvement_surcharge|lpep_dropoff_datetime|lpep_pickup_datetime|month|mta_tax|passenger_count|payment_type|     pickup_datetime|service|store_and_fwd_flag|tip_amount|tolls_amount|total_amount|tpep_dropoff_datetime|tpep_pickup_datetime|trip_distance|trip_type|year|  Borough|LocationID|                Zone|service_zone|\n",
      "+------------+------------+----------+-------+--------+--------------------+--------------------+--------------------+-----+-----------+-----------------+---------------------+---------------------+--------------------+-----+-------+---------------+------------+--------------------+-------+------------------+----------+------------+------------+---------------------+--------------------+-------------+---------+----+---------+----------+--------------------+------------+\n",
      "|          90|         148|      null|   null|    null|                null|              B02864|2020-01-01T01:02:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:45:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|Manhattan|       148|     Lower East Side| Yellow Zone|\n",
      "|          79|         114|      null|   null|    null|                null|              B02682|2020-01-01T00:53:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:47:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|Manhattan|       114|Greenwich Village...| Yellow Zone|\n",
      "|         125|           4|      null|   null|    null|                null|              B02764|2020-01-01T00:21:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:04:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|Manhattan|         4|       Alphabet City| Yellow Zone|\n",
      "|         113|         231|      null|   null|    null|                null|              B02764|2020-01-01T00:33:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:26:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|Manhattan|       231|TriBeCa/Civic Center| Yellow Zone|\n",
      "|         144|         114|      null|   null|    null|                null|              B02764|2020-01-01T00:46:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:37:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|Manhattan|       114|Greenwich Village...| Yellow Zone|\n",
      "|         137|         144|      null|   null|    null|                null|              B02764|2020-01-01T01:07:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:49:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|Manhattan|       144| Little Italy/NoLiTa| Yellow Zone|\n",
      "|         148|         249|      null|   null|    null|                null|              B02870|2020-01-01T00:36:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:21:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|Manhattan|       249|        West Village| Yellow Zone|\n",
      "|           4|         148|      null|   null|    null|                null|              B02870|2020-01-01T00:42:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:38:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|Manhattan|       148|     Lower East Side| Yellow Zone|\n",
      "|           7|          79|      null|   null|    null|                null|              B02870|2020-01-01T01:09:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:46:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|Manhattan|        79|        East Village| Yellow Zone|\n",
      "|         236|         140|      null|   null|    null|                null|              B02836|2020-01-01T00:23:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:15:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|Manhattan|       140|     Lenox Hill East| Yellow Zone|\n",
      "|         149|          75|      null|   null|    null|                null|              B02836|2020-01-01T01:13:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:28:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|Manhattan|        75|   East Harlem South|   Boro Zone|\n",
      "|         229|          33|      null|   null|    null|                null|              B02875|2020-01-01T00:40:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:14:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020| Brooklyn|        33|    Brooklyn Heights|   Boro Zone|\n",
      "|          74|         229|      null|   null|    null|                null|              B02875|2020-01-01T00:59:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:45:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|Manhattan|       229|Sutton Place/Turt...| Yellow Zone|\n",
      "|         238|         114|      null|   null|    null|                null|              B02764|2020-01-01T00:42:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:16:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|Manhattan|       114|Greenwich Village...| Yellow Zone|\n",
      "|          89|          85|      null|   null|    null|                null|              B02617|2020-01-01T00:45:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:40:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020| Brooklyn|        85|             Erasmus|   Boro Zone|\n",
      "|         202|          61|      null|   null|    null|                null|              B02617|2020-01-01T01:25:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:55:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020| Brooklyn|        61| Crown Heights North|   Boro Zone|\n",
      "|         265|         162|      null|   null|    null|                null|              B02883|2020-01-01T01:18:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:28:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020|Manhattan|       162|        Midtown East| Yellow Zone|\n",
      "|          71|          71|      null|   null|    null|                null|              B02872|2020-01-01T00:22:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:20:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020| Brooklyn|        71|East Flatbush/Far...|   Boro Zone|\n",
      "|          35|          71|      null|   null|    null|                null|              B02872|2020-01-01T00:35:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:26:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020| Brooklyn|        71|East Flatbush/Far...|   Boro Zone|\n",
      "|          76|          61|      null|   null|    null|                null|              B02872|2020-01-01T01:07:...| null|       null|           HV0003|                 null|                 null|                null|   01|   null|           null|        null|2020-01-01T00:45:...|  fhvhv|              null|      null|        null|        null|                 null|                null|         null|     null|2020| Brooklyn|        61| Crown Heights North|   Boro Zone|\n",
      "+------------+------------+----------+-------+--------+--------------------+--------------------+--------------------+-----+-----------+-----------------+---------------------+---------------------+--------------------+-----+-------+---------------+------------+--------------------+-------+------------------+----------+------------+------------+---------------------+--------------------+-------------+---------+----+---------+----------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "(20.882759264000015, DataFrame[DOLocationID: bigint, PULocationID: bigint, RatecodeID: string, SR_Flag: string, VendorID: string, congestion_surcharge: string, dispatching_base_num: string, dropoff_datetime: string, extra: string, fare_amount: double, hvfhs_license_num: string, improvement_surcharge: string, lpep_dropoff_datetime: string, lpep_pickup_datetime: string, month: string, mta_tax: string, passenger_count: bigint, payment_type: string, pickup_datetime: string, service: string, store_and_fwd_flag: string, tip_amount: double, tolls_amount: string, total_amount: string, tpep_dropoff_datetime: string, tpep_pickup_datetime: string, trip_distance: double, trip_type: string, year: string, Borough: string, LocationID: bigint, Zone: string, service_zone: string])"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_monthly_totals_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1303340a7f4213b179f74a737e7ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_monthly_totals_pandas(taxiPath, taxiLookupPath):\n",
    "    taxi = pd.read_json(taxiPath)\n",
    "    taxi_lookup = pd.read_json(taxiLookupPath)\n",
    "    taxi_filtered = tax.dropna(subset=['pickup_datetime', 'dropoff_datetime'])\n",
    "    \n",
    "    groupDF = taxi_filtered.join(taxi_lookup.set_index('LocationID'), on='PULocationID')\n",
    "    groupDF['pickup_month'] = pd.to_datetime(groupDF['pickup_datetime'], format='%m%Y')\n",
    "    groupDF = groupDF.groupby('pickup_month', 'borough').agg('count').sort_values(by=['count', 'borough'], ascending=[False, True])\n",
    "    groupDF\n",
    "    return groupDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71031545e5b042d7ae2bfbac0ea4f983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyarrow and local java libraries required for HDFS\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 45, in timer_method\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 233, in timeit\n",
      "    return Timer(stmt, setup, timer, globals).timeit(number)\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 177, in timeit\n",
      "    timing = self.inner(it, self.timer)\n",
      "  File \"<timeit-src>\", line 6, in inner\n",
      "  File \"<stdin>\", line 2, in get_monthly_totals_pandas\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/pandas/util/_decorators.py\", line 199, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/pandas/util/_decorators.py\", line 296, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/pandas/io/json/_json.py\", line 594, in read_json\n",
      "    path_or_buf, encoding=encoding, compression=compression\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/pandas/io/common.py\", line 222, in get_filepath_or_buffer\n",
      "    filepath_or_buffer, mode=mode or \"rb\", **(storage_options or {})\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/fsspec/core.py\", line 438, in open\n",
      "    **kwargs\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/fsspec/core.py\", line 287, in open_files\n",
      "    expand=expand,\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/fsspec/core.py\", line 600, in get_fs_token_paths\n",
      "    cls = get_filesystem_class(protocol)\n",
      "  File \"/tmp/1603064188681-0/lib/python3.7/site-packages/fsspec/registry.py\", line 204, in get_filesystem_class\n",
      "    raise ImportError(bit[\"err\"]) from e\n",
      "ImportError: pyarrow and local java libraries required for HDFS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_monthly_totals_pandas(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c372edca93674129a627d0d84613c8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_monthly_totals_pandas(taxiPath, taxiLookupPath):\n",
    "    taxiPyspark = spark.read.json(taxiPath)\n",
    "    taxiLookupPySpark = spark.read.json(taxiLookupPath)\n",
    "    \n",
    "    taxi = taxiPyspark.toPandas()\n",
    "    taxi_lookup = taxiLookupPyspark.toPandas()\n",
    "    taxi_filtered = taxi.dropna(subset=['pickup_datetime', 'dropoff_datetime'])\n",
    "    \n",
    "    groupDF = taxi_filtered.join(taxi_lookup.set_index('LocationID'), on='PULocationID')\n",
    "    groupDF['pickup_month'] = pd.to_datetime(groupDF['pickup_datetime'], format='%m%Y')\n",
    "    groupDF = groupDF.groupby('pickup_month', 'borough').agg('count').sort_values(by=['count', 'borough'], ascending=[False, True])\n",
    "    groupDF\n",
    "    return groupDF\n",
    "\n",
    "def get_monthly_totals_pyspark(taxiPath, taxiLookupPath):\n",
    "    taxi = spark.read.json(taxiPath)\n",
    "    taxi_lookup = spark.read.json(taxiLookupPath)\n",
    "    \n",
    "    taxi_filtered = (taxi\n",
    "     .filter(taxi.pickup_datetime.isNotNull())\n",
    "     .filter(taxi.dropoff_datetime.isNotNull()))\n",
    "                     \n",
    "    groupDF = taxi_filtered.join(taxi_lookup.select(\"Borough\", \"LocationID\"), taxi_filtered.PULocationID == taxi_lookup.LocationID)\n",
    "    groupDF = groupDF.withColumn(\"pickup_month\", f.date_format(\"pickup_datetime\", \"yyyyMM\"))\n",
    "    groupDF = groupDF.groupBy(\"pickup_month\", \"borough\").count().orderBy(f.desc(\"pickup_month\"), f.desc(\"count\"), \"borough\")\n",
    "    groupDF.show()\n",
    "    return groupDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3bf9f49ba64de0b824d6cd18f71158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+--------+\n",
      "|pickup_month|      borough|   count|\n",
      "+------------+-------------+--------+\n",
      "|      202101|    Manhattan|       3|\n",
      "|      202007|    Manhattan|       6|\n",
      "|      202006|    Manhattan|       1|\n",
      "|      202005|    Manhattan|       5|\n",
      "|      202004|    Manhattan|       1|\n",
      "|      202003|    Manhattan|       5|\n",
      "|      202002|    Manhattan|      34|\n",
      "|      202002|       Queens|       9|\n",
      "|      202002|     Brooklyn|       3|\n",
      "|      202002|        Bronx|       1|\n",
      "|      202002|      Unknown|       1|\n",
      "|      202001|    Manhattan|14817800|\n",
      "|      202001|     Brooklyn| 5628274|\n",
      "|      202001|       Queens| 4509457|\n",
      "|      202001|        Bronx| 2536611|\n",
      "|      202001|      Unknown| 1615247|\n",
      "|      202001|Staten Island|  260258|\n",
      "|      202001|          EWR|    3779|\n",
      "|      201912|    Manhattan|     129|\n",
      "|      201912|       Queens|      17|\n",
      "+------------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "(46.01737120200005, DataFrame[pickup_month: string, borough: string, count: bigint])"
     ]
    }
   ],
   "source": [
    "# print(timer_method(\"get_monthly_totals_pandas(taxiPath, taxiLookupPath)\"))\n",
    "print(timer_method(\"get_monthly_totals_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fafbdcd07fa24578a6eb945e051eab70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5G"
     ]
    }
   ],
   "source": [
    "## Expected error for maxResultSize: This won't work. Could try the subsequent cells\n",
    "## Those restart the state of the notebook and don't work as expected\n",
    "## Need to restart the cluster and edit the Software config with: [{\"classification\":\"spark-defaults\", \"properties\":{\"spark.driver.maxResultSize\":\"5G\", \"spark.ui.killEnabled\":\"true\"}, \"configurations\":[]}]\n",
    "## Then need to reun the taxi and taxi lookup ingests\n",
    "## Run -> Run All Above Selected Cell\n",
    "## Second expected error for {\"msg\":\"requirement failed: Session isn't active.\"} and will hang. Driver node ran out of mem. Will need to go and upscale\n",
    "spark.conf.set(\"spark.driver.maxResultSize\", \"5G\")\n",
    "print(spark.conf.get('spark.driver.maxResultSize'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1603063175185_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-6-49.us-east-2.compute.internal:20888/proxy/application_1603063175185_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-4-197.us-east-2.compute.internal:8042/node/containerlogs/container_1603063175185_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.driver.maxResultSize': '5G'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1603063175185_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-6-49.us-east-2.compute.internal:20888/proxy/application_1603063175185_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-4-197.us-east-2.compute.internal:8042/node/containerlogs/container_1603063175185_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"conf\":{\"spark.driver.maxResultSize\":\"5G\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': 'python3', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1603063175185_0003</td><td>pyspark</td><td>dead</td><td><a target=\"_blank\" href=\"http://ip-172-31-6-49.us-east-2.compute.internal:20888/proxy/application_1603063175185_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-6-49.us-east-2.compute.internal:8188/applicationhistory/logs/ip-172-31-8-43.us-east-2.compute.internal:8041/container_1603063175185_0003_01_000001/container_1603063175185_0003_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a600607890474dd58fe9eb486ee8a135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_monthly_totals_concat_pandas(taxiPath, taxiLookupPath):\n",
    "    taxiPyspark = spark.read.json(taxiPath)\n",
    "    taxiLookupPySpark = spark.read.json(taxiLookupPath)\n",
    "    \n",
    "    taxi = taxiPyspark.toPandas()\n",
    "    taxi_lookup = taxiLookupPyspark.toPandas()\n",
    "    taxi_filtered = taxi.dropna(subset=['pickup_datetime', 'dropoff_datetime'])\n",
    "    \n",
    "    groupDF = taxi_filtered.join(taxi_lookup.set_index('LocationID'), on='PULocationID')\n",
    "    groupDF['pickup_month'] = groupDF['month'] + groupDF['year']\n",
    "    groupDF = groupDF.groupby('pickup_month', 'borough').agg('count').sort_values(by=['count', 'borough'], ascending=[False, True])\n",
    "    groupDF\n",
    "    return groupDF\n",
    "    \n",
    "def get_monthly_totals_concat_pyspark(taxiPath, taxiLookupPath):\n",
    "    taxi = spark.read.json(taxiPath)\n",
    "    taxi_lookup = spark.read.json(taxiLookupPath)\n",
    "    taxi_filtered = (taxi\n",
    "     .filter(taxi.pickup_datetime.isNotNull())\n",
    "     .filter(taxi.dropoff_datetime.isNotNull()))\n",
    "         \n",
    "    groupDF = taxi_filtered.withColumn(\"pickup_month\", f.concat(\"month\", \"year\"))#.select(\"pickup_datetime\", \"borough\", \"pickup_month\")\n",
    "    groupDF = groupDF.groupBy(\"pickup_month\", \"borough\").count().orderBy(f.desc(\"pickup_month\"), f.desc(\"count\"), \"borough\")\n",
    "    groupDF.show()\n",
    "    return groupDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7aa78e259814f98ae4ed04bdfc0d454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 2 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "#\n",
      "# There is insufficient memory for the Java Runtime Environment to continue.\n",
      "# Native memory allocation (mmap) failed to map 2498269184 bytes for committing reserved memory.\n",
      "# An error report file with more information is saved as:\n",
      "# /tmp/hs_err_pid20386.log\n",
      "\n",
      "stderr: \n",
      "20/10/19 00:18:13 INFO TaskSetManager: Finished task 31.0 in stage 118.0 (TID 2090) in 3819 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 28) (31/76)\n",
      "20/10/19 00:18:13 INFO BlockManagerInfo: Removed taskresult_2090 on ip-172-31-8-43.us-east-2.compute.internal:40247 in memory (size: 20.3 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:13 INFO BlockManagerInfo: Added taskresult_2091 in memory on ip-172-31-4-197.us-east-2.compute.internal:37927 (size: 20.4 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:13 INFO TaskSetManager: Starting task 47.0 in stage 118.0 (TID 2107, ip-172-31-4-197.us-east-2.compute.internal, executor 27, partition 47, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:13 INFO TaskSetManager: Finished task 28.0 in stage 118.0 (TID 2091) in 3820 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 27) (32/76)\n",
      "20/10/19 00:18:13 INFO BlockManagerInfo: Removed taskresult_2091 on ip-172-31-4-197.us-east-2.compute.internal:37927 in memory (size: 20.4 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:15 INFO BlockManagerInfo: Added taskresult_2093 in memory on ip-172-31-8-43.us-east-2.compute.internal:41187 (size: 20.4 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:15 INFO TaskSetManager: Starting task 48.0 in stage 118.0 (TID 2108, ip-172-31-8-43.us-east-2.compute.internal, executor 25, partition 48, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:15 INFO BlockManagerInfo: Added taskresult_2096 in memory on ip-172-31-4-197.us-east-2.compute.internal:43813 (size: 23.0 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:15 INFO BlockManagerInfo: Added taskresult_2095 in memory on ip-172-31-8-43.us-east-2.compute.internal:41187 (size: 23.1 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:15 INFO TaskSetManager: Starting task 53.0 in stage 118.0 (TID 2109, ip-172-31-4-197.us-east-2.compute.internal, executor 26, partition 53, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:15 INFO TaskSetManager: Starting task 49.0 in stage 118.0 (TID 2110, ip-172-31-8-43.us-east-2.compute.internal, executor 25, partition 49, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:15 INFO BlockManagerInfo: Added taskresult_2094 in memory on ip-172-31-8-43.us-east-2.compute.internal:41187 (size: 23.0 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:15 INFO BlockManagerInfo: Added taskresult_2092 in memory on ip-172-31-8-43.us-east-2.compute.internal:41187 (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:15 INFO BlockManagerInfo: Added taskresult_2097 in memory on ip-172-31-4-197.us-east-2.compute.internal:37927 (size: 23.0 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:15 INFO TaskSetManager: Starting task 50.0 in stage 118.0 (TID 2111, ip-172-31-8-43.us-east-2.compute.internal, executor 25, partition 50, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:15 INFO TaskSetManager: Starting task 51.0 in stage 118.0 (TID 2112, ip-172-31-8-43.us-east-2.compute.internal, executor 25, partition 51, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:15 INFO TaskSetManager: Starting task 54.0 in stage 118.0 (TID 2113, ip-172-31-4-197.us-east-2.compute.internal, executor 27, partition 54, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:15 INFO TaskSetManager: Finished task 34.0 in stage 118.0 (TID 2093) in 3533 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 25) (33/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2093 on ip-172-31-8-43.us-east-2.compute.internal:41187 in memory (size: 20.4 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2099 in memory on ip-172-31-4-197.us-east-2.compute.internal:37927 (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2098 in memory on ip-172-31-8-43.us-east-2.compute.internal:40247 (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 57.0 in stage 118.0 (TID 2114, ip-172-31-4-197.us-east-2.compute.internal, executor 27, partition 57, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 52.0 in stage 118.0 (TID 2115, ip-172-31-8-43.us-east-2.compute.internal, executor 28, partition 52, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 32.0 in stage 118.0 (TID 2096) in 3504 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 26) (34/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2096 on ip-172-31-4-197.us-east-2.compute.internal:43813 in memory (size: 23.0 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 41.0 in stage 118.0 (TID 2095) in 3619 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 25) (35/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2095 on ip-172-31-8-43.us-east-2.compute.internal:41187 in memory (size: 23.1 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 35.0 in stage 118.0 (TID 2097) in 3491 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 27) (36/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2097 on ip-172-31-4-197.us-east-2.compute.internal:37927 in memory (size: 23.0 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 38.0 in stage 118.0 (TID 2094) in 3770 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 25) (37/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2100 in memory on ip-172-31-8-43.us-east-2.compute.internal:40247 (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 55.0 in stage 118.0 (TID 2116, ip-172-31-8-43.us-east-2.compute.internal, executor 28, partition 55, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2094 on ip-172-31-8-43.us-east-2.compute.internal:41187 in memory (size: 23.0 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2105 in memory on ip-172-31-4-197.us-east-2.compute.internal:43813 (size: 22.9 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 36.0 in stage 118.0 (TID 2099) in 3426 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 27) (38/76)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 59.0 in stage 118.0 (TID 2117, ip-172-31-4-197.us-east-2.compute.internal, executor 26, partition 59, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2099 on ip-172-31-4-197.us-east-2.compute.internal:37927 in memory (size: 20.3 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 33.0 in stage 118.0 (TID 2092) in 3898 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 25) (39/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2102 in memory on ip-172-31-4-197.us-east-2.compute.internal:43813 (size: 20.4 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2092 on ip-172-31-8-43.us-east-2.compute.internal:41187 in memory (size: 20.3 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2104 in memory on ip-172-31-4-197.us-east-2.compute.internal:37927 (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 60.0 in stage 118.0 (TID 2118, ip-172-31-4-197.us-east-2.compute.internal, executor 26, partition 60, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 61.0 in stage 118.0 (TID 2119, ip-172-31-4-197.us-east-2.compute.internal, executor 27, partition 61, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 42.0 in stage 118.0 (TID 2098) in 3631 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 28) (40/76)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 44.0 in stage 118.0 (TID 2105) in 3361 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 26) (41/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2105 on ip-172-31-4-197.us-east-2.compute.internal:43813 in memory (size: 22.9 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2098 on ip-172-31-8-43.us-east-2.compute.internal:40247 in memory (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2106 in memory on ip-172-31-8-43.us-east-2.compute.internal:40247 (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 56.0 in stage 118.0 (TID 2120, ip-172-31-8-43.us-east-2.compute.internal, executor 28, partition 56, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 40.0 in stage 118.0 (TID 2104) in 3475 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 27) (42/76)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 39.0 in stage 118.0 (TID 2102) in 3636 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 26) (43/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2102 on ip-172-31-4-197.us-east-2.compute.internal:43813 in memory (size: 20.4 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2104 on ip-172-31-4-197.us-east-2.compute.internal:37927 in memory (size: 20.3 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 43.0 in stage 118.0 (TID 2100) in 3689 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 28) (44/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2101 in memory on ip-172-31-4-197.us-east-2.compute.internal:43813 (size: 20.4 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 62.0 in stage 118.0 (TID 2121, ip-172-31-4-197.us-east-2.compute.internal, executor 26, partition 62, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2100 on ip-172-31-8-43.us-east-2.compute.internal:40247 in memory (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2103 in memory on ip-172-31-8-43.us-east-2.compute.internal:40247 (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 58.0 in stage 118.0 (TID 2122, ip-172-31-8-43.us-east-2.compute.internal, executor 28, partition 58, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 46.0 in stage 118.0 (TID 2106) in 3494 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 28) (45/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2107 in memory on ip-172-31-4-197.us-east-2.compute.internal:37927 (size: 23.1 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 63.0 in stage 118.0 (TID 2123, ip-172-31-4-197.us-east-2.compute.internal, executor 27, partition 63, NODE_LOCAL, 8577 bytes)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2106 on ip-172-31-8-43.us-east-2.compute.internal:40247 in memory (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 37.0 in stage 118.0 (TID 2101) in 3820 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 26) (46/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2101 on ip-172-31-4-197.us-east-2.compute.internal:43813 in memory (size: 20.4 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 45.0 in stage 118.0 (TID 2103) in 3758 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 28) (47/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2103 on ip-172-31-8-43.us-east-2.compute.internal:40247 in memory (size: 20.3 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 47.0 in stage 118.0 (TID 2107) in 3398 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 27) (48/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2107 on ip-172-31-4-197.us-east-2.compute.internal:37927 in memory (size: 23.1 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:17 INFO BlockManagerInfo: Added taskresult_2109 in memory on ip-172-31-4-197.us-east-2.compute.internal:43813 (size: 14.9 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:17 INFO TaskSetManager: Starting task 64.0 in stage 118.0 (TID 2124, ip-172-31-4-197.us-east-2.compute.internal, executor 26, partition 64, NODE_LOCAL, 8767 bytes)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Added taskresult_2108 in memory on ip-172-31-8-43.us-east-2.compute.internal:41187 (size: 15.4 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Added taskresult_2110 in memory on ip-172-31-8-43.us-east-2.compute.internal:41187 (size: 15.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Finished task 53.0 in stage 118.0 (TID 2109) in 2181 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 26) (49/76)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Starting task 65.0 in stage 118.0 (TID 2125, ip-172-31-8-43.us-east-2.compute.internal, executor 25, partition 65, NODE_LOCAL, 8767 bytes)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Starting task 66.0 in stage 118.0 (TID 2126, ip-172-31-8-43.us-east-2.compute.internal, executor 25, partition 66, NODE_LOCAL, 8767 bytes)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Added taskresult_2112 in memory on ip-172-31-8-43.us-east-2.compute.internal:41187 (size: 15.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Added taskresult_2113 in memory on ip-172-31-4-197.us-east-2.compute.internal:37927 (size: 14.9 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Starting task 68.0 in stage 118.0 (TID 2127, ip-172-31-4-197.us-east-2.compute.internal, executor 27, partition 68, NODE_LOCAL, 8767 bytes)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Starting task 67.0 in stage 118.0 (TID 2128, ip-172-31-8-43.us-east-2.compute.internal, executor 25, partition 67, NODE_LOCAL, 8767 bytes)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Removed taskresult_2109 on ip-172-31-4-197.us-east-2.compute.internal:43813 in memory (size: 14.9 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Added taskresult_2111 in memory on ip-172-31-8-43.us-east-2.compute.internal:41187 (size: 15.2 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Starting task 69.0 in stage 118.0 (TID 2129, ip-172-31-8-43.us-east-2.compute.internal, executor 25, partition 69, NODE_LOCAL, 8957 bytes)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Finished task 49.0 in stage 118.0 (TID 2110) in 2233 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 25) (50/76)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Removed taskresult_2110 on ip-172-31-8-43.us-east-2.compute.internal:41187 in memory (size: 15.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Finished task 54.0 in stage 118.0 (TID 2113) in 2158 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 27) (51/76)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Finished task 48.0 in stage 118.0 (TID 2108) in 2324 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 25) (52/76)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Removed taskresult_2113 on ip-172-31-4-197.us-east-2.compute.internal:37927 in memory (size: 14.9 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Removed taskresult_2108 on ip-172-31-8-43.us-east-2.compute.internal:41187 in memory (size: 15.4 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Finished task 51.0 in stage 118.0 (TID 2112) in 2238 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 25) (53/76)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Added taskresult_2114 in memory on ip-172-31-4-197.us-east-2.compute.internal:37927 (size: 14.9 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Starting task 70.0 in stage 118.0 (TID 2130, ip-172-31-4-197.us-east-2.compute.internal, executor 27, partition 70, NODE_LOCAL, 8957 bytes)\n"
     ]
    }
   ],
   "source": [
    "# print(timer_method(\"get_monthly_totals_concat_pandas(taxiPath, taxiLookupPath)\"))\n",
    "print(timer_method(\"get_monthly_totals_concat_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.8 - Case Study 2: Month over month, get the borough with the most amount of pickups per month\n",
    "\n",
    "Add pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68a17dded08486dad5ef0b441415205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_most_pickups_per_month_pandas(taxiPath, taxiLookupPath):\n",
    "    inputDF = get_monthly_totals_pandas(taxiPath, taxiLookupPath)\n",
    "    firstDF = inputDF.groupby('pickup_month').sort_values(by=['pickup_month', 'count'], ascending=[True, False]).first()\n",
    "    firstDF\n",
    "    return firstDF\n",
    "\n",
    "def get_most_pickups_per_month_pyspark(taxiPath, taxiLookupPath):\n",
    "    inputDF = get_monthly_totals_pyspark(taxiPath, taxiLookupPath)\n",
    "    firstDF = inputDF.groupBy(\"pickup_month\").orderBy(f.desc(\"pickup_month\"), f.desc(\"count\")).agg(f.first(\"borough\"))\n",
    "    firstDF.explain()\n",
    "    firstDF.show()\n",
    "    return firstDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa98beafe9db4f9bba0ecc533db04d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3be9dfce8d543feb922051617b090b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '400' from https://172.31.6.49:18888/sessions/2/statements/90 with error payload: {\"msg\":\"requirement failed: Session isn't active.\"}\n"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_most_pickups_per_month_pandas(taxiPath, taxiLookupPath)\"))\n",
    "#print(timer_method(\"get_most_pickups_per_month_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c08702b3219447eb9b98053cd08e74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 2 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "#\n",
      "# There is insufficient memory for the Java Runtime Environment to continue.\n",
      "# Native memory allocation (mmap) failed to map 2498269184 bytes for committing reserved memory.\n",
      "# An error report file with more information is saved as:\n",
      "# /tmp/hs_err_pid20386.log\n",
      "\n",
      "stderr: \n",
      "20/10/19 00:18:13 INFO TaskSetManager: Finished task 31.0 in stage 118.0 (TID 2090) in 3819 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 28) (31/76)\n",
      "20/10/19 00:18:13 INFO BlockManagerInfo: Removed taskresult_2090 on ip-172-31-8-43.us-east-2.compute.internal:40247 in memory (size: 20.3 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:13 INFO BlockManagerInfo: Added taskresult_2091 in memory on ip-172-31-4-197.us-east-2.compute.internal:37927 (size: 20.4 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:13 INFO TaskSetManager: Starting task 47.0 in stage 118.0 (TID 2107, ip-172-31-4-197.us-east-2.compute.internal, executor 27, partition 47, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:13 INFO TaskSetManager: Finished task 28.0 in stage 118.0 (TID 2091) in 3820 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 27) (32/76)\n",
      "20/10/19 00:18:13 INFO BlockManagerInfo: Removed taskresult_2091 on ip-172-31-4-197.us-east-2.compute.internal:37927 in memory (size: 20.4 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:15 INFO BlockManagerInfo: Added taskresult_2093 in memory on ip-172-31-8-43.us-east-2.compute.internal:41187 (size: 20.4 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:15 INFO TaskSetManager: Starting task 48.0 in stage 118.0 (TID 2108, ip-172-31-8-43.us-east-2.compute.internal, executor 25, partition 48, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:15 INFO BlockManagerInfo: Added taskresult_2096 in memory on ip-172-31-4-197.us-east-2.compute.internal:43813 (size: 23.0 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:15 INFO BlockManagerInfo: Added taskresult_2095 in memory on ip-172-31-8-43.us-east-2.compute.internal:41187 (size: 23.1 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:15 INFO TaskSetManager: Starting task 53.0 in stage 118.0 (TID 2109, ip-172-31-4-197.us-east-2.compute.internal, executor 26, partition 53, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:15 INFO TaskSetManager: Starting task 49.0 in stage 118.0 (TID 2110, ip-172-31-8-43.us-east-2.compute.internal, executor 25, partition 49, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:15 INFO BlockManagerInfo: Added taskresult_2094 in memory on ip-172-31-8-43.us-east-2.compute.internal:41187 (size: 23.0 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:15 INFO BlockManagerInfo: Added taskresult_2092 in memory on ip-172-31-8-43.us-east-2.compute.internal:41187 (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:15 INFO BlockManagerInfo: Added taskresult_2097 in memory on ip-172-31-4-197.us-east-2.compute.internal:37927 (size: 23.0 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:15 INFO TaskSetManager: Starting task 50.0 in stage 118.0 (TID 2111, ip-172-31-8-43.us-east-2.compute.internal, executor 25, partition 50, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:15 INFO TaskSetManager: Starting task 51.0 in stage 118.0 (TID 2112, ip-172-31-8-43.us-east-2.compute.internal, executor 25, partition 51, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:15 INFO TaskSetManager: Starting task 54.0 in stage 118.0 (TID 2113, ip-172-31-4-197.us-east-2.compute.internal, executor 27, partition 54, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:15 INFO TaskSetManager: Finished task 34.0 in stage 118.0 (TID 2093) in 3533 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 25) (33/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2093 on ip-172-31-8-43.us-east-2.compute.internal:41187 in memory (size: 20.4 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2099 in memory on ip-172-31-4-197.us-east-2.compute.internal:37927 (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2098 in memory on ip-172-31-8-43.us-east-2.compute.internal:40247 (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 57.0 in stage 118.0 (TID 2114, ip-172-31-4-197.us-east-2.compute.internal, executor 27, partition 57, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 52.0 in stage 118.0 (TID 2115, ip-172-31-8-43.us-east-2.compute.internal, executor 28, partition 52, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 32.0 in stage 118.0 (TID 2096) in 3504 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 26) (34/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2096 on ip-172-31-4-197.us-east-2.compute.internal:43813 in memory (size: 23.0 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 41.0 in stage 118.0 (TID 2095) in 3619 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 25) (35/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2095 on ip-172-31-8-43.us-east-2.compute.internal:41187 in memory (size: 23.1 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 35.0 in stage 118.0 (TID 2097) in 3491 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 27) (36/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2097 on ip-172-31-4-197.us-east-2.compute.internal:37927 in memory (size: 23.0 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 38.0 in stage 118.0 (TID 2094) in 3770 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 25) (37/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2100 in memory on ip-172-31-8-43.us-east-2.compute.internal:40247 (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 55.0 in stage 118.0 (TID 2116, ip-172-31-8-43.us-east-2.compute.internal, executor 28, partition 55, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2094 on ip-172-31-8-43.us-east-2.compute.internal:41187 in memory (size: 23.0 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2105 in memory on ip-172-31-4-197.us-east-2.compute.internal:43813 (size: 22.9 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 36.0 in stage 118.0 (TID 2099) in 3426 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 27) (38/76)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 59.0 in stage 118.0 (TID 2117, ip-172-31-4-197.us-east-2.compute.internal, executor 26, partition 59, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2099 on ip-172-31-4-197.us-east-2.compute.internal:37927 in memory (size: 20.3 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 33.0 in stage 118.0 (TID 2092) in 3898 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 25) (39/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2102 in memory on ip-172-31-4-197.us-east-2.compute.internal:43813 (size: 20.4 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2092 on ip-172-31-8-43.us-east-2.compute.internal:41187 in memory (size: 20.3 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2104 in memory on ip-172-31-4-197.us-east-2.compute.internal:37927 (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 60.0 in stage 118.0 (TID 2118, ip-172-31-4-197.us-east-2.compute.internal, executor 26, partition 60, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 61.0 in stage 118.0 (TID 2119, ip-172-31-4-197.us-east-2.compute.internal, executor 27, partition 61, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 42.0 in stage 118.0 (TID 2098) in 3631 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 28) (40/76)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 44.0 in stage 118.0 (TID 2105) in 3361 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 26) (41/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2105 on ip-172-31-4-197.us-east-2.compute.internal:43813 in memory (size: 22.9 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2098 on ip-172-31-8-43.us-east-2.compute.internal:40247 in memory (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2106 in memory on ip-172-31-8-43.us-east-2.compute.internal:40247 (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 56.0 in stage 118.0 (TID 2120, ip-172-31-8-43.us-east-2.compute.internal, executor 28, partition 56, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 40.0 in stage 118.0 (TID 2104) in 3475 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 27) (42/76)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 39.0 in stage 118.0 (TID 2102) in 3636 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 26) (43/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2102 on ip-172-31-4-197.us-east-2.compute.internal:43813 in memory (size: 20.4 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2104 on ip-172-31-4-197.us-east-2.compute.internal:37927 in memory (size: 20.3 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 43.0 in stage 118.0 (TID 2100) in 3689 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 28) (44/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2101 in memory on ip-172-31-4-197.us-east-2.compute.internal:43813 (size: 20.4 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 62.0 in stage 118.0 (TID 2121, ip-172-31-4-197.us-east-2.compute.internal, executor 26, partition 62, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2100 on ip-172-31-8-43.us-east-2.compute.internal:40247 in memory (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2103 in memory on ip-172-31-8-43.us-east-2.compute.internal:40247 (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 58.0 in stage 118.0 (TID 2122, ip-172-31-8-43.us-east-2.compute.internal, executor 28, partition 58, NODE_LOCAL, 8387 bytes)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 46.0 in stage 118.0 (TID 2106) in 3494 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 28) (45/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Added taskresult_2107 in memory on ip-172-31-4-197.us-east-2.compute.internal:37927 (size: 23.1 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Starting task 63.0 in stage 118.0 (TID 2123, ip-172-31-4-197.us-east-2.compute.internal, executor 27, partition 63, NODE_LOCAL, 8577 bytes)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2106 on ip-172-31-8-43.us-east-2.compute.internal:40247 in memory (size: 20.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 37.0 in stage 118.0 (TID 2101) in 3820 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 26) (46/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2101 on ip-172-31-4-197.us-east-2.compute.internal:43813 in memory (size: 20.4 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 45.0 in stage 118.0 (TID 2103) in 3758 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 28) (47/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2103 on ip-172-31-8-43.us-east-2.compute.internal:40247 in memory (size: 20.3 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:16 INFO TaskSetManager: Finished task 47.0 in stage 118.0 (TID 2107) in 3398 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 27) (48/76)\n",
      "20/10/19 00:18:16 INFO BlockManagerInfo: Removed taskresult_2107 on ip-172-31-4-197.us-east-2.compute.internal:37927 in memory (size: 23.1 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:17 INFO BlockManagerInfo: Added taskresult_2109 in memory on ip-172-31-4-197.us-east-2.compute.internal:43813 (size: 14.9 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:17 INFO TaskSetManager: Starting task 64.0 in stage 118.0 (TID 2124, ip-172-31-4-197.us-east-2.compute.internal, executor 26, partition 64, NODE_LOCAL, 8767 bytes)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Added taskresult_2108 in memory on ip-172-31-8-43.us-east-2.compute.internal:41187 (size: 15.4 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Added taskresult_2110 in memory on ip-172-31-8-43.us-east-2.compute.internal:41187 (size: 15.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Finished task 53.0 in stage 118.0 (TID 2109) in 2181 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 26) (49/76)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Starting task 65.0 in stage 118.0 (TID 2125, ip-172-31-8-43.us-east-2.compute.internal, executor 25, partition 65, NODE_LOCAL, 8767 bytes)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Starting task 66.0 in stage 118.0 (TID 2126, ip-172-31-8-43.us-east-2.compute.internal, executor 25, partition 66, NODE_LOCAL, 8767 bytes)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Added taskresult_2112 in memory on ip-172-31-8-43.us-east-2.compute.internal:41187 (size: 15.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Added taskresult_2113 in memory on ip-172-31-4-197.us-east-2.compute.internal:37927 (size: 14.9 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Starting task 68.0 in stage 118.0 (TID 2127, ip-172-31-4-197.us-east-2.compute.internal, executor 27, partition 68, NODE_LOCAL, 8767 bytes)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Starting task 67.0 in stage 118.0 (TID 2128, ip-172-31-8-43.us-east-2.compute.internal, executor 25, partition 67, NODE_LOCAL, 8767 bytes)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Removed taskresult_2109 on ip-172-31-4-197.us-east-2.compute.internal:43813 in memory (size: 14.9 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Added taskresult_2111 in memory on ip-172-31-8-43.us-east-2.compute.internal:41187 (size: 15.2 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Starting task 69.0 in stage 118.0 (TID 2129, ip-172-31-8-43.us-east-2.compute.internal, executor 25, partition 69, NODE_LOCAL, 8957 bytes)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Finished task 49.0 in stage 118.0 (TID 2110) in 2233 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 25) (50/76)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Removed taskresult_2110 on ip-172-31-8-43.us-east-2.compute.internal:41187 in memory (size: 15.3 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Finished task 54.0 in stage 118.0 (TID 2113) in 2158 ms on ip-172-31-4-197.us-east-2.compute.internal (executor 27) (51/76)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Finished task 48.0 in stage 118.0 (TID 2108) in 2324 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 25) (52/76)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Removed taskresult_2113 on ip-172-31-4-197.us-east-2.compute.internal:37927 in memory (size: 14.9 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Removed taskresult_2108 on ip-172-31-8-43.us-east-2.compute.internal:41187 in memory (size: 15.4 MB, free: 4.7 GB)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Finished task 51.0 in stage 118.0 (TID 2112) in 2238 ms on ip-172-31-8-43.us-east-2.compute.internal (executor 25) (53/76)\n",
      "20/10/19 00:18:18 INFO BlockManagerInfo: Added taskresult_2114 in memory on ip-172-31-4-197.us-east-2.compute.internal:37927 (size: 14.9 MB, free: 4.8 GB)\n",
      "20/10/19 00:18:18 INFO TaskSetManager: Starting task 70.0 in stage 118.0 (TID 2130, ip-172-31-4-197.us-east-2.compute.internal, executor 27, partition 70, NODE_LOCAL, 8957 bytes)\n"
     ]
    }
   ],
   "source": [
    "def get_most_pickups_per_month_window_pyspark(taxiPath, taxiLookupPath):\n",
    "    from pyspark.sql import Window\n",
    "    inputDF = get_monthly_totals_pyspark(taxiPath, taxiFilteredPath)\n",
    "    win = Window.partitionBy(\"borough\", \"pickup_month\").orderBy(f.desc(\"count\"))\n",
    "    firstDF = inputDF.withColumn(\"row_num\", f.row_number().over(win)).where(\"row_num == 1\")\n",
    "    firstDF = firstDF.orderBy(f.desc(\"count\"), f.desc(\"pickup_month\"))\n",
    "    firstDF.explain()\n",
    "    firstDF.show()\n",
    "    return firstDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c0f3a5b43648738f23818553dfb24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name 'timer_method' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'timer_method' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"get_most_pickups_per_month_window_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab - 2.9 Run and time the overall pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f194bf466704c7bbfc70032201f8b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reset notebook kernal\n",
    "def ingest_main():\n",
    "    ingest_taxi_data_multi_service(\"s3://nyc-tlc/trip data/yellow_tripdata_2020-01.csv\")\n",
    "    ingest_taxi_lookup(\"s3://nyc-tlc/misc/taxi _zone_lookup.csv\")\n",
    "    get_most_pickups_per_month_window_pyspark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811c0cc089f346b69b2f026d141ee3a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name 'timer_method' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'timer_method' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"ingest_main\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
