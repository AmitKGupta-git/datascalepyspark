{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0497da08ef941b59dedf6a5256ad86e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Using cached https://files.pythonhosted.org/packages/1c/6f/50be8234f5c2c95a047647bfded004f78bf0de5b46d43fbffef3f7f2e4c6/boto3-1.16.3-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.4.0,>=0.3.0 (from boto3)\n",
      "  Using cached https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3)\n",
      "Collecting botocore<1.20.0,>=1.19.3 (from boto3)\n",
      "  Using cached https://files.pythonhosted.org/packages/10/4f/0af21f061279d1fa5f6bb0939f1b2499da61947740faa7276e8039b08c56/botocore-1.19.3-py2.py3-none-any.whl\n",
      "Collecting urllib3<1.26,>=1.25.4; python_version != \"3.4\" (from botocore<1.20.0,>=1.19.3->boto3)\n",
      "  Using cached https://files.pythonhosted.org/packages/56/aa/4ef5aa67a9a62505db124a5cb5262332d1d4153462eb8fd89c9fa41e5d92/urllib3-1.25.11-py2.py3-none-any.whl\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore<1.20.0,>=1.19.3->boto3)\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.3->boto3)\n",
      "Installing collected packages: urllib3, python-dateutil, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.16.3 botocore-1.19.3 python-dateutil-2.8.1 s3transfer-0.3.3 urllib3-1.25.11\n",
      "\n",
      "Collecting pandas\n",
      "  Using cached https://files.pythonhosted.org/packages/25/47/22fc373440e144e2111363adaa07abb09ec1f03fbc071b6d9fc0bbf65f68/pandas-1.1.3-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib64/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /mnt/tmp/1603427163664-0/lib/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.1.3\n",
      "\n",
      "Collecting requests\n",
      "  Using cached https://files.pythonhosted.org/packages/45/1e/0c169c6a5381e241ba7404532c16a21d86ab872c9bed8bdcd4c423954103/requests-2.24.0-py2.py3-none-any.whl\n",
      "Collecting chardet<4,>=3.0.2 (from requests)\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\n",
      "Collecting idna<3,>=2.5 (from requests)\n",
      "  Using cached https://files.pythonhosted.org/packages/a2/38/928ddce2273eaa564f6f50de919327bf3a00f091b5baba8dfa9460f3a8a8/idna-2.10-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /mnt/tmp/1603427163664-0/lib/python3.7/site-packages (from requests)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached https://files.pythonhosted.org/packages/5e/c4/6c4fe722df5343c33226f0b4e0bb042e4dc13483228b4718baf286f86d87/certifi-2020.6.20-py2.py3-none-any.whl\n",
      "Installing collected packages: chardet, idna, certifi, requests\n",
      "Successfully installed certifi-2020.6.20 chardet-3.0.4 idna-2.10 requests-2.24.0\n",
      "\n",
      "Collecting s3fs\n",
      "  Using cached https://files.pythonhosted.org/packages/a7/58/732ea1c735d725b1cc4cf365ae6326c22569a5e88c8502d13844e91f08ef/s3fs-0.5.1-py3-none-any.whl\n",
      "Collecting aiobotocore>=1.0.1 (from s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/73/25/a81b015035012131056a6b7a339eec052f86f33e35fd91f160e961ea2a5e/aiobotocore-1.1.2-py3-none-any.whl\n",
      "Collecting fsspec>=0.8.0 (from s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/a5/8b/1df260f860f17cb08698170153ef7db672c497c1840dcc8613ce26a8a005/fsspec-0.8.4-py3-none-any.whl\n",
      "Collecting aioitertools>=0.5.1 (from aiobotocore>=1.0.1->s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/83/42/90df27c516ce54fa26964bc4a632ecaf352c7e99574b515255e48b4a7cc7/aioitertools-0.7.0-py3-none-any.whl\n",
      "Collecting botocore<1.17.45,>=1.17.44 (from aiobotocore>=1.0.1->s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/1e/6a/b6490235c01c941a24a86235e2a641e9505cf0ce4b4968d4987573d92bec/botocore-1.17.44-py2.py3-none-any.whl\n",
      "Collecting wrapt>=1.10.10 (from aiobotocore>=1.0.1->s3fs)\n",
      "Collecting aiohttp>=3.3.1 (from aiobotocore>=1.0.1->s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/12/a2/ca3ba17c50ebeb3e7473330d8d1ce08fb83506a9bc985bcc0716354d2018/aiohttp-3.6.3-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting typing_extensions>=3.7 (from aioitertools>=0.5.1->aiobotocore>=1.0.1->s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/60/7a/e881b5abb54db0e6e671ab088d079c57ce54e8a01a3ca443f561ccadb37e/typing_extensions-3.7.4.3-py3-none-any.whl\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /mnt/tmp/1603427163664-0/lib/python3.7/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /mnt/tmp/1603427163664-0/lib/python3.7/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs)\n",
      "Collecting docutils<0.16,>=0.10 (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /mnt/tmp/1603427163664-0/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "Collecting async-timeout<4.0,>=3.0 (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
      "Collecting attrs>=17.3.0 (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/14/df/479736ae1ef59842f512548bacefad1abed705e400212acba43f9b0fa556/attrs-20.2.0-py2.py3-none-any.whl\n",
      "Collecting yarl<1.6.0,>=1.0 (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/5b/1c/556b99a3a11916e05cd2128367f507dc330fc30ed1f5991e1ffe4dabf635/yarl-1.5.1-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting multidict<5.0,>=4.5 (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "  Using cached https://files.pythonhosted.org/packages/85/b8/a9fe777dab4c6aa067b516a34fe995213707e490ea1e72f823949a830a6a/multidict-4.7.6-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs)\n",
      "Requirement already satisfied: idna>=2.0 in /mnt/tmp/1603427163664-0/lib/python3.7/site-packages (from yarl<1.6.0,>=1.0->aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "Installing collected packages: typing-extensions, aioitertools, docutils, botocore, wrapt, async-timeout, attrs, multidict, yarl, aiohttp, aiobotocore, fsspec, s3fs\n",
      "  Found existing installation: botocore 1.19.3\n",
      "    Uninstalling botocore-1.19.3:\n",
      "      Successfully uninstalled botocore-1.19.3\n",
      "Successfully installed aiobotocore-1.1.2 aiohttp-3.6.3 aioitertools-0.7.0 async-timeout-3.0.1 attrs-20.2.0 botocore-1.17.44 docutils-0.15.2 fsspec-0.8.4 multidict-4.7.6 s3fs-0.5.1 typing-extensions-3.7.4.3 wrapt-1.12.1 yarl-1.5.1"
     ]
    }
   ],
   "source": [
    "# Install libraries within the notebook scope\n",
    "sc.install_pypi_package(\"boto3\")\n",
    "sc.install_pypi_package(\"pandas\")\n",
    "sc.install_pypi_package(\"requests\")\n",
    "sc.install_pypi_package(\"s3fs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04beb0b56e9e43aaafb09b30bdf42e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import boto3\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql import functions as f, types as t, Window\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import s3fs\n",
    "import subprocess\n",
    "import timeit\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Removes truncation of columns, column values in Pandas\n",
    "# by default\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "# Monkey patching the DataFrame transform method for spark 2.4\n",
    "def transform(self, f):\n",
    "    return f(self)\n",
    "DataFrame.transform = transform\n",
    "\n",
    "# Override the timeit template to return the command's\n",
    "# return value in addition to the time\n",
    "# Reference: https://stackoverflow.com/questions/24812253/how-can-i-capture-return-value-with-python-timeit-module\n",
    "timeit.template = \"\"\"\n",
    "def inner(_it, _timer{init}):\n",
    "    {setup}\n",
    "    _t0 = _timer()\n",
    "    for _i in _it:\n",
    "        retval = {stmt}\n",
    "    _t1 = _timer()\n",
    "    return _t1 - _t0, retval\n",
    "\"\"\"\n",
    "\n",
    "def shell_cmd(cmd):\n",
    "    for line in subprocess.check_output(cmd, shell=True).split(b'\\n'):\n",
    "        print(line)\n",
    "\n",
    "def timer_method(cmd):\n",
    "    # Setting globals = globals() enables the timeit function\n",
    "    # to return the value generated by cmd\n",
    "    return timeit.timeit(cmd, number=1, globals = globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set your s3 bucket name\n",
    "This should be data-scale-oreilly-{your name}   \n",
    "If you dont remember check the [S3 console](https://s3.console.aws.amazon.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b7dc4c8654493d9e39b3a4160a2a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MY_BUCKET_NAME = \"data-scale-oreilly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 3.1 - Leveraging File Types\n",
    "\n",
    "Write functions to:\n",
    "* Write out the taxi-lookup dataset to local storage as csv, json, and parquet files\n",
    "* Write a function to read the files and to print/return a dataframe containing the counts of zones per borough\n",
    "* Time how long it takes to write out the different file types and to perform the aggregation using each file type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ab061873474943a445e466c1bb669c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# These should point to the paths where the data from the taxi and taxi-lookup ingests were written\n",
    "taxiPath = \"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/\"\n",
    "taxiLookupPath = \"hdfs:///tmp/data/nyc-taxi/zone-lookup/output/section2/json/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0691039713f4e7493c2993f7ed3decb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def write_csv(inputDF):\n",
    "    (inputDF\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"header\", True)\n",
    "        .csv(\"hdfs:///tmp/data/nyc-taxi/taxi-zone/output/section3/csv\"))\n",
    "\n",
    "def write_json(inputDF):\n",
    "    (inputDF\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .json(\"hdfs:///tmp/data/nyc-taxi/taxi-zone/output/section3/json\"))\n",
    "\n",
    "def write_parquet(inputDF):\n",
    "    (inputDF\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(\"hdfs:///tmp/data/nyc-taxi/taxi-zone/output/section3/parquet\"))\n",
    "        \n",
    "def read_test_csv():\n",
    "    readDF = (spark\n",
    "                 .read\n",
    "                 .option('header', True).csv(\"hdfs:///tmp/data/nyc-taxi/taxi-zone/output/section3/csv\"))\n",
    "    readDF.show()\n",
    "    dfCount = readDF.groupBy(\"borough\").count()\n",
    "    print(dfCount)\n",
    "    return dfCount\n",
    "\n",
    "def read_test_json():\n",
    "    readDF = (spark\n",
    "                 .read\n",
    "                 .json(\"hdfs:///tmp/data/nyc-taxi/taxi-zone/output/section3/json\"))\n",
    "    readDF.show()\n",
    "    dfCount = readDF.groupBy(\"borough\").count()\n",
    "    print(dfCount)\n",
    "    return dfCount\n",
    "\n",
    "def read_test_parquet():\n",
    "    readDF = (spark\n",
    "                 .read\n",
    "                 .parquet(\"hdfs:///tmp/data/nyc-taxi/taxi-zone/output/section3/parquet\"))\n",
    "    readDF.show()\n",
    "    dfCount = readDF.groupBy(\"borough\").count()\n",
    "    print(dfCount)\n",
    "    return dfCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adb92346d21a403e8ce08fe047adfab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- LocationID: long (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n",
      "+-------------+----------+--------------------+------------+\n",
      "|      Borough|LocationID|                Zone|service_zone|\n",
      "+-------------+----------+--------------------+------------+\n",
      "|          EWR|         1|      Newark Airport|         EWR|\n",
      "|       Queens|         2|         Jamaica Bay|   Boro Zone|\n",
      "|        Bronx|         3|Allerton/Pelham G...|   Boro Zone|\n",
      "|    Manhattan|         4|       Alphabet City| Yellow Zone|\n",
      "|Staten Island|         5|       Arden Heights|   Boro Zone|\n",
      "|Staten Island|         6|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|       Queens|         7|             Astoria|   Boro Zone|\n",
      "|       Queens|         8|        Astoria Park|   Boro Zone|\n",
      "|       Queens|         9|          Auburndale|   Boro Zone|\n",
      "|       Queens|        10|        Baisley Park|   Boro Zone|\n",
      "|     Brooklyn|        11|          Bath Beach|   Boro Zone|\n",
      "|    Manhattan|        12|        Battery Park| Yellow Zone|\n",
      "|    Manhattan|        13|   Battery Park City| Yellow Zone|\n",
      "|     Brooklyn|        14|           Bay Ridge|   Boro Zone|\n",
      "|       Queens|        15|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|       Queens|        16|             Bayside|   Boro Zone|\n",
      "|     Brooklyn|        17|             Bedford|   Boro Zone|\n",
      "|        Bronx|        18|        Bedford Park|   Boro Zone|\n",
      "|       Queens|        19|           Bellerose|   Boro Zone|\n",
      "|        Bronx|        20|             Belmont|   Boro Zone|\n",
      "+-------------+----------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CSV write time: (0.5711434690001624, None)\n",
      "JSON write time: (0.2429550319993723, None)\n",
      "Parquet write time: (0.7616551030005212, None)"
     ]
    }
   ],
   "source": [
    "inputDF = (spark\n",
    "              .read\n",
    "              .json(taxiLookupPath))\n",
    "inputDF.printSchema()\n",
    "inputDF.show()\n",
    "\n",
    "print(f'CSV write time: {timer_method(\"write_csv(inputDF)\")}')\n",
    "print(f'JSON write time: {timer_method(\"write_json(inputDF)\")}')\n",
    "print(f'Parquet write time: {timer_method(\"write_parquet(inputDF)\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1441b6702f46ed9eb5638cfc5dc62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------------------+------------+\n",
      "|      Borough|LocationID|                Zone|service_zone|\n",
      "+-------------+----------+--------------------+------------+\n",
      "|          EWR|         1|      Newark Airport|         EWR|\n",
      "|       Queens|         2|         Jamaica Bay|   Boro Zone|\n",
      "|        Bronx|         3|Allerton/Pelham G...|   Boro Zone|\n",
      "|    Manhattan|         4|       Alphabet City| Yellow Zone|\n",
      "|Staten Island|         5|       Arden Heights|   Boro Zone|\n",
      "|Staten Island|         6|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|       Queens|         7|             Astoria|   Boro Zone|\n",
      "|       Queens|         8|        Astoria Park|   Boro Zone|\n",
      "|       Queens|         9|          Auburndale|   Boro Zone|\n",
      "|       Queens|        10|        Baisley Park|   Boro Zone|\n",
      "|     Brooklyn|        11|          Bath Beach|   Boro Zone|\n",
      "|    Manhattan|        12|        Battery Park| Yellow Zone|\n",
      "|    Manhattan|        13|   Battery Park City| Yellow Zone|\n",
      "|     Brooklyn|        14|           Bay Ridge|   Boro Zone|\n",
      "|       Queens|        15|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|       Queens|        16|             Bayside|   Boro Zone|\n",
      "|     Brooklyn|        17|             Bedford|   Boro Zone|\n",
      "|        Bronx|        18|        Bedford Park|   Boro Zone|\n",
      "|       Queens|        19|           Bellerose|   Boro Zone|\n",
      "|        Bronx|        20|             Belmont|   Boro Zone|\n",
      "+-------------+----------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "DataFrame[borough: string, count: bigint]\n",
      "CSV read and transform time: (0.31398385200009216, DataFrame[borough: string, count: bigint])\n",
      "+-------------+----------+--------------------+------------+\n",
      "|      Borough|LocationID|                Zone|service_zone|\n",
      "+-------------+----------+--------------------+------------+\n",
      "|          EWR|         1|      Newark Airport|         EWR|\n",
      "|       Queens|         2|         Jamaica Bay|   Boro Zone|\n",
      "|        Bronx|         3|Allerton/Pelham G...|   Boro Zone|\n",
      "|    Manhattan|         4|       Alphabet City| Yellow Zone|\n",
      "|Staten Island|         5|       Arden Heights|   Boro Zone|\n",
      "|Staten Island|         6|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|       Queens|         7|             Astoria|   Boro Zone|\n",
      "|       Queens|         8|        Astoria Park|   Boro Zone|\n",
      "|       Queens|         9|          Auburndale|   Boro Zone|\n",
      "|       Queens|        10|        Baisley Park|   Boro Zone|\n",
      "|     Brooklyn|        11|          Bath Beach|   Boro Zone|\n",
      "|    Manhattan|        12|        Battery Park| Yellow Zone|\n",
      "|    Manhattan|        13|   Battery Park City| Yellow Zone|\n",
      "|     Brooklyn|        14|           Bay Ridge|   Boro Zone|\n",
      "|       Queens|        15|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|       Queens|        16|             Bayside|   Boro Zone|\n",
      "|     Brooklyn|        17|             Bedford|   Boro Zone|\n",
      "|        Bronx|        18|        Bedford Park|   Boro Zone|\n",
      "|       Queens|        19|           Bellerose|   Boro Zone|\n",
      "|        Bronx|        20|             Belmont|   Boro Zone|\n",
      "+-------------+----------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "DataFrame[borough: string, count: bigint]\n",
      "JSON read and transform time: (0.25518597999962367, DataFrame[borough: string, count: bigint])\n",
      "+-------------+----------+--------------------+------------+\n",
      "|      Borough|LocationID|                Zone|service_zone|\n",
      "+-------------+----------+--------------------+------------+\n",
      "|          EWR|         1|      Newark Airport|         EWR|\n",
      "|       Queens|         2|         Jamaica Bay|   Boro Zone|\n",
      "|        Bronx|         3|Allerton/Pelham G...|   Boro Zone|\n",
      "|    Manhattan|         4|       Alphabet City| Yellow Zone|\n",
      "|Staten Island|         5|       Arden Heights|   Boro Zone|\n",
      "|Staten Island|         6|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|       Queens|         7|             Astoria|   Boro Zone|\n",
      "|       Queens|         8|        Astoria Park|   Boro Zone|\n",
      "|       Queens|         9|          Auburndale|   Boro Zone|\n",
      "|       Queens|        10|        Baisley Park|   Boro Zone|\n",
      "|     Brooklyn|        11|          Bath Beach|   Boro Zone|\n",
      "|    Manhattan|        12|        Battery Park| Yellow Zone|\n",
      "|    Manhattan|        13|   Battery Park City| Yellow Zone|\n",
      "|     Brooklyn|        14|           Bay Ridge|   Boro Zone|\n",
      "|       Queens|        15|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|       Queens|        16|             Bayside|   Boro Zone|\n",
      "|     Brooklyn|        17|             Bedford|   Boro Zone|\n",
      "|        Bronx|        18|        Bedford Park|   Boro Zone|\n",
      "|       Queens|        19|           Bellerose|   Boro Zone|\n",
      "|        Bronx|        20|             Belmont|   Boro Zone|\n",
      "+-------------+----------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "DataFrame[borough: string, count: bigint]\n",
      "Parquet read and transform time: (0.22099308599990763, DataFrame[borough: string, count: bigint])"
     ]
    }
   ],
   "source": [
    "print(f'CSV read and transform time: {timer_method(\"read_test_csv()\")}')\n",
    "print(f'JSON read and transform time: {timer_method(\"read_test_json()\")}')\n",
    "print(f'Parquet read and transform time: {timer_method(\"read_test_parquet()\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 3.2 Partitioning\n",
    "\n",
    "Write functions to:\n",
    "* Based on the class discussion , update the following functions with the most approrpiate partitioning values\n",
    "* Write a function to read the written files and to print/return a dataframe containing the counts of taxi rides per pickup_month\n",
    "* Time how long it takes to write out the data using the different partitioning methodologies and to perform the aggregation using strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50cb6f4ac7a4b8f9c8a7c9258aaeeb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def write_paritioned_parquet(inputDF):\n",
    "    (inputDF\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy('pickup_month')\n",
    "        .parquet(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/partitioned\"))\n",
    "            \n",
    "def write_coalesce_parquet(inputDF):\n",
    "    (inputDF\n",
    "        .coalesce(5)\n",
    "        .write     \n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy('pickup_month')\n",
    "        .parquet(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/coalesced\"))\n",
    "            \n",
    "def write_repartition_parquet(inputDF):\n",
    "    (inputDF\n",
    "        .repartition(5)\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy('pickup_month')\n",
    "        .parquet(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/repartitioned\"))\n",
    "            \n",
    "def write_sorted_parquet(inputDF): #RLE, need to verify that this still matters\n",
    "    (inputDF.orderBy('pickup_month','passenger_count', 'PULocationID', 'DOLocationID', 'trip_distance', 'fare_amount', 'tip_amount', 'tpep_dropoff_datetime', 'tpep_pickup_datetime')\n",
    "        .coalesce(5)     \n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy('pickup_month')\n",
    "        .parquet(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/sorted\"))\n",
    "\n",
    "# def write_parition(inputDF, partition):\n",
    "#     writeDF = inputDF.filter(f\"pickup_month == 'f{partition}'\")\n",
    "#     writeDF.show()\n",
    "#     (writeDF.orderBy('passenger_count', 'PULocationID', 'DOLocationID', 'trip_distance', 'fare_amount', 'tip_amount', 'tpep_dropoff_datetime', 'tpep_pickup_datetime')\n",
    "#         .repartition(5)\n",
    "#         .write\n",
    "#         .mode(\"overwrite\")\n",
    "#         .parquet(f\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/manual/pickup_month={parition}\"))\n",
    "\n",
    "# def write_pre_defined_partitions(inputDF):\n",
    "#     ##multiparll over list of partitions\n",
    "#     #need to verify if this is actually faster\n",
    "#     from multiprocessing import Process\n",
    "#     partitionList = [x[0] for x in inputDF.select(\"pickup_month\").distinct().collect()]\n",
    "#     print(parititionList)\n",
    "#     for partiion in partitionList:\n",
    "#         writeProcess = Process(target=write_parition, args=(inputDF, partiion))\n",
    "#         writeProcess.start()\n",
    "#         writeProcess.join()\n",
    "                \n",
    "#     spark.sql(\"MSCK RPAIR TABLE table_name\")\n",
    "#     spark.sql(\"REFRESH TABLE table_name\")\n",
    "\n",
    "def read_test_parquet(readPath):\n",
    "    readDF = (spark\n",
    "                 .read\n",
    "                 .parquet(readPath).sort(\"pickup_month\", \"passenger_count\"))\n",
    "    return readDF\n",
    "def agg_test_parquet(baseDF):\n",
    "    dfCount = baseDF.groupBy(\"pickup_month\").count()\n",
    "    print(dfCount)\n",
    "    return dfCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf2336498674c3b8044a0504d6a7e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+----------+--------+--------------------+--------------------+-----+-----------+---------------------+-------------------+-----+-------+---------------+------------+--------------------+-------+------------------+----------+------------+------------+---------------------+--------------------+-------------+----+------------+\n",
      "|DOLocationID|PULocationID|RatecodeID|VendorID|congestion_surcharge|    dropoff_datetime|extra|fare_amount|improvement_surcharge|        ingested_on|month|mta_tax|passenger_count|payment_type|     pickup_datetime|service|store_and_fwd_flag|tip_amount|tolls_amount|total_amount|tpep_dropoff_datetime|tpep_pickup_datetime|trip_distance|year|pickup_month|\n",
      "+------------+------------+----------+--------+--------------------+--------------------+-----+-----------+---------------------+-------------------+-----+-------+---------------+------------+--------------------+-------+------------------+----------+------------+------------+---------------------+--------------------+-------------+----+------------+\n",
      "|         239|         238|         1|       1|                 2.5|2020-01-01T00:33:...|    3|        6.0|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           1|2020-01-01T00:28:...| yellow|                 N|      1.47|           0|       11.27|  2020-01-01 00:33:03| 2020-01-01 00:28:15|          1.2|2020|      202001|\n",
      "|         238|         239|         1|       1|                 2.5|2020-01-01T00:43:...|    3|        7.0|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           1|2020-01-01T00:35:...| yellow|                 N|       1.5|           0|        12.3|  2020-01-01 00:43:04| 2020-01-01 00:35:39|          1.2|2020|      202001|\n",
      "|         238|         238|         1|       1|                 2.5|2020-01-01T00:53:...|    3|        6.0|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           1|2020-01-01T00:47:...| yellow|                 N|       1.0|           0|        10.8|  2020-01-01 00:53:52| 2020-01-01 00:47:41|          0.6|2020|      202001|\n",
      "|         151|         238|         1|       1|                   0|2020-01-01T01:00:...|  0.5|        5.5|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           1|2020-01-01T00:55:...| yellow|                 N|      1.36|           0|        8.16|  2020-01-01 01:00:14| 2020-01-01 00:55:23|          0.8|2020|      202001|\n",
      "|         193|         193|         1|       2|                   0|2020-01-01T00:04:...|  0.5|        3.5|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           2|2020-01-01T00:01:...| yellow|                 N|       0.0|           0|         4.8|  2020-01-01 00:04:16| 2020-01-01 00:01:58|          0.0|2020|      202001|\n",
      "|         193|           7|         1|       2|                   0|2020-01-01T00:10:...|  0.5|        2.5|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           2|2020-01-01T00:09:...| yellow|                 N|       0.0|           0|         3.8|  2020-01-01 00:10:37| 2020-01-01 00:09:44|         0.03|2020|      202001|\n",
      "|         193|         193|         1|       2|                   0|2020-01-01T00:39:...|  0.5|        2.5|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           1|2020-01-01T00:39:...| yellow|                 N|      0.01|           0|        3.81|  2020-01-01 00:39:29| 2020-01-01 00:39:25|          0.0|2020|      202001|\n",
      "|         193|         193|         5|       2|                 2.5|2019-12-18T15:28:...|    0|       0.01|                  0.3|2020-10-23 03:48:16|   01|      0|              1|           1|2019-12-18T15:27:...| yellow|                 N|       0.0|           0|        2.81|  2019-12-18 15:28:59| 2019-12-18 15:27:49|          0.0|2020|      201912|\n",
      "|         193|         193|         1|       2|                 2.5|2019-12-18T15:31:...|  0.5|        2.5|                  0.3|2020-10-23 03:48:16|   01|    0.5|              4|           1|2019-12-18T15:30:...| yellow|                 N|       0.0|           0|         6.3|  2019-12-18 15:31:35| 2019-12-18 15:30:35|          0.0|2020|      201912|\n",
      "|          48|         246|         1|       1|                 2.5|2020-01-01T00:40:...|    3|        8.0|                  0.3|2020-10-23 03:48:16|   01|    0.5|              2|           1|2020-01-01T00:29:...| yellow|                 N|      2.35|           0|       14.15|  2020-01-01 00:40:28| 2020-01-01 00:29:01|          0.7|2020|      202001|\n",
      "|          79|         246|         1|       1|                 2.5|2020-01-01T01:12:...|    3|       12.0|                  0.3|2020-10-23 03:48:16|   01|    0.5|              2|           1|2020-01-01T00:55:...| yellow|                 N|      1.75|           0|       17.55|  2020-01-01 01:12:03| 2020-01-01 00:55:11|          2.4|2020|      202001|\n",
      "|         161|         163|         1|       1|                 2.5|2020-01-01T00:51:...|    3|        9.5|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           2|2020-01-01T00:37:...| yellow|                 N|       0.0|           0|        13.3|  2020-01-01 00:51:41| 2020-01-01 00:37:15|          0.8|2020|      202001|\n",
      "|         144|         161|         1|       1|                 2.5|2020-01-01T01:21:...|    3|       17.0|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           1|2020-01-01T00:56:...| yellow|                 N|      4.15|           0|       24.95|  2020-01-01 01:21:44| 2020-01-01 00:56:27|          3.3|2020|      202001|\n",
      "|         239|          43|         1|       2|                 2.5|2020-01-01T00:27:...|  0.5|        6.0|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           1|2020-01-01T00:21:...| yellow|                 N|      1.96|           0|       11.76|  2020-01-01 00:27:31| 2020-01-01 00:21:54|         1.07|2020|      202001|\n",
      "|          25|         143|         1|       2|                 2.5|2020-01-01T01:15:...|  0.5|       28.5|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           1|2020-01-01T00:38:...| yellow|                 N|      4.84|           0|       37.14|  2020-01-01 01:15:21| 2020-01-01 00:38:01|         7.76|2020|      202001|\n",
      "|         234|         211|         1|       1|                 2.5|2020-01-01T00:27:...|    3|        9.0|                  0.3|2020-10-23 03:48:16|   01|    0.5|              3|           2|2020-01-01T00:15:...| yellow|                 N|       0.0|           0|        12.8|  2020-01-01 00:27:06| 2020-01-01 00:15:35|          1.6|2020|      202001|\n",
      "|          90|         234|         1|       1|                 2.5|2020-01-01T00:44:...|    3|        4.0|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           1|2020-01-01T00:41:...| yellow|                 Y|       1.0|           0|         8.8|  2020-01-01 00:44:22| 2020-01-01 00:41:20|          0.5|2020|      202001|\n",
      "|         142|         246|         1|       1|                 2.5|2020-01-01T01:13:...|    3|       11.5|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           2|2020-01-01T00:56:...| yellow|                 N|       0.0|           0|        15.3|  2020-01-01 01:13:34| 2020-01-01 00:56:38|          1.7|2020|      202001|\n",
      "|         216|         138|         1|       2|                   0|2020-01-01T00:25:...|  0.5|       24.5|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           2|2020-01-01T00:08:...| yellow|                 N|       0.0|           0|        25.8|  2020-01-01 00:25:29| 2020-01-01 00:08:21|         8.45|2020|      202001|\n",
      "|         162|         170|         1|       1|                 2.5|2020-01-01T00:27:...|    3|        3.0|                  0.3|2020-10-23 03:48:16|   01|    0.5|              1|           4|2020-01-01T00:25:...| yellow|                 N|       0.0|           0|         6.8|  2020-01-01 00:27:05| 2020-01-01 00:25:39|          0.0|2020|      202001|\n",
      "+------------+------------+----------+--------+--------------------+--------------------+-----+-----------+---------------------+-------------------+-----+-------+---------------+------------+--------------------+-------+------------------+----------+------------+------------+---------------------+--------------------+-------------+----+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "(22.651904846000434, None)\n",
      "(30.66731532600079, None)\n",
      "(26.72706687000027, None)\n",
      "(40.60118591399987, None)"
     ]
    }
   ],
   "source": [
    "# Will talk through how to access the spark ui/view progress of the running code while this operation is running\n",
    "inputDF = (spark\n",
    "              .read\n",
    "              .json(taxiPath)\n",
    "              .withColumn(\"pickup_month\", f.date_format(\"pickup_datetime\", \"yyyyMM\")))\n",
    "inputDF.show()\n",
    "\n",
    "print(timer_method(\"write_paritioned_parquet(inputDF)\"))\n",
    "print(timer_method(\"write_coalesce_parquet(inputDF)\"))\n",
    "print(timer_method(\"write_repartition_parquet(inputDF)\"))\n",
    "print(timer_method(\"write_sorted_parquet(inputDF)\"))\n",
    "# print(timer_method(\"write_pre_defined_partitions(inputDF)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "addbd0c09c62458288ab69956b0e6b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.1992224259993236, DataFrame[DOLocationID: bigint, PULocationID: bigint, RatecodeID: string, SR_Flag: string, VendorID: string, congestion_surcharge: string, dispatching_base_num: string, dropoff_datetime: string, extra: string, fare_amount: double, hvfhs_license_num: string, improvement_surcharge: string, lpep_dropoff_datetime: string, lpep_pickup_datetime: string, month: string, mta_tax: string, passenger_count: bigint, payment_type: string, pickup_datetime: string, service: string, store_and_fwd_flag: string, tip_amount: double, tolls_amount: string, total_amount: string, tpep_dropoff_datetime: string, tpep_pickup_datetime: string, trip_distance: double, trip_type: string, year: string, pickup_month: int])\n",
      "DataFrame[pickup_month: int, count: bigint]\n",
      "(0.014794507999795314, DataFrame[pickup_month: int, count: bigint])\n",
      "(0.11366183999962232, DataFrame[DOLocationID: bigint, PULocationID: bigint, RatecodeID: string, SR_Flag: string, VendorID: string, congestion_surcharge: string, dispatching_base_num: string, dropoff_datetime: string, extra: string, fare_amount: double, hvfhs_license_num: string, improvement_surcharge: string, lpep_dropoff_datetime: string, lpep_pickup_datetime: string, month: string, mta_tax: string, passenger_count: bigint, payment_type: string, pickup_datetime: string, service: string, store_and_fwd_flag: string, tip_amount: double, tolls_amount: string, total_amount: string, tpep_dropoff_datetime: string, tpep_pickup_datetime: string, trip_distance: double, trip_type: string, year: string, pickup_month: int])\n",
      "DataFrame[pickup_month: int, count: bigint]\n",
      "(0.008831649999592628, DataFrame[pickup_month: int, count: bigint])\n",
      "(0.142102648999753, DataFrame[DOLocationID: bigint, PULocationID: bigint, RatecodeID: string, SR_Flag: string, VendorID: string, congestion_surcharge: string, dispatching_base_num: string, dropoff_datetime: string, extra: string, fare_amount: double, hvfhs_license_num: string, improvement_surcharge: string, lpep_dropoff_datetime: string, lpep_pickup_datetime: string, month: string, mta_tax: string, passenger_count: bigint, payment_type: string, pickup_datetime: string, service: string, store_and_fwd_flag: string, tip_amount: double, tolls_amount: string, total_amount: string, tpep_dropoff_datetime: string, tpep_pickup_datetime: string, trip_distance: double, trip_type: string, year: string, pickup_month: int])\n",
      "DataFrame[pickup_month: int, count: bigint]\n",
      "(0.010416021999844816, DataFrame[pickup_month: int, count: bigint])\n",
      "(0.13625907900041057, DataFrame[DOLocationID: bigint, PULocationID: bigint, RatecodeID: string, SR_Flag: string, VendorID: string, congestion_surcharge: string, dispatching_base_num: string, dropoff_datetime: string, extra: string, fare_amount: double, hvfhs_license_num: string, improvement_surcharge: string, lpep_dropoff_datetime: string, lpep_pickup_datetime: string, month: string, mta_tax: string, passenger_count: bigint, payment_type: string, pickup_datetime: string, service: string, store_and_fwd_flag: string, tip_amount: double, tolls_amount: string, total_amount: string, tpep_dropoff_datetime: string, tpep_pickup_datetime: string, trip_distance: double, trip_type: string, year: string, pickup_month: int])\n",
      "DataFrame[pickup_month: int, count: bigint]\n",
      "(0.008053859000028751, DataFrame[pickup_month: int, count: bigint])"
     ]
    }
   ],
   "source": [
    "testPaths = [\n",
    "    'hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/partitioned',\n",
    "    'hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/coalesced',\n",
    "    'hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/repartitioned',\n",
    "    'hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/sorted',\n",
    "#     'hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/manual/'\n",
    "]\n",
    "\n",
    "for testPath in testPaths:\n",
    "    print(timer_method(f\"read_test_parquet('{testPath}')\"))\n",
    "    baseDF = read_test_parquet(testPath)\n",
    "    print(timer_method(\"agg_test_parquet(baseDF)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aace003c728740bdbf971d4c93e48168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29381039"
     ]
    }
   ],
   "source": [
    "# Normally want to leverage a respository\n",
    "# In scala you could apply the types to the dataframe to create a dataset. That is not possible in PySpark 2.4\n",
    "# Can leverage a SQL DDL or a spark structType. \n",
    "taxiSchema = inputDF.schema\n",
    "print(spark.read.schema(taxiSchema).parquet('hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/sorted').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756643dad894477bbd3b28099c679c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o434.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 47.0 failed 4 times, most recent failure: Lost task 0.3 in stage 47.0 (TID 515, ip-172-31-4-197.us-east-2.compute.internal, executor 10): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1603063175185_0004/container_1603063175185_0004_01_000013/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1603063175185_0004/container_1603063175185_0004_01_000013/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1603063175185_0004/container_1603063175185_0004_01_000013/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1603063175185_0004/container_1603063175185_0004_01_000013/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 730, in prepare\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1389, in verify\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1368, in verify_struct\n",
      "ValueError: Length of object (30) does not match with length of fields (27)\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:297)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:289)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:407)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
      "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3395)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2552)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2552)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2552)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2766)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1603063175185_0004/container_1603063175185_0004_01_000013/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1603063175185_0004/container_1603063175185_0004_01_000013/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1603063175185_0004/container_1603063175185_0004_01_000013/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1603063175185_0004/container_1603063175185_0004_01_000013/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 730, in prepare\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1389, in verify\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1368, in verify_struct\n",
      "ValueError: Length of object (30) does not match with length of fields (27)\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:297)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:289)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 382, in show\n",
      "    print(self._jdf.showString(n, 20, vertical))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o434.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 47.0 failed 4 times, most recent failure: Lost task 0.3 in stage 47.0 (TID 515, ip-172-31-4-197.us-east-2.compute.internal, executor 10): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1603063175185_0004/container_1603063175185_0004_01_000013/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1603063175185_0004/container_1603063175185_0004_01_000013/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1603063175185_0004/container_1603063175185_0004_01_000013/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1603063175185_0004/container_1603063175185_0004_01_000013/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 730, in prepare\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1389, in verify\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1368, in verify_struct\n",
      "ValueError: Length of object (30) does not match with length of fields (27)\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:297)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:289)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:407)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
      "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3395)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2552)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2552)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2552)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2766)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1603063175185_0004/container_1603063175185_0004_01_000013/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1603063175185_0004/container_1603063175185_0004_01_000013/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1603063175185_0004/container_1603063175185_0004_01_000013/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1603063175185_0004/container_1603063175185_0004_01_000013/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 730, in prepare\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1389, in verify\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1368, in verify_struct\n",
      "ValueError: Length of object (30) does not match with length of fields (27)\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:297)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:289)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxiSchema = t.StructType([ \\\n",
    "    t.StructField('DOLocationID',t.LongType(),True), \\\n",
    "    t.StructField('PULocationID',t.LongType(),True), \\\n",
    "    t.StructField('RatecodeID',t.StringType(),True), \\\n",
    "    t.StructField('SR_Flag',t.StringType(),True), \\\n",
    "    t.StructField('VendorID',t.StringType(),True), \\\n",
    "    t.StructField('congestion_surcharge',t.StringType(),True), \\\n",
    "    t.StructField('dispatching_base_num',t.StringType(),True), \\\n",
    "    t.StructField('dropoff_datetime',t.StringType(),True), \\\n",
    "    t.StructField('extra',t.StringType(),True), \\\n",
    "    t.StructField('fare_amount',t.DoubleType(),True), \\\n",
    "    t.StructField('hvfhs_license_num',t.StringType(),True), \\\n",
    "    t.StructField('improvement_surcharge',t.StringType(),True), \\\n",
    "    t.StructField('lpep_dropoff_datetime',t.StringType(),True), \\\n",
    "    t.StructField('lpep_pickup_datetime',t.StringType(),True), \\\n",
    "    t.StructField('mta_tax',t.StringType(),True), \\\n",
    "    t.StructField('passenger_count',t.LongType(),True), \\\n",
    "    t.StructField('payment_type',t.StringType(),True), \\\n",
    "    t.StructField('pickup_datetime',t.StringType(),True), \\\n",
    "    t.StructField('service',t.StringType(),True), \\\n",
    "    t.StructField('store_and_fwd_flag',t.StringType(),True), \\\n",
    "    t.StructField('tip_amount',t.DoubleType(),True), \\\n",
    "    t.StructField('tolls_amount',t.StringType(),True), \\\n",
    "    t.StructField('total_amount',t.StringType(),True), \\\n",
    "    t.StructField('tpep_dropoff_datetime',t.StringType(),True), \\\n",
    "    t.StructField('tpep_pickup_datetime',t.StringType(),True), \\\n",
    "    t.StructField('trip_distance',t.DoubleType(),True), \\\n",
    "#     StructField('trip_type',t.StringType(),True), \\\n",
    "    t.StructField('pickup_month',t.StringType(),True)])\n",
    "print(spark.read.schema(taxiSchema).parquet('hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/sorted').count())\n",
    "\n",
    "# stations = \"col1 STRING, col2 INT\"\n",
    "spark.createDataFrame(inputDF.rdd,schema=taxiSchema,verifySchema=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 3.3 - Case Study 3: Find the average taxi rides per zip code on the 10 worst air quality days of each month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write functions to: \n",
    "* Select the best iteration of the taxi data for this workload\n",
    "* Ingest the borough zipcode mapping and explode the zip code columns\n",
    "* Ingest air quality data from: https://www.airnowapi.org/aq/observation/zipCode/historical/?format=application/json&zipCode={zipCode}&date={date}T00-0000&distance=100&API_KEY={apiKey}\n",
    "    * https://docs.airnowapi.org/HistoricalObservationsByZip/docs\n",
    "    * Make sure you have created a account at: https://docs.airnowapi.org/account/request/\n",
    "    * Use the zip codes from the borough zipcode mapping\n",
    "    * Due to API call limitations only pull 100 days of air quality for the following zipcodes (11212, 10023, 11374, 11414) starting on 2020/06/01\n",
    "    * #### Be aware that you only have 500 requests per hour to that api endpoint\n",
    "* Find the average taxi rides per borough on the 10 worst air quality days of each month\n",
    "* Persist the resulting agg dataframe using .cache()\n",
    "* Run and time the full ingest\n",
    "\n",
    "Make sure to write the ingests with an eye on efficiency for this specific workload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d78034945dd4720bf5e96bb18df91c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_taxi_df():\n",
    "    taxiDF = spark.read.parquet(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/sorted\")\n",
    "#     taxi_lookup = spark.read.parquet(\"hdfs:///tmp/data/nyc-taxi/zone-lookup/output/section2/json/\") #expected error\n",
    "    taxi_lookup = spark.read.json(\"hdfs:///tmp/data/nyc-taxi/zone-lookup/output/section2/json/\")\n",
    "    taxi_filtered = (taxiDF\n",
    "     .filter(taxiDF.pickup_datetime.isNotNull())\n",
    "     .filter(taxiDF.dropoff_datetime.isNotNull()))\n",
    "                     \n",
    "    groupDF = taxi_filtered.join(taxi_lookup.select(\"Borough\", \"LocationID\"), taxi_filtered.PULocationID == taxi_lookup.LocationID)\n",
    "    groupDF.show()\n",
    "    return groupDF\n",
    "\n",
    "def get_zip_code_mapping_df():\n",
    "    zipReadDF = spark.read.option('header', True).csv(f\"s3://{MY_BUCKET_NAME}/data/borough-zip-mapping/ny-zip-codes.csv\")\n",
    "    returnZipDF = zipReadDF.select('Borough', 'Neighborhood', f.explode(f.split('ZIP Codes', ',')).alias('zip'))\n",
    "    returnZipDF.show()\n",
    "    return returnZipDF\n",
    "\n",
    "\n",
    "def get_air_quality_df(zipDF):\n",
    "#     zipList = [x[0] for x in zipDF.select('zip').collect()]\n",
    "    zipList = ['11212', '10023']#, '11374', '11414']\n",
    "    airQualitySchema = t.StructType([\n",
    "        t.StructField(\"AQI\", t.LongType(), True),\n",
    "        t.StructField(\"Category\", t.MapType(t.StringType(), t.LongType()), True),\n",
    "        t.StructField(\"DateObserved\", t.StringType(), True),\n",
    "        t.StructField(\"HourObserved\", t.LongType(), True),\n",
    "        t.StructField(\"Latitude\", t.DoubleType(), True),\n",
    "        t.StructField(\"LocalTimeZone\", t.StringType(), True),\n",
    "        t.StructField(\"Longitude\", t.DoubleType(), True),\n",
    "        t.StructField(\"ParameterName\", t.StringType(), True),\n",
    "        t.StructField(\"ReportingArea\", t.StringType(), True),\n",
    "        t.StructField(\"StateCode\", t.StringType(), True),\n",
    "        t.StructField(\"zip\", t.StringType(), True),\n",
    "        ])\n",
    "    \n",
    "    airDF = spark.createDataFrame([], airQualitySchema)\n",
    "\n",
    "    for zipCode in zipList:\n",
    "        for d in range(0,99):\n",
    "            startDate = datetime.strptime(\"2020-06-01\", \"%Y-%m-%d\").date()\n",
    "            dateDelta = timedelta(days=d)\n",
    "            endDate = startDate + dateDelta\n",
    "            \n",
    "            apiPath = f\"https://www.airnowapi.org/aq/observation/zipCode/historical/?format=application/json&zipCode={zipCode}&date={endDate}T00-0000&distance=100&API_KEY=8DFC7E6B-F641-41D9-95DC-9CF3B90AF038\"\n",
    "            request = requests.get(apiPath)\n",
    "            requestDF = spark.createDataFrame(request.json())\n",
    "            requestDF = requestDF.withColumn('zip', f.lit(zipCode))\n",
    "            airDF = airDF.unionAll(requestDF)\n",
    "        \n",
    "    returnAirDF = airDF.withColumn(\"categoryNumber\", f.col(\"Category.Number\"))\n",
    "    returnAirDF.show()\n",
    "    \n",
    "    return returnAirDF\n",
    "\n",
    "def calculate_hottest_days(taxiDF, airQualityDF, zipDF):\n",
    "    joinAirDF = airQualityDF.withColumn('air_day', f.date_format(\"dateObserved\", \"yyyyMMdd\")).withColumnRenamed(\"zip\", \"air_zip\")\n",
    "    \n",
    "    zipDF = zipDF.withColumnRenamed(f.col('Borough'), 'zipBorough')\n",
    "    \n",
    "    taxiZipDF = taxiDF.join(zipDF.select('zip', 'ZipBorough'), zipDF.zipBorough == taxiDF.Borough)\n",
    "    \n",
    "    joinTaxiDF = (taxiZipDF.withColumn(\"pickup_day\", f.date_format(\"pickup_datetime\", \"yyyyMMdd\"))\n",
    "                        .withColumn(\"pickup_month\", f.date_format(\"pickup_datetime\", \"yyyyMM\")))\n",
    "\n",
    "    joinCondition = [joinTaxiDF.pickup_day == joinAirDF.air_day, joinTaxiDF.zip == joinAirDF.air_zip]\n",
    "    \n",
    "    joinedAirDF = joinTaxiDF.join(joinAirDF.select('categoryNumber', 'air_day', 'air_zip'), joinCondition)\n",
    "    \n",
    "    aggDF = (joinedAirDF.groupBy('pickup_month', 'pickup_day', 'zip')\n",
    "                        .agg(f.count('pickup_day').alias('count_rides'), f.avg('categoryNumber').alias('avg_cat')))\n",
    "    \n",
    "    win = Window.partitionBy(\"zip\", \"pickup_month\").orderBy(f.desc(\"avg_cat\", 'count_rides'))\n",
    "    aggDF = aggDF.withColumn(\"row_num\", f.row_number().over(win)).where(\"row_num >= 10\")\n",
    "    \n",
    "    aggDF.show()\n",
    "    return aggDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf7a7a0172349728f63ff412e9fec4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_case_study():\n",
    "    taxiDF = get_taxi_df()\n",
    "    zipDF = get_zip_code_mapping_df()\n",
    "    airQualityDF = get_air_quality_df(zipDF)\n",
    "    airQualityDF.show()\n",
    "    aggedDF = calculate_hottest_days(taxiDF, airQualityDF, zipDF)\n",
    "    aggedDF.cache()\n",
    "    print(aggedDF.count())\n",
    "    # .cache is lazy evaluated, so we do the count to force the action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd3c4d6f6fd4301a5cb4f0da29e92b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Column is not iterable\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 44, in timer_method\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 232, in timeit\n",
      "    return Timer(stmt, setup, timer, globals).timeit(number)\n",
      "  File \"/usr/lib64/python3.7/timeit.py\", line 176, in timeit\n",
      "    timing = self.inner(it, self.timer)\n",
      "  File \"<timeit-src>\", line 6, in inner\n",
      "  File \"<stdin>\", line 6, in run_case_study\n",
      "  File \"<stdin>\", line 59, in calculate_hottest_days\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 2013, in withColumnRenamed\n",
      "    return DataFrame(self._jdf.withColumnRenamed(existing, new), self.sql_ctx)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1248, in __call__\n",
      "    args_command, temp_args = self._build_args(*args)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1212, in _build_args\n",
      "    (new_args, temp_args) = self._get_args(args)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1199, in _get_args\n",
      "    temp_arg = converter.convert(arg, self.gateway_client)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_collections.py\", line 500, in convert\n",
      "    for element in object:\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 344, in __iter__\n",
      "    raise TypeError(\"Column is not iterable\")\n",
      "TypeError: Column is not iterable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"run_case_study()\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 3.4 - Write out data out to S3 for long term storage\n",
    "\n",
    "Write functions to:\n",
    "* Write the taxi/taxi-lookup data to S3 using the most appropriate storage format/partitioning methodology\n",
    "* Write the cached dataframe from the previous usecase to S3 using the most appropriate storage format/partitioning methodology\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05632453935a49f48f9cdde9d33b36f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Normally want to avoid writing to EBS then copying to S3\n",
    "# Intermediate workloads should be written to local EBS, while finished workloads/longterm storage should be read directly from/written directly to S3\n",
    "# Luckily, EMRFS let's us write out directly to S3\n",
    "\n",
    "def write_to_s3():\n",
    "    taxiDF = (spark\n",
    "              .read\n",
    "              .json(taxiPath)\n",
    "              .withColumn(\"pickup_month\", f.date_format(\"pickup_datetime\", \"yyyyMM\")))\n",
    "    taxiLookupDF = (spark\n",
    "              .read\n",
    "              .json(taxiLookupPath)\n",
    "              .withColumn(\"pickup_month\", f.date_format(\"pickup_datetime\", \"yyyyMM\")))\n",
    "    \n",
    "    (taxiDF.orderBy('pickup_month', 'passenger_count', 'PULocationID', 'DOLocationID', 'trip_distance', 'fare_amount', 'tip_amount', 'tpep_dropoff_datetime', 'tpep_pickup_datetime')\n",
    "        .coalesce(5)     \n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy('pickup_month')\n",
    "        .parquet(f\"s3://{MY_BUCKET_NAME}/tmp/data/nyc-taxi/taxi-data/final\"))\n",
    "    \n",
    "    (taxiLookupDF.orderBy('borough', 'service_zone', 'locationID', 'zone')\n",
    "        .coalesce(1)     \n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(f\"s3://{MY_BUCKET_NAME}/tmp/data/nyc-taxi/taxi-lookup/final\"))\n",
    "    \n",
    "    # We are leveraging (dangerously) a quirk of python here and a functionality of spark that let's us persist dataframes in memory. We are doing this to workaround a limitation of the timing function and the 500 api call limit from the air quality api\n",
    "    (aggedDF.orderBy('pickup_month', 'row_num', 'zip', 'avg_cat', 'count_rides')\n",
    "        .coalesce(1)     \n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy('pickup_month')\n",
    "        .parquet(f\"s3://{MY_BUCKET_NAME}/tmp/data/nyc-taxi/taxi-agg/final\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f72936410e04c1fb9cc54158d3d778c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"cannot resolve '`pickup_datetime`' given input columns: [Borough, LocationID, Zone, service_zone];;\\n'Project [Borough#14408, LocationID#14409L, Zone#14410, service_zone#14411, date_format('pickup_datetime, yyyyMM, Some(UTC)) AS pickup_month#14416]\\n+- Relation[Borough#14408,LocationID#14409L,Zone#14410,service_zone#14411] json\\n\"\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 13, in write_to_s3\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 2000, in withColumn\n",
      "    return DataFrame(self._jdf.withColumn(colName, col._jc), self.sql_ctx)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: \"cannot resolve '`pickup_datetime`' given input columns: [Borough, LocationID, Zone, service_zone];;\\n'Project [Borough#14408, LocationID#14409L, Zone#14410, service_zone#14411, date_format('pickup_datetime, yyyyMM, Some(UTC)) AS pickup_month#14416]\\n+- Relation[Borough#14408,LocationID#14409L,Zone#14410,service_zone#14411] json\\n\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(timer_method(\"write_to_s3()\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
