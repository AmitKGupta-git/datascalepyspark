{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries within the notebook scope\n",
    "sc.install_pypi_package(\"boto3\")\n",
    "sc.install_pypi_package(\"pandas\")\n",
    "sc.install_pypi_package(\"requests\")\n",
    "sc.install_pypi_package(\"s3fs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql import functions as f, types as t\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import s3fs\n",
    "import subprocess\n",
    "import timeit\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Removes truncation of columns, column values in Pandas\n",
    "# by default\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "# Monkey patching the DataFrame transform method for spark 2.4\n",
    "def transform(self, f):\n",
    "    return f(self)\n",
    "DataFrame.transform = transform\n",
    "\n",
    "# Override the timeit template to return the command's\n",
    "# return value in addition to the time\n",
    "# Reference: https://stackoverflow.com/questions/24812253/how-can-i-capture-return-value-with-python-timeit-module\n",
    "timeit.template = \"\"\"\n",
    "def inner(_it, _timer{init}):\n",
    "    {setup}\n",
    "    _t0 = _timer()\n",
    "    for _i in _it:\n",
    "        retval = {stmt}\n",
    "    _t1 = _timer()\n",
    "    return _t1 - _t0, retval\n",
    "\"\"\"\n",
    "\n",
    "def shell_cmd(cmd):\n",
    "    for line in subprocess.check_output(cmd, shell=True).split(b'\\n'):\n",
    "        print(line)\n",
    "\n",
    "def timer_method(cmd):\n",
    "    # Setting globals = globals() enables the timeit function\n",
    "    # to return the value generated by cmd\n",
    "    return timeit.timeit(cmd, number=1, globals = globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 3.1 - Leveraging File Types\n",
    "\n",
    "Write functions to:\n",
    "* Write out the taxi-lookup dataset to local storage as csv, json, and parquet files\n",
    "* Write a function to read the files and to print/return a dataframe containing the counts of zones per borough\n",
    "* Time how long it takes to write out the different file types and to perform the aggregation using each file type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These should point to the paths where the data from the taxi and taxi-lookup ingests were written\n",
    "taxiPath = \"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/\"\n",
    "taxiLookupPath = \"hdfs:///tmp/data/nyc-taxi/zone-lookup/output/section2/json/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(inputDF):\n",
    "    (inputDF\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"header\", True)\n",
    "        .csv(\"hdfs:///tmp/data/nyc-taxi/taxi-zone/output/section3/csv\"))\n",
    "\n",
    "def write_json(inputDF):\n",
    "    (inputDF\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .json(\"hdfs:///tmp/data/nyc-taxi/taxi-zone/output/section3/json\"))\n",
    "\n",
    "def write_parquet(inputDF):\n",
    "    (inputDF\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(\"hdfs:///tmp/data/nyc-taxi/taxi-zone/output/section3/parquet\"))\n",
    "        \n",
    "def read_test_csv():\n",
    "    readDF = (spark\n",
    "                 .read\n",
    "                 .option('header', True).csv(\"hdfs:///tmp/data/nyc-taxi/taxi-zone/output/section3/csv\"))\n",
    "    readDF.show()\n",
    "    dfCount = readDF.groupBy(\"borough\").count()\n",
    "    print(dfCount)\n",
    "    return dfCount\n",
    "\n",
    "def read_test_json():\n",
    "    readDF = (spark\n",
    "                 .read\n",
    "                 .json(\"hdfs:///tmp/data/nyc-taxi/taxi-zone/output/section3/json\"))\n",
    "    readDF.show()\n",
    "    dfCount = readDF.groupBy(\"borough\").count()\n",
    "    print(dfCount)\n",
    "    return dfCount\n",
    "\n",
    "def read_test_parquet():\n",
    "    readDF = (spark\n",
    "                 .read\n",
    "                 .parquet(\"hdfs:///tmp/data/nyc-taxi/taxi-zone/output/section3/parquet\"))\n",
    "    readDF.show()\n",
    "    dfCount = readDF.groupBy(\"borough\").count()\n",
    "    print(dfCount)\n",
    "    return dfCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = (spark\n",
    "              .read\n",
    "              .json(taxiLookupPath))\n",
    "inputDF.printSchema()\n",
    "inputDF.show()\n",
    "\n",
    "print(f'CSV write time: {timer_method(\"write_csv(inputDF)\")}')\n",
    "print(f'JSON write time: {timer_method(\"write_json(inputDF)\")}')\n",
    "print(f'Parquet write time: {timer_method(\"write_parquet(inputDF)\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'CSV read and transform time: {timer_method(\"read_test_csv()\")}')\n",
    "print(f'JSON read and transform time: {timer_method(\"read_test_json()\")}')\n",
    "print(f'Parquet read and transform time: {timer_method(\"read_test_parquet()\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 3.2 Partitioning\n",
    "\n",
    "Write functions to:\n",
    "* Based on the class discussion , update the following functions with the most approrpiate partitioning values\n",
    "* Write a function to read the written files and to print/return a dataframe containing the counts of taxi rides per pickup_month\n",
    "* Time how long it takes to write out the data using the different partitioning methodologies and to perform the aggregation using strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_paritioned_parquet(inputDF):\n",
    "    (inputDF\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy('pickup_month')\n",
    "        .parquet(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/partitioned\"))\n",
    "            \n",
    "def write_coalesce_parquet(inputDF):\n",
    "    (inputDF\n",
    "        .coalesce(5)\n",
    "        .write     \n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy('pickup_month')\n",
    "        .parquet(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/coalesced\"))\n",
    "            \n",
    "def write_repartition_parquet(inputDF):\n",
    "    (inputDF\n",
    "        .repartition(5)\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy('pickup_month')\n",
    "        .parquet(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/repartitioned\"))\n",
    "            \n",
    "def write_sorted_parquet(inputDF): #RLE, need to verify that this still matters\n",
    "    (inputDF.orderBy('pickup_month','passenger_count', 'PULocationID', 'DOLocationID', 'trip_distance', 'fare_amount', 'tip_amount', 'tpep_dropoff_datetime', 'tpep_pickup_datetime')\n",
    "        .coalesce(5)     \n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy('pickup_month')\n",
    "        .parquet(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/sorted\"))\n",
    "\n",
    "# def write_parition(inputDF, partition):\n",
    "#     writeDF = inputDF.filter(f\"pickup_month == 'f{partition}'\")\n",
    "#     writeDF.show()\n",
    "#     (writeDF.orderBy('passenger_count', 'PULocationID', 'DOLocationID', 'trip_distance', 'fare_amount', 'tip_amount', 'tpep_dropoff_datetime', 'tpep_pickup_datetime')\n",
    "#         .repartition(5)\n",
    "#         .write\n",
    "#         .mode(\"overwrite\")\n",
    "#         .parquet(f\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/manual/pickup_month={parition}\"))\n",
    "\n",
    "# def write_pre_defined_partitions(inputDF):\n",
    "#     ##multiparll over list of partitions\n",
    "#     #need to verify if this is actually faster\n",
    "#     from multiprocessing import Process\n",
    "#     partitionList = [x[0] for x in inputDF.select(\"pickup_month\").distinct().collect()]\n",
    "#     print(parititionList)\n",
    "#     for partiion in partitionList:\n",
    "#         writeProcess = Process(target=write_parition, args=(inputDF, partiion))\n",
    "#         writeProcess.start()\n",
    "#         writeProcess.join()\n",
    "                \n",
    "#     spark.sql(\"MSCK RPAIR TABLE table_name\")\n",
    "#     spark.sql(\"REFRESH TABLE table_name\")\n",
    "\n",
    "def read_test_parquet(readPath):\n",
    "    readDF = (spark\n",
    "                 .read\n",
    "                 .parquet(readPath).sort(\"pickup_month\", \"passenger_count\"))\n",
    "    return readDF\n",
    "def agg_test_parquet(baseDF):\n",
    "    dfCount = baseDF.groupBy(\"pickup_month\").count()\n",
    "    print(dfCount)\n",
    "    return dfCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will talk through how to access the spark ui/view progress of the running code while this operation is running\n",
    "inputDF = (spark\n",
    "              .read\n",
    "              .json(taxiPath)\n",
    "              .withColumn(\"pickup_month\", f.date_format(\"pickup_datetime\", \"yyyyMM\")))\n",
    "inputDF.show()\n",
    "\n",
    "print(timer_method(\"write_paritioned_parquet(inputDF)\"))\n",
    "print(timer_method(\"write_coalesce_parquet(inputDF)\"))\n",
    "print(timer_method(\"write_repartition_parquet(inputDF)\"))\n",
    "print(timer_method(\"write_sorted_parquet(inputDF)\"))\n",
    "# print(timer_method(\"write_pre_defined_partitions(inputDF)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPaths = [\n",
    "    'hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/partitioned',\n",
    "    'hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/coalesced',\n",
    "    'hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/repartitioned',\n",
    "    'hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/sorted',\n",
    "#     'hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/manual/'\n",
    "]\n",
    "\n",
    "for testPath in testPaths:\n",
    "    print(timer_method(f\"read_test_parquet('{testPath}')\"))\n",
    "    baseDF = read_test_parquet(testPath)\n",
    "    print(timer_method(\"agg_test_parquet(baseDF)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally want to leverage a respository\n",
    "# In scala you could apply the types to the dataframe to create a dataset. That is not possible in PySpark 2.4\n",
    "# Can leverage a SQL DDL or a spark structType. \n",
    "taxiSchema = inputDF.schema\n",
    "print(spark.read.schema(taxiSchema).parquet('hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/sorted').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxiSchema = t.StructType([ \\\n",
    "    t.StructField('DOLocationID',t.LongType(),True), \\\n",
    "    t.StructField('PULocationID',t.LongType(),True), \\\n",
    "    t.StructField('RatecodeID',t.StringType(),True), \\\n",
    "    t.StructField('SR_Flag',t.StringType(),True), \\\n",
    "    t.StructField('VendorID',t.StringType(),True), \\\n",
    "    t.StructField('congestion_surcharge',t.StringType(),True), \\\n",
    "    t.StructField('dispatching_base_num',t.StringType(),True), \\\n",
    "    t.StructField('dropoff_datetime',t.StringType(),True), \\\n",
    "    t.StructField('extra',t.StringType(),True), \\\n",
    "    t.StructField('fare_amount',t.DoubleType(),True), \\\n",
    "    t.StructField('hvfhs_license_num',t.StringType(),True), \\\n",
    "    t.StructField('improvement_surcharge',t.StringType(),True), \\\n",
    "    t.StructField('lpep_dropoff_datetime',t.StringType(),True), \\\n",
    "    t.StructField('lpep_pickup_datetime',t.StringType(),True), \\\n",
    "    t.StructField('mta_tax',t.StringType(),True), \\\n",
    "    t.StructField('passenger_count',t.LongType(),True), \\\n",
    "    t.StructField('payment_type',t.StringType(),True), \\\n",
    "    t.StructField('pickup_datetime',t.StringType(),True), \\\n",
    "    t.StructField('service',t.StringType(),True), \\\n",
    "    t.StructField('store_and_fwd_flag',t.StringType(),True), \\\n",
    "    t.StructField('tip_amount',t.DoubleType(),True), \\\n",
    "    t.StructField('tolls_amount',t.StringType(),True), \\\n",
    "    t.StructField('total_amount',t.StringType(),True), \\\n",
    "    t.StructField('tpep_dropoff_datetime',t.StringType(),True), \\\n",
    "    t.StructField('tpep_pickup_datetime',t.StringType(),True), \\\n",
    "    t.StructField('trip_distance',t.DoubleType(),True), \\\n",
    "#     StructField('trip_type',t.StringType(),True), \\\n",
    "    t.StructField('pickup_month',t.StringType(),True)])\n",
    "print(spark.read.schema(taxiSchema).parquet('hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/sorted').count())\n",
    "\n",
    "# stations = \"col1 STRING, col2 INT\"\n",
    "spark.createDataFrame(inputDF.rdd,schema=taxiSchema,verifySchema=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 3.3 - Case Study 3: Find the average taxi rides per zip code on the 10 worst air quality days of each month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write functions to: \n",
    "* Select the best iteration of the taxi data for this workload\n",
    "* Ingest the borough zipcode mapping and explode the zip code columns\n",
    "* Ingest air quality data from: https://www.airnowapi.org/aq/observation/zipCode/historical/?format=application/json&zipCode={zipCode}&date={date}T00-0000&distance=100&API_KEY={apiKey}\n",
    "    * https://docs.airnowapi.org/HistoricalObservationsByZip/docs\n",
    "    * Make sure you have created a account at: https://docs.airnowapi.org/account/request/\n",
    "    * Use the zip codes from the borough zipcode mapping\n",
    "    * Due to API call limitations only pull 100 days of air quality for the following zipcodes (11212, 10023, 11374, 11414) starting on 2020/06/01\n",
    "    * #### Be aware that you only have 500 requests per hour to that api endpoint\n",
    "* Find the average taxi rides per borough on the 10 worst air quality days of each month\n",
    "* Persist the resulting agg dataframe using .cache()\n",
    "* Run and time the full ingest\n",
    "\n",
    "Make sure to write the ingests with an eye on efficiency for this specific workload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_df():\n",
    "    taxiDF = spark.read.parquet(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section3/sorted\")\n",
    "#     taxi_lookup = spark.read.parquet(\"hdfs:///tmp/data/nyc-taxi/zone-lookup/output/section2/json/\") #expected error\n",
    "    taxi_lookup = spark.read.json(\"hdfs:///tmp/data/nyc-taxi/zone-lookup/output/section2/json/\")\n",
    "    taxi_filtered = (taxiDF\n",
    "     .filter(taxiDF.pickup_datetime.isNotNull())\n",
    "     .filter(taxiDF.dropoff_datetime.isNotNull()))\n",
    "                     \n",
    "    groupDF = taxi_filtered.join(taxi_lookup.select(\"Borough\", \"LocationID\"), taxi_filtered.PULocationID == taxi_lookup.LocationID)\n",
    "    groupDF.show()\n",
    "    return groupDF\n",
    "\n",
    "def get_zip_code_mapping_df():\n",
    "    zipReadDF = spark.read.option('header', True).csv(\"s3://data-scale-oreilly/data/borough-zip-mapping/ny-zip-codes.csv\")\n",
    "    returnZipDF = zipReadDF.select('Borough', 'Neighborhood', f.explode(f.split('ZIP Codes', ',')).alias('zip'))\n",
    "    returnZipDF.show()\n",
    "    return returnZipDF\n",
    "\n",
    "\n",
    "def get_air_quality_df(zipDF):\n",
    "#     zipList = [x[0] for x in zipDF.select('zip').collect()]\n",
    "    zipList = ['11212', '10023']#, '11374', '11414']\n",
    "    airQualitySchema = t.StructType([\n",
    "        t.StructField(\"AQI\", t.LongType(), True),\n",
    "        t.StructField(\"Category\", t.MapType(t.StringType(), t.LongType()), True),\n",
    "        t.StructField(\"DateObserved\", t.StringType(), True),\n",
    "        t.StructField(\"HourObserved\", t.LongType(), True),\n",
    "        t.StructField(\"Latitude\", t.DoubleType(), True),\n",
    "        t.StructField(\"LocalTimeZone\", t.StringType(), True),\n",
    "        t.StructField(\"Longitude\", t.DoubleType(), True),\n",
    "        t.StructField(\"ParameterName\", t.StringType(), True),\n",
    "        t.StructField(\"ReportingArea\", t.StringType(), True),\n",
    "        t.StructField(\"StateCode\", t.StringType(), True),\n",
    "        t.StructField(\"zip\", t.StringType(), True),\n",
    "        ])\n",
    "    \n",
    "    airDF = spark.createDataFrame([], airQualitySchema)\n",
    "\n",
    "    for zipCode in zipList:\n",
    "        for d in range(0,99):\n",
    "            startDate = datetime.strptime(\"2020-06-01\", \"%Y-%m-%d\").date()\n",
    "            dateDelta = timedelta(days=d)\n",
    "            endDate = startDate + dateDelta\n",
    "            \n",
    "            apiPath = f\"https://www.airnowapi.org/aq/observation/zipCode/historical/?format=application/json&zipCode={zipCode}&date={endDate}T00-0000&distance=100&API_KEY=8DFC7E6B-F641-41D9-95DC-9CF3B90AF038\"\n",
    "            request = requests.get(apiPath)\n",
    "            requestDF = spark.createDataFrame(request.json())\n",
    "            requestDF = requestDF.withColumn('zip', f.lit(zipCode))\n",
    "            airDF = airDF.unionAll(requestDF)\n",
    "        \n",
    "    returnAirDF = airDF.withColumn(\"categoryNumber\", f.col(\"Category.Number\"))\n",
    "    returnAirDF.show()\n",
    "    \n",
    "    return returnAirDF\n",
    "\n",
    "def calculate_hottest_days(taxiDF, airQualityDF, zipDF):\n",
    "    joinAirDF = airQualityDF.withColumn('air_day', f.date_format(\"dateObserved\", \"yyyyMMdd\")).withColumnRenamed(\"zip\", \"air_zip\")\n",
    "    \n",
    "    zipDF = zipDF.withColumn('zipBorough', f.col('Borough'))\n",
    "    \n",
    "    taxiZipDF = taxiDF.join(zipDF.select('zip', 'ZipBorough'), zipDF.zipBorough == taxiDF.Borough)\n",
    "    \n",
    "    joinTaxiDF = (taxiZipDF.withColumn(\"pickup_day\", f.date_format(\"pickup_datetime\", \"yyyyMMdd\"))\n",
    "                        .withColumn(\"pickup_month\", f.date_format(\"pickup_datetime\", \"yyyyMM\")))\n",
    "\n",
    "    joinCondition = [joinTaxiDF.pickup_day == joinAirDF.air_day, joinTaxiDF.zip == joinAirDF.air_zip]\n",
    "    \n",
    "    joinedAirDF = joinTaxiDF.join(joinAirDF.select('categoryNumber', 'air_day', 'air_zip'), joinCondition)\n",
    "    \n",
    "    aggDF = (joinedAirDF.groupBy('pickup_month', 'pickup_day', 'zip')\n",
    "                        .agg(f.count('pickup_day').alias('count_rides'), f.avg('categoryNumber').alias('avg_cat')))\n",
    "    \n",
    "    win = Window.partitionBy(\"zip\", \"pickup_month\").orderBy(f.desc(\"avg_cat\", 'count_rides'))\n",
    "    aggDF = aggDF.withColumn(\"row_num\", f.row_number().over(win)).where(\"row_num >= 10\")\n",
    "    \n",
    "    aggDF.show()\n",
    "    return aggDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_case_study():\n",
    "    taxiDF = get_taxi_df()\n",
    "    zipDF = get_zip_code_mapping_df()\n",
    "    airQualityDF = get_air_quality_df(zipDF)\n",
    "    airQualityDF.show()\n",
    "    aggedDF = calculate_hottest_days(taxiDF, airQualityDF, zipDF)\n",
    "    aggedDF.cache()\n",
    "    print(aggedDF.count())\n",
    "    # .cache is lazy evaluated, so we do the count to force the action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timer_method(\"run_case_study()\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 3.4 - Write out data out to S3 for long term storage\n",
    "\n",
    "Write functions to:\n",
    "* Write the taxi/taxi-lookup data to S3 using the most appropriate storage format/partitioning methodology\n",
    "* Write the cached dataframe from the previous usecase to S3 using the most appropriate storage format/partitioning methodology\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally want to avoid writing to EBS then copying to S3\n",
    "# Intermediate workloads should be written to local EBS, while finished workloads/longterm storage should be read directly from/written directly to S3\n",
    "# Luckily, EMRFS let's us write out directly to S3\n",
    "\n",
    "def write_to_s3():\n",
    "    taxiDF = (spark\n",
    "              .read\n",
    "              .json(taxiPath)\n",
    "              .withColumn(\"pickup_month\", f.date_format(\"pickup_datetime\", \"yyyyMM\")))\n",
    "    taxiLookupDF = (spark\n",
    "              .read\n",
    "              .json(taxiLookupPath)\n",
    "              .withColumn(\"pickup_month\", f.date_format(\"pickup_datetime\", \"yyyyMM\")))\n",
    "    \n",
    "    (taxiDF.orderBy('pickup_month', 'passenger_count', 'PULocationID', 'DOLocationID', 'trip_distance', 'fare_amount', 'tip_amount', 'tpep_dropoff_datetime', 'tpep_pickup_datetime')\n",
    "        .coalesce(5)     \n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy('pickup_month')\n",
    "        .parquet(\"s3://tmp/data/nyc-taxi/taxi-data/final\"))\n",
    "    \n",
    "    (taxiLookupDF.orderBy('borough', 'service_zone', 'locationID', 'zone')\n",
    "        .coalesce(1)     \n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(\"s3://tmp/data/nyc-taxi/taxi-lookup/final\"))\n",
    "    \n",
    "    # We are leveraging (dangerously) a quirk of python here and a functionality of spark that let's us persist dataframes in memory. We are doing this to workaround a limitation of the timing function and the 500 api call limit from the air quality api\n",
    "    (aggedDF.orderBy('pickup_month', 'row_num', 'zip', 'avg_cat', 'count_rides')\n",
    "        .coalesce(1)     \n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy('pickup_month')\n",
    "        .parquet(\"s3://tmp/data/nyc-taxi/taxi-agg/final\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
