{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2697f5787b4355b16bfb4670bcfd8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Using cached https://files.pythonhosted.org/packages/cc/a8/b5037dc144e458b3574c085d891b85ab2035b63ab946b5c91c23f2dfc1c6/boto3-1.16.4-py2.py3-none-any.whl\n",
      "Collecting botocore<1.20.0,>=1.19.4 (from boto3)\n",
      "  Using cached https://files.pythonhosted.org/packages/2b/55/9347e51769db0fe3487ed2ae5f438b3cc6aa2916e5e9d05e60a04855373e/botocore-1.19.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0 (from boto3)\n",
      "  Using cached https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3<1.26,>=1.25.4; python_version != \"3.4\" in /usr/local/lib/python3.7/site-packages (from botocore<1.20.0,>=1.19.4->boto3)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore<1.20.0,>=1.19.4->boto3)\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.4->boto3)\n",
      "Installing collected packages: python-dateutil, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.16.4 botocore-1.19.4 python-dateutil-2.8.1 s3transfer-0.3.3\n",
      "\n",
      "Collecting pandas\n",
      "  Using cached https://files.pythonhosted.org/packages/25/47/22fc373440e144e2111363adaa07abb09ec1f03fbc071b6d9fc0bbf65f68/pandas-1.1.3-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib64/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /mnt/tmp/1603670357090-0/lib/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.1.3\n",
      "\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests)\n",
      "\n",
      "Collecting fsspec\n",
      "  Using cached https://files.pythonhosted.org/packages/a5/8b/1df260f860f17cb08698170153ef7db672c497c1840dcc8613ce26a8a005/fsspec-0.8.4-py3-none-any.whl\n",
      "Installing collected packages: fsspec\n",
      "Successfully installed fsspec-0.8.4"
     ]
    }
   ],
   "source": [
    "# Install libraries within the notebook scope\n",
    "sc.install_pypi_package(\"boto3\")\n",
    "sc.install_pypi_package(\"pandas\")\n",
    "sc.install_pypi_package(\"requests\")\n",
    "#sc.install_pypi_package(\"s3fs\")\n",
    "sc.install_pypi_package(\"fsspec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5509e60570544fbf86baf0f14efcc5d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import boto3\n",
    "from datetime import datetime\n",
    "import fsspec\n",
    "import pandas as pd\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql import functions as f, types as t, Window\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "# import s3fs\n",
    "import subprocess\n",
    "import timeit\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Removes truncation of columns, column values in Pandas\n",
    "# by default\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "# Monkey patching the DataFrame transform method for Spark 2.4\n",
    "# This is available by default in Spark 3.0\n",
    "def transform(self, f):\n",
    "    return f(self)\n",
    "DataFrame.transform = transform\n",
    "\n",
    "# Override the timeit template to return the command's\n",
    "# return value in addition to the time\n",
    "# Reference: https://stackoverflow.com/questions/24812253/how-can-i-capture-return-value-with-python-timeit-module\n",
    "timeit.template = \"\"\"\n",
    "def inner(_it, _timer{init}):\n",
    "    {setup}\n",
    "    _t0 = _timer()\n",
    "    for _i in _it:\n",
    "        retval = {stmt}\n",
    "    _t1 = _timer()\n",
    "    return _t1 - _t0, retval\n",
    "\"\"\"\n",
    "\n",
    "def shell_cmd(cmd):\n",
    "    \"\"\"\n",
    "    Wrapper for running shell commands and printing the output\n",
    "    Some helpful recipes:\n",
    "    - List files on hdfs: shell_cmd(\"hdfs dfs -ls hdfs:///tmp/data/\")\n",
    "    - Remove files from hdfs: shell_cmd(\"hdfs dfs -rm -r hdfs:///tmp/data/test_pyspark\")\n",
    "    \"\"\"\n",
    "    for line in subprocess.check_output(cmd, shell=True).split(b'\\n'):\n",
    "        print(line)\n",
    "\n",
    "def timer_method(cmd):\n",
    "    \"\"\"\n",
    "    Wrapper for timeit that returns the value of a function and its runtime\n",
    "    To use, pass a string of the function you wish to time\n",
    "    Example: \n",
    "     run_time, result = timer_method(\"myfunction(arg1, arg2)\")\n",
    "    \"\"\"\n",
    "    # Setting globals = globals() enables the timeit function\n",
    "    # to return the value generated by cmd\n",
    "    return timeit.timeit(cmd, number=1, globals = globals())\n",
    "\n",
    "MY_BUCKET_NAME = \"data-scale-oreilly\"\n",
    "taxi_data_path = \"s3://nyc-tlc/trip data/yellow_tripdata_2020-01.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying transformations to dataframes\n",
    "Pyspark vs Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06b2f795bbf4a10afde44ba20d57986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform_taxi_ps(ps_df_taxi):\n",
    "    res = (ps_df_taxi\n",
    "            .select(*column_subset)\n",
    "            .withColumn(\"tpep_pickup_datetime\", f.col(\"tpep_pickup_datetime\").cast(t.TimestampType()))\n",
    "            .withColumn(\"tpep_dropoff_datetime\", f.col(\"tpep_dropoff_datetime\").cast(t.TimestampType()))\n",
    "            .withColumn(\"passenger_count\", f.col(\"passenger_count\").cast(t.IntegerType()))\n",
    "            .withColumn(\"trip_distance\", f.col(\"trip_distance\").cast(t.FloatType()))\n",
    "            .withColumn(\"PULocationID\", f.col(\"PULocationID\").cast(t.IntegerType()))\n",
    "            .withColumn(\"DOLocationID\", f.col(\"DOLocationID\").cast(t.IntegerType()))\n",
    "            .withColumn(\"fare_amount\", f.col(\"fare_amount\").cast(t.FloatType()))\n",
    "            .withColumn(\"tip_amount\", f.col(\"tip_amount\").cast(t.FloatType()))             \n",
    "           )\n",
    "    res.count()  # Run this to force pyspark to collect the data\n",
    "    return res\n",
    "\n",
    "def transform_taxi_pd(pd_df_taxi):\n",
    "    res = pd_df_taxi[[*column_subset]].copy()\n",
    "    res.tpep_pickup_datetime = pd.to_datetime(res.tpep_pickup_datetime)\n",
    "    res.tpep_dropoff_datetime = pd.to_datetime(res.tpep_dropoff_datetime)\n",
    "    res.passenger_count = pd.to_numeric(res.passenger_count, errors='coerce').astype('Int64')\n",
    "    res.trip_distance = pd.to_numeric(res.trip_distance, errors='coerce')\n",
    "    res.PULocationID = pd.to_numeric(res.PULocationID, errors='coerce').astype('Int64')\n",
    "    res.DOLocationID = pd.to_numeric(res.DOLocationID, errors='coerce').astype('Int64')\n",
    "    res.fare_amount = pd.to_numeric(res.fare_amount, errors='coerce')\n",
    "    res.tip_amount = pd.to_numeric(res.tip_amount, errors='coerce')\n",
    "    res.count()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c24481473c543078c8d22521886ccb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pd_df_taxi = pd.read_csv(taxi_data_path, keep_default_na=False)\n",
    "ps_df_taxi = spark.read.option('header', True).option('inferSchema', True).csv(taxi_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather data ingestion\n",
    "\n",
    "Example of writing ingest for fixed-width data \n",
    "\n",
    "NOAA GHCND dataset  \n",
    "https://docs.opendata.aws/noaa-ghcn-pds/readme.html  \n",
    "\n",
    "Scroll down to 'FORMAT OF “ghcnd-stations.txt” file' for the schema of the fixed-width stations data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28242fac22ed4ac4b8cacaeb225013ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ghcnd_stations_path = \"https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt\"\n",
    "stations_s3 = f\"s3://{MY_BUCKET_NAME}/data/ghcnd/stations/input/ghcnd_stations.txt\"\n",
    "stations_local = \"hdfs:///tmp/data/ghcnd/stations/input/ghcnd-stations.txt\"\n",
    "stations_output = \"hdfs:///tmp/data/ghcnd/stations/output/ghcnd-stations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0286b7b378b436e9fa92bfac9a05343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Workaround reading HTTPS -> HDFS, HTTPS -> S3 -> HDFS\n",
    "# Spark cant read data directly from HTTP, so copy the file to S3 and read into a dataframe from there\n",
    "# Then save the file to HDFS for further processing\n",
    "ingest_timestamp = datetime.strftime(datetime.now(), \"%Y-%m-%d %H:%M:%S%z\")\n",
    "resp = requests.get(ghcnd_stations_path)\n",
    "if resp.status_code != 200:\n",
    "    print(\"Couldn't get station data\")\n",
    "else:\n",
    "    s3 = boto3.client('s3')\n",
    "    res = s3.put_object(Body=resp.content, Bucket=MY_BUCKET_NAME, Key=f\"data/ghcnd/stations/input/ghcnd_stations.txt\")\n",
    "    if res['ResponseMetadata']['HTTPStatusCode'] != 200:\n",
    "        print(f\"Unable to create ghcnd_stations.txt in s3, response {res['ResponseMetadata']['HTTPStatusCode']}\")\n",
    "    else:\n",
    "        (spark\n",
    "         .read\n",
    "         .text(stations_s3)\n",
    "         .write\n",
    "         .format(\"text\")\n",
    "         .mode(\"overwrite\")\n",
    "         .save(stations_local))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120e9156974e41cca5a03af6fa12c810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------+\n",
      "|value                                                                                |\n",
      "+-------------------------------------------------------------------------------------+\n",
      "|ACW00011604  17.1167  -61.7833   10.1    ST JOHNS COOLIDGE FLD                       |\n",
      "|ACW00011647  17.1333  -61.7833   19.2    ST JOHNS                                    |\n",
      "|AE000041196  25.3330   55.5170   34.0    SHARJAH INTER. AIRP            GSN     41196|\n",
      "|AEM00041194  25.2550   55.3640   10.4    DUBAI INTL                             41194|\n",
      "|AEM00041217  24.4330   54.6510   26.8    ABU DHABI INTL                         41217|\n",
      "|AEM00041218  24.2620   55.6090  264.9    AL AIN INTL                            41218|\n",
      "|AF000040930  35.3170   69.0170 3366.0    NORTH-SALANG                   GSN     40930|\n",
      "|AFM00040938  34.2100   62.2280  977.2    HERAT                                  40938|\n",
      "|AFM00040948  34.5660   69.2120 1791.3    KABUL INTL                             40948|\n",
      "|AFM00040990  31.5000   65.8500 1010.0    KANDAHAR AIRPORT                       40990|\n",
      "+-------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "# Take a look at the stations file we just saved to HDFS\n",
    "stations = spark.read.text(stations_local)\n",
    "stations.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934d4577ad8c46899fb21aacebb348e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|id         |\n",
      "+-----------+\n",
      "|ACW00011604|\n",
      "|ACW00011647|\n",
      "|AE000041196|\n",
      "|AEM00041194|\n",
      "|AEM00041217|\n",
      "|AEM00041218|\n",
      "|AF000040930|\n",
      "|AFM00040938|\n",
      "|AFM00040948|\n",
      "|AFM00040990|\n",
      "+-----------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "# Example of doing a substring transformation\n",
    "(stations\n",
    "    .withColumn(\"id\", f.col(\"value\").substr(0, 11))\n",
    "    .drop(\"value\")\n",
    ").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab - Build the ingestion for the weather stations data\n",
    "\n",
    "Reference the fixed width schema provided under **FORMAT OF “ghcnd-stations.txt” file**   \n",
    "https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt  \n",
    "#### Important note - FORMAT OF is incorrect for: elevation - should be 7 char not 6\n",
    "\n",
    "Create transformation code to convert the value column into the following schema:\n",
    "* id String\n",
    "* lat Float\n",
    "* long Float\n",
    "* elevation Float\n",
    "* state String\n",
    "* name String\n",
    "\n",
    "Drop the value column, save the data in JSON format to s3://data-scale-oreilly/data/ghcnd/stations/output/section2_json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a6d963acf2418aa27d27e34c10a790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform_stations_ps(stations_local):\n",
    "    return (spark.read.text(stations_local)\n",
    "     .withColumn(\"id\", f.col(\"value\").substr(0, 11))\n",
    "     .withColumn(\"lat\", f.col(\"value\").substr(13, 8))\n",
    "     .withColumn(\"long\", f.col(\"value\").substr(22, 9))\n",
    "     .withColumn(\"elevation\", f.col(\"value\").substr(32, 7))\n",
    "     .withColumn(\"state\", f.col(\"value\").substr(39, 2))\n",
    "     .withColumn(\"name\", f.col(\"value\").substr(42, 30))\n",
    "     .drop(\"value\")\n",
    "    )\n",
    "\n",
    "def transform_stations_pd(stations_s3):\n",
    "    return pd.read_fwf(stations_s3, \n",
    "                     [(0,10), (13, 20), (22, 30), (32, 38), (39, 40), (41, 71)],\n",
    "                    names=[\"id\", \"lat\", \"long\", \"elevation\", \"state\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0afb9c7285425b89898d9a9acf3972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ps_run_time, stations_df = timer_method(\"transform_stations_ps(stations_local)\")\n",
    "# pd_run_time, pd_stations_df = timer_method(\"transform_stations_pd(stations_s3)\")\n",
    "# print(f\"pyspark runtime: {ps_run_time} pandas runtime {pd_run_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A key aspect of designing scalable systems is to be judicious about the data being stored and processed. \n",
    "#### The GHCND stations file contains data on stations across the US, but we are only interested in data near NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea219163be74653a0bc264bdf98225b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115084"
     ]
    }
   ],
   "source": [
    "stations_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3554dfbabf364cd39e3a0b09382a96de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets look at some performance tradeoffs between pyspark and pandas\n",
    "# The .toPandas() function in pyspark can be convenient if you are familiar with pandas manipulations\n",
    "# but this can quickly become very expensive as it collects all data on the driver to do the conversion.\n",
    "\n",
    "def filter_ny_stations_pandas(stations_df):\n",
    "    ny_stations = stations_df.filter(\"state == 'NY'\")\n",
    "\n",
    "    # filter down to just stations in NY in NYC. Lat of south Yonkers ~40.9124\n",
    "    ny_pandas = ny_stations.toPandas()\n",
    "    ny_pandas[ny_pandas.columns] = ny_pandas.apply(lambda x: x.str.strip())\n",
    "    nyc_stations = ny_pandas[ny_pandas['lat'].apply(lambda x: float(x)) < 40.9124]\n",
    "    res = spark.createDataFrame(nyc_stations)\n",
    "    res.count()\n",
    "    return res\n",
    "\n",
    "def filter_ny_stations_pyspark(stations_df):\n",
    "    print(\"Filtering stations to NY only\")\n",
    "    ny_stations = stations_df.filter(\"state == 'NY'\")\n",
    "    res = (ny_stations\n",
    "            .withColumn(\"lat\", f.col(\"lat\").cast(t.FloatType()))\n",
    "            .filter(\"lat < 40.9124\"))\n",
    "    res.count()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e963048d91405287160646be331802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas runtime 10.893973524000103\n",
      "Filtering stations to NY only\n",
      "pyspark runtime 0.40042098499998247"
     ]
    }
   ],
   "source": [
    "pd_run_time, pd_ny_stations = timer_method(\"filter_ny_stations_pandas(stations_df)\")\n",
    "print(f\"pandas runtime {pd_run_time}\")\n",
    "ps_run_time, ps_ny_stations = timer_method(\"filter_ny_stations_pyspark(stations_df)\")\n",
    "print(f\"pyspark runtime {ps_run_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab - Write two ingest functions for the stations data, one using filter_ny_stations_pandas and the other using filter_ny_stations_pyspark. What do you notice about the differences?\n",
    "\n",
    "The functions should:\n",
    "* Read the station data from local or s3\n",
    "* Transform the station data into columns from the fixed width format\n",
    "* Use the above filter functions\n",
    "* Write the output to stations_output as json in overwrite mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_station(stations):\n",
    "    return (stations\n",
    "     .withColumn(\"id\", f.col(\"value\").substr(0, 11))\n",
    "     .withColumn(\"lat\", f.col(\"value\").substr(13, 8))\n",
    "     .withColumn(\"long\", f.col(\"value\").substr(22, 9))\n",
    "     .withColumn(\"elevation\", f.col(\"value\").substr(32, 7))\n",
    "     .withColumn(\"state\", f.col(\"value\").substr(39, 2))\n",
    "     .withColumn(\"name\", f.col(\"value\").substr(42, 30))\n",
    "     .drop(\"value\")\n",
    "    )\n",
    "\n",
    "def ingest_station_pandas():\n",
    "    stations = spark.read.text(stations_local)\n",
    "    (stations.transform(transform_station)\n",
    "     .transform(filter_ny_stations_pandas)\n",
    "     .coalesce(1)\n",
    "     .write\n",
    "     .mode(\"overwrite\")\n",
    "     .json(stations_output)\n",
    "    )\n",
    "    \n",
    "def ingest_station_pyspark():\n",
    "    stations = spark.read.text(stations_s3)\n",
    "    (stations.transform(transform_station)\n",
    "     .transform(filter_ny_stations_pyspark)\n",
    "     .coalesce(1)\n",
    "     .write\n",
    "     .mode(\"overwrite\")\n",
    "     .json(stations_output)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_result = timer_method(\"ingest_station_pyspark()\")\n",
    "pd_result = timer_method(\"ingest_station_pandas()\")\n",
    "print(f\"pandas: {pd_result} pyspark: {ps_result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
