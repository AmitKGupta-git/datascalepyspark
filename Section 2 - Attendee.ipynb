{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries within the notebook scope\n",
    "sc.install_pypi_package(\"boto3\")\n",
    "sc.install_pypi_package(\"pandas\")\n",
    "sc.install_pypi_package(\"requests\")\n",
    "sc.install_pypi_package(\"s3fs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql import functions as f, types as t\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "import s3fs\n",
    "import subprocess\n",
    "import timeit\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Removes truncation of columns, column values in Pandas\n",
    "# by default\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "# Monkey patching the DataFrame transform method for Spark 2.4\n",
    "# This is available by default in Spark 3.0\n",
    "def transform(self, f):\n",
    "    return f(self)\n",
    "DataFrame.transform = transform\n",
    "\n",
    "# Override the timeit template to return the command's\n",
    "# return value in addition to the time\n",
    "# Reference: https://stackoverflow.com/questions/24812253/how-can-i-capture-return-value-with-python-timeit-module\n",
    "timeit.template = \"\"\"\n",
    "def inner(_it, _timer{init}):\n",
    "    {setup}\n",
    "    _t0 = _timer()\n",
    "    for _i in _it:\n",
    "        retval = {stmt}\n",
    "    _t1 = _timer()\n",
    "    return _t1 - _t0, retval\n",
    "\"\"\"\n",
    "\n",
    "def shell_cmd(cmd):\n",
    "    for line in subprocess.check_output(cmd, shell=True).split(b'\\n'):\n",
    "        print(line)\n",
    "\n",
    "def timer_method(cmd):\n",
    "    # Setting globals = globals() enables the timeit function\n",
    "    # to return the value generated by cmd\n",
    "    return timeit.timeit(cmd, number=1, globals = globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting from an S3 bucket - NYC Taxi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "* Taxi data \n",
    "* Data dictionaries \n",
    "* Taxi zone lookup table\n",
    "\n",
    "Take a few minutes to look through the Data Dictionaries and Metadata and the Taxi Zone Maps and Lookup Tables. What are some things you notice about the data?\n",
    "\n",
    "Data ingestion has the ultimate goal of collecting, aggregating, and surfacing data for a specific purpose; an analysis, an API, a dashboard, etc. Think about how you might use the taxi data to answer the following questions:\n",
    "\n",
    "1. Which borough is the most popular pickup or drop off spot?\n",
    "1. Are green taxis more popular for trips within the same borough vs yellow taxis?\n",
    "1. Build a recommendation engine that predicts surge pricing for a given time of day based on historical data  \n",
    "\n",
    "With this in mind, lets work through bringing this data onto the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, if you copy the link from the taxi data website you will see:\n",
    "# https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-01.csv\n",
    "# Two things - first, the portion of the URL following \"aws.com\" is the \n",
    "# bucket name. Second, in \"trip+data\" the \"+\" is a space\n",
    "taxi_data_path = \"s3://nyc-tlc/trip data/yellow_tripdata_2020-01.csv\"\n",
    "\n",
    "# When working with big data it can be challenging to view the data. How would you \n",
    "# go about getting a sample of this data? (download it, use requests, pandas, etc)\n",
    "\n",
    "# Pandas uses s3fs to read_csv from s3:\n",
    "pd_run_time, pd_df_taxi = timer_method(\"pd.read_csv(taxi_data_path, keep_default_na=False)\")\n",
    "print(f\"runtime: {pd_run_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the data. Notice how pandas will try to assign types. Is this desirable?\n",
    "# Why or why not?\n",
    "# Since we have column names it also seems this data has a header\n",
    "pd_df_taxi.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reference, look at the Spark DataFrameReader, csv:\n",
    "# https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html\n",
    "ps_run_time, ps_df_taxi = timer_method(\"spark.read.option('header', True).csv(taxi_data_path)\")\n",
    "print(f\"runtime: {ps_run_time}\")\n",
    "\n",
    "# Talk through the spark UI here, partcularly note that the cell number will show up in the SparkUI\n",
    "# Job list next to the Job id. For example. this would be Job 2 (cell number)\n",
    "# Look at runtime and talk about lazy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_df_taxi.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see how the spark dataframe reader interpreted the data\n",
    "# Talk about nullable vs non nullable and maybe a small bit about data schemas\n",
    "ps_df_taxi.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Talk through ingest practices around retaining original data vs augmenting\n",
    "# For example, we may want to keep the data in its default format so we can\n",
    "# refer back to it if there are bugs in our data ingestion code\n",
    "ps_df_taxi.write.option(\"header\", True).csv(\"hdfs:///tmp/input/taxi_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discuss how spark writes files out\n",
    "shell_cmd(\"hdfs dfs -ls hdfs:///tmp/input/taxi_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the data, what do you notice?\n",
    "column_subset = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID', 'fare_amount', 'tip_amount']\n",
    "ps_df_taxi.select(*column_subset).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep in mind the datatype when considering these results\n",
    "ps_df_taxi.select(*column_subset).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casting columns to a type\n",
    "(ps_df_taxi.select(\"passenger_count\")\n",
    " .withColumn(\"passenger_count\", f.col(\"passenger_count\").cast(t.IntegerType()))\n",
    ").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casting pandas columns to a type - this will give an error on empty cells\n",
    "(pd_df_taxi[[*column_subset]]\n",
    "         .astype({'passenger_count': 'Int64'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert to Integer using pandas, we have to first deal with the null values\n",
    "# to_numeric with 'coerce' will fill invalid integer values with np.NaN\n",
    "# the Int64 type in later versions of pandas will convert np.NaN to a nullable\n",
    "# integer type: https://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html\n",
    "pd.to_numeric(pd_df_taxi.passenger_count, errors='coerce').astype('Int64').unique()\n",
    "\n",
    "# Some other type conversions you may find useful:\n",
    "# pd.to_datetime(pd_df_taxi.tpep_pickup_datetime)\n",
    "# pd.to_numeric produces float type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.1 - Transform the column_subset of taxi data to data types that accurately represent the data\n",
    "\n",
    "Result: a transformed_taxi dataframe with the column_subset columns cast to an appropriate type\n",
    "\n",
    "Available types are listed in the pyspark.sql.types module https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#module-pyspark.sql.types  \n",
    "\n",
    "This is imported as `t`, so to apply the IntegerType use `t.IntegerType()`\n",
    "\n",
    "For pandas, see the following resources on converting types\n",
    "https://stackoverflow.com/questions/15891038/change-column-type-in-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Talk about transforming\n",
    "# the data into a type schema to surface for analytical operations - i.e. pay the penalty of time\n",
    "# on the ingest to transform strings to timestamps so the analysis side can use datetime methods\n",
    "# without having to remember to cast\n",
    "\n",
    "\n",
    "def transform_taxi_ps(ps_df_taxi):\n",
    "    \"\"\"\n",
    "    ps_df_taxi: pyspark dataframe\n",
    "    returns: dataframe of column_subset with types applied to all fields\n",
    "    \"\"\"\n",
    "\n",
    "def transform_taxi_pd(pd_df_taxi):\n",
    "    \"\"\"\n",
    "    pd_df_taxi: pandas dataframe\n",
    "    returns: dataframe of column_subset with types applied to all fields\n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_run_time, ps_transform_taxi = timer_method(\"transform_taxi_ps(ps_df_taxi)\")\n",
    "pd_run_time, pd_transform_taxi = timer_method(\"transform_taxi_pd(pd_df_taxi)\")\n",
    "print(f\"pyspark runtime: {ps_run_time} pandas runtime {pd_run_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_transform_taxi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_transform_taxi.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this only shows results for numeric and string columns - now that the data has been cast to \n",
    "# types, we can explore it a bit more\n",
    "ps_transform_taxi.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the date ranges...\n",
    "ps_transform_taxi.select(\"tpep_pickup_datetime\").sort(f.asc(\"tpep_pickup_datetime\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out transformed data to EBS\n",
    "ps_transform_taxi.write.mode(\"append\").json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.2 - Write an ingestion function that does the following:\n",
    "Given a file path to a taxi data csv (i.e. s3://nyc-tlc/trip data/green_tripdata_2020-01.csv)\n",
    "1. Read the file into a Spark dataframe\n",
    "2. Transform the column_subset\n",
    "3. Write the data as json to hdfs in append mode to `hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json`\n",
    "\n",
    "Function signature:  \n",
    "`def ingest_taxi_data(file_name)`\n",
    "\n",
    "Inputs can be created from:  \n",
    "`taxi_data_prefix = \"s3://nyc-tlc/trip data/\"\n",
    "taxi_data_files = [\"yellow_tripdata_2019-01.csv\", \"yellow_tripdata_2018-01.csv\", \"yellow_tripdata_2017-01.csv\"]  `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_taxi_data(file_name):\n",
    "    \"\"\"\n",
    "    file_name: path to file, i.e. s3://bucket/file.csv\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the ingest for several files\n",
    "taxi_data_prefix = \"s3://nyc-tlc/trip data/\"\n",
    "taxi_data_files = [\"yellow_tripdata_2019-01.csv\", \"yellow_tripdata_2018-01.csv\", \"yellow_tripdata_2017-01.csv\"]\n",
    "for file_name in taxi_data_files: \n",
    "    taxi_data_path = f\"{taxi_data_prefix}{file_name}\"\n",
    "    ingest_taxi_data(taxi_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "df.printSchema()\n",
    "\n",
    "# Talk through how the read.json interpreted the Integers as Longs, set stage for using schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing ingestion code\n",
    "\n",
    "The `ingest_taxi_data` method is not well structured for testing:\n",
    "* Writes to the file system\n",
    "* Requires an input file to test\n",
    "* What other shortcomings?\n",
    "\n",
    "To make this code more testable, split out the transformation logic so it can be unit tested.  \n",
    "Definining a transformation function that takes a dataframe and returns a dataframe provides a better interface for unit testing, and a more extensible structure in case we need to add more dataframe functions before or after the transformation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_taxi_data(df):\n",
    "    return (df.withColumn(\"tpep_pickup_datetime\", f.col(\"tpep_pickup_datetime\").cast(t.TimestampType()))\n",
    "     .withColumn(\"tpep_dropoff_datetime\", f.col(\"tpep_dropoff_datetime\").cast(t.TimestampType()))\n",
    "     .withColumn(\"passenger_count\", f.col(\"passenger_count\").cast(t.IntegerType()))\n",
    "     .withColumn(\"trip_distance\", f.col(\"trip_distance\").cast(t.FloatType()))\n",
    "     .withColumn(\"PULocationID\", f.col(\"PULocationID\").cast(t.IntegerType()))\n",
    "     .withColumn(\"DOLocationID\", f.col(\"DOLocationID\").cast(t.IntegerType()))\n",
    "     .withColumn(\"fare_amount\", f.col(\"fare_amount\").cast(t.FloatType()))\n",
    "     .withColumn(\"tip_amount\", f.col(\"tip_amount\").cast(t.FloatType())))\n",
    "    \n",
    "def ingest_taxi_data_transform(file_name):\n",
    "    # Requires patching of Dataframe.transform method in Spark 2.4, but available natively\n",
    "    # in Spark 3.0 https://mungingdata.com/pyspark/chaining-dataframe-transformations/\n",
    "    df_input = (spark\n",
    "         .read\n",
    "         .option('header', True).csv(taxi_data_path)\n",
    "         .select(*column_subset)\n",
    "         .transform(transform_taxi_data)\n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )\n",
    "    \n",
    "def ingest_taxi_data_method(file_name):\n",
    "    # Equivalent code without using the monkey-patched transform method for DataFrame\n",
    "    df_input = (spark\n",
    "         .read\n",
    "         .option('header', True).csv(taxi_data_path)\n",
    "         .select(*column_subset))\n",
    "    \n",
    "    (transform_taxi_data(df_input)\n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    \"{'tpep_pickup_datetime': '2020-05-23 21:05:23', 'tpep_dropoff_datetime': '2020-05-23 08:05:23', 'passenger_count': 0, 'trip_distance': 10.5, 'PULocationID': 1, 'DOLocationID': 254, 'fare_amount': 0.05, 'tip_amount': 1.00}\",\n",
    "    \"{'tpep_pickup_datetime': '2020-10-01 01:05:23', 'tpep_dropoff_datetime': '2020-10-01 02:05:23', 'passenger_count': 1, 'trip_distance': 0.1, 'PULocationID': 45, 'DOLocationID': 3, 'fare_amount': 10.0, 'tip_amount': 5.00}\",\n",
    "    \"{'tpep_pickup_datetime': '2020-02-02 15:22:23', 'tpep_dropoff_datetime': '2020-02-03 15:44:23', 'passenger_count': 3, 'trip_distance': 3.25, 'PULocationID': 10, 'DOLocationID': 24, 'fare_amount': 5.05, 'tip_amount': 1.00}\"\n",
    "]\n",
    "expected_types = {'DOLocationID': 'int', 'PULocationID': 'int', 'fare_amount': 'float', 'passenger_count': 'int', 'tip_amount': 'float', 'tpep_dropoff_datetime': 'timestamp', 'tpep_pickup_datetime': 'timestamp', 'trip_distance': 'float'}\n",
    "\n",
    "test_df = spark.read.json(sc.parallelize(test_data))\n",
    "test = transform_taxi_data(test_df)\n",
    "test_types = {item[0]:item[1] for item in test.dtypes}\n",
    "\n",
    "assert expected_types == test_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_cmd(\"hdfs dfs -rm hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re run the ingestion using the functions with the transformation broken out\n",
    "taxi_data_prefix = \"s3://nyc-tlc/trip data/\"\n",
    "taxi_data_files = [\"yellow_tripdata_2019-01.csv\", \"yellow_tripdata_2018-01.csv\", \"yellow_tripdata_2017-01.csv\"]\n",
    "for file_name in taxi_data_files: \n",
    "    taxi_data_path = f\"{taxi_data_prefix}{file_name}\"\n",
    "    ingest_taxi_data_method(file_name)\n",
    "    #ingest_taxi_data_transform(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets try running the ingestion code on the other taxi data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try using the ingest code we created for yellow taxi for all the taxis\n",
    "# This will fail because the datetime fields have different names across different servcies\n",
    "\n",
    "taxi_data_prefix = \"s3://nyc-tlc/trip data/\"\n",
    "taxi_data_files = [\"green_tripdata_2020-01.csv\", \"fhv_tripdata_2020-01.csv\", \"fhvhv_tripdata_2020-01.csv\"]\n",
    "for file_name in taxi_data_files: \n",
    "    taxi_data_path = f\"{taxi_data_prefix}{file_name}\"\n",
    "    ingest_taxi_data_transform(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data modeling\n",
    "Create a model for all taxi data, given that there are differences across the services in the kind of data collected\n",
    "\n",
    "Commmon fields across all services:\n",
    "* PULocationID\n",
    "* DOLocationID\n",
    "\n",
    "Fields we want to normalize across all services - this data is in all services but is named differently\n",
    "* pickup datetime\n",
    "* drop off datetime\n",
    "\n",
    "Service specific fields. These are only in green or yellow data\n",
    "* Passenger_count\n",
    "* Trip_distance\n",
    "* Fare_amount\n",
    "* Tip_amount\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.3 - Using pyspark, write different transformation functions for each taxi service type to match the following signature and schema:\n",
    "Fields / Types:\n",
    "\n",
    "* pickup_datetime Timestamp\n",
    "* dropoff_datetime Timestamp\n",
    "* passenger_count Integer\n",
    "* fare_amount Float\n",
    "* tip_amount Float\n",
    "* PULocationID Integer\n",
    "* DOLocationID Integer\n",
    "\n",
    "`def transform_function(dataframe):  \n",
    "    return transformed_dataframe\n",
    "`\n",
    "\n",
    "Once the transformation functions are done, rewrite `ingest_taxi_data` to use these new functions depending on the file being processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_yellow_taxi(df):\n",
    "\n",
    "        \n",
    "def transform_green_taxi(df):\n",
    "\n",
    " \n",
    "def transform_fhv(df):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the ability to read all the taxi data into the same dataset, how will we be able to tell where the original data came from? The file name provides information including:\n",
    "* Service type (yellow, green, etc)\n",
    "* File date\n",
    "\n",
    "We want to augment the taxi data with this information so we can refer back to it in analysis.\n",
    "\n",
    "Is there other data we might want to augment the raw data with? Some things to consider:\n",
    "* Additional fields that could help with analysis\n",
    "* Metadata, like when the record was last updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using matched groups, we can extract information from the taxi file names\n",
    "TAXI_DATA_PATTERN = \"(?P<service>[a-zA-Z0-9]+)_tripdata_(?P<year>[0-9]{4})-(?P<month>[0-9]{2}).csv\"\n",
    "\n",
    "def extract_file_info(file_name):\n",
    "    m = re.match(TAXI_DATA_PATTERN, file_name)\n",
    "    if m is not None:\n",
    "        return (m.group(1), m.group(2), m.group(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_taxi_data_multi_service(file_name):\n",
    "    print(f\"Processing {file_name}\")\n",
    "    (service, year, month) = extract_file_info(Path(file_name).name)\n",
    "    input_df = spark.read.option('header', True).csv(file_name)\n",
    "    \n",
    "    if service == 'yellow':\n",
    "        df_transform = transform_yellow_taxi(input_df)\n",
    "    elif service == 'green':\n",
    "        df_transform = transform_green_taxi(input_df)\n",
    "    else:\n",
    "        # FHV. What happens if there are more taxi services added?\n",
    "        df_transform = transform_fhv(input_df)\n",
    "\n",
    "    (df_transform\n",
    "         .withColumn(\"service\", f.lit(service))\n",
    "         .withColumn(\"year\", f.lit(year))\n",
    "         .withColumn(\"month\", f.lit(month))\n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_cmd(\"hdfs dfs -rm -r hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data_prefix = \"s3://nyc-tlc/trip data/\"\n",
    "taxi_data_files = [\"yellow_tripdata_2020-01.csv\", \"green_tripdata_2020-01.csv\", \"fhv_tripdata_2020-01.csv\", \"fhvhv_tripdata_2020-01.csv\"]\n",
    "for file_name in taxi_data_files: \n",
    "    taxi_data_path = f\"{taxi_data_prefix}{file_name}\"\n",
    "    ingest_taxi_data_multi_service(taxi_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi_output = spark.read.json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi_output.groupby(\"service\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling bad data\n",
    "How to design for the inevitability of bad data  \n",
    "Reference: https://blog.knoldus.com/apache-spark-handle-corrupt-bad-records/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_data = [\n",
    "    \"{'pickup_datetime': '2020-05-23 21:05:23', 'fare_amount': '0.05'}\",\n",
    "    \"{'pickup_datetime': '2020-05-23 08:05:23', 'fare_amount': '10.05'}\",\n",
    "    \"{'pickup_datetime': '2020-05-23 21:05:23', 'fare_amount}\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_df = spark.read.json(sc.parallelize(bad_data), mode=\"PERMISSIVE\", columnNameOfCorruptRecord=\"_corrupt_record\")\n",
    "corrupt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_df = spark.read.json(sc.parallelize(bad_data), mode=\"DROPMALFORMED\", columnNameOfCorruptRecord=\"_corrupt_record\")\n",
    "corrupt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_df = spark.read.json(sc.parallelize(bad_data), mode=\"FAILFAST\", columnNameOfCorruptRecord=\"_corrupt_record\")\n",
    "corrupt_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.4 - Write an ingestion for the taxi zone lookup\n",
    "File location - Yes, there is a space between taxi and the '_'  \n",
    "\n",
    "s3://nyc-tlc/misc/taxi _zone_lookup.csv\n",
    "\n",
    "`def ingest_taxi_lookup():`\n",
    "1. Read taxi data\n",
    "1. Cast to correct data types\n",
    "1. Save to hdfs:///tmp/data/nyc-taxi/zone-lookup/output/section2/json\n",
    "\n",
    "Refer back to Taxi Data page for more info: https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page  \n",
    "Remember to structure your code for testability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_taxi_lookup():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timer_method(\"ingest_taxi_lookup()\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather data ingestion\n",
    "\n",
    "NOAA GHCND dataset  \n",
    "https://docs.opendata.aws/noaa-ghcn-pds/readme.html  \n",
    "\n",
    "Scroll down to 'FORMAT OF “ghcnd-stations.txt” file' for the schema of the fixed-width stations data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghcnd_stations_path = \"https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt\"\n",
    "stations_s3 = \"s3://data-scale-oreilly/data/ghcnd/stations/input/ghcnd_stations.txt\"\n",
    "stations_local = \"hdfs:///tmp/data/ghcnd/stations/input/ghcnd-stations.txt\"\n",
    "stations_output = \"hdfs:///tmp/data/ghcnd/stations/output/ghcnd-stations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround reading HTTPS -> HDFS, HTTPS -> S3 -> HDFS\n",
    "# Spark cant read data directly from HTTP, so copy the file to S3 and read into a dataframe from there\n",
    "# Then save the file to HDFS for further processing\n",
    "ingest_timestamp = datetime.strftime(datetime.now(), \"%Y-%m-%d %H:%M:%S%z\")\n",
    "resp = requests.get(ghcnd_stations_path)\n",
    "if resp.status_code != 200:\n",
    "    print(\"Couldn't get station data\")\n",
    "else:\n",
    "    s3 = boto3.client('s3')\n",
    "    res = s3.put_object(Body=resp.content, Bucket=\"data-scale-oreilly\", Key=f\"data/ghcnd/stations/input/ghcnd_stations.txt\")\n",
    "    if res['ResponseMetadata']['HTTPStatusCode'] != 200:\n",
    "        print(f\"Unable to create ghcnd_stations.txt in s3, response {res['ResponseMetadata']['HTTPStatusCode']}\")\n",
    "    else:\n",
    "        (spark\n",
    "         .read\n",
    "         .text(stations_s3)\n",
    "         .write\n",
    "         .format(\"text\")\n",
    "         .mode(\"overwrite\")\n",
    "         .save(stations_local))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the stations file we just saved to HDFS\n",
    "stations = spark.read.text(stations_local)\n",
    "stations.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of doing a substring transformation\n",
    "(stations\n",
    "    .withColumn(\"id\", f.col(\"value\").substr(0, 11))\n",
    "    .drop(\"value\")\n",
    ").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does this work using pandas?\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_fwf.html\n",
    "pd.read_fwf(stations_s3, [(0,10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.5 - Build the ingestion for the weather stations data\n",
    "\n",
    "Reference the fixed width schema provided under **FORMAT OF “ghcnd-stations.txt” file**   \n",
    "https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt  \n",
    "#### Important note - FORMAT OF is incorrect for: elevation - should be 7 char not 6\n",
    "\n",
    "Create transformation code to convert the value column into the following schema:\n",
    "* id String\n",
    "* lat Float\n",
    "* long Float\n",
    "* elevation Float\n",
    "* state String\n",
    "* name String\n",
    "\n",
    "Drop the value column, save the data in JSON format to s3://data-scale-oreilly/data/ghcnd/stations/output/section2_json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_stations_ps(stations_local):\n",
    "    \"\"\"\n",
    "    stations_local: Path to fixed width data\n",
    "    returns: pyspark dataframe\n",
    "    \"\"\"\n",
    "\n",
    "def transform_stations_pd(stations_s3):\n",
    "    \"\"\"\n",
    "    stations_s3: Path to fixed width data\n",
    "    returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_run_time, stations_df = timer_method(\"transform_stations_ps(stations_local)\")\n",
    "pd_run_time, pd_stations_df = timer_method(\"transform_stations_pd(stations_s3)\")\n",
    "print(f\"pyspark runtime: {ps_run_time} pandas runtime {pd_run_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A key aspect of designing scalable systems is to be judicious about the data being stored and processed. \n",
    "#### The GHCND stations file contains data on stations across the US, but we are only interested in data near NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at some performance tradeoffs between pyspark and pandas\n",
    "# The .toPandas() function in pyspark can be convenient if you are familiar with pandas manipulations\n",
    "# but this can quickly become very expensive as it collects all data on the driver to do the conversion.\n",
    "\n",
    "def filter_ny_stations_pandas(stations_df):\n",
    "    ny_stations = stations_df.filter(\"state == 'NY'\")\n",
    "\n",
    "    # filter down to just stations in NY in NYC. Lat of south Yonkers ~40.9124\n",
    "    ny_pandas = ny_stations.toPandas()\n",
    "    ny_pandas[ny_pandas.columns] = ny_pandas.apply(lambda x: x.str.strip())\n",
    "    nyc_stations = ny_pandas[ny_pandas['lat'].apply(lambda x: float(x)) < 40.9124]\n",
    "    return spark.createDataFrame(nyc_stations)\n",
    "\n",
    "def filter_ny_stations_pyspark(stations_df):\n",
    "    print(\"Filtering stations to NY only\")\n",
    "    ny_stations = stations_df.filter(\"state == 'NY'\")\n",
    "    return (ny_stations\n",
    "            .withColumn(\"lat\", f.col(\"lat\").cast(t.FloatType()))\n",
    "            .filter(\"lat < 40.9124\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_run_time, pd_ny_stations = timer_method(\"filter_ny_stations_pandas(stations_df)\")\n",
    "print(f\"runtime {pd_run_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are these runtimes really comprable with lazy eval? I think we need to do some combined actions, \n",
    "# like filter and aggregate, to look at performance tradeoffs\n",
    "ps_run_time, ps_ny_stations = timer_method(\"filter_ny_stations_pyspark(stations_df)\")\n",
    "print(f\"runtime {ps_run_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Lab 2.6 - Write two ingest functions for the stations data, one using filter_ny_stations_pandas and the other using filter_ny_stations_pyspark. What do you notice about the differences?\n",
    "\n",
    "The functions should:\n",
    "* Read the station data from local or s3\n",
    "* Transform the station data into columns from the fixed width format\n",
    "* Use the above filter functions\n",
    "* Write the output to stations_output as json in overwrite mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_station_pandas():\n",
    "    \n",
    "    \n",
    "def ingest_station_pyspark():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shell_cmd(\"hdfs dfs -rm -r hdfs:///tmp/data/test_pyspark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_result = timer_method(\"ingest_station_pyspark()\")\n",
    "pd_result = timer_method(\"ingest_station_pandas()\")\n",
    "print(f\"pandas: {pd_result} pyspark: {ps_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.7 - Case Study 1: Month over month, get the total count of of raxi rides per borough\n",
    "\n",
    "These functions should:\n",
    "* Read the taxi and taxi lookup data from local\n",
    "* Join the taxi lookup table to the taxi data to get the name of the relevant boroughs for both \n",
    "* Get the count of taxi rides per month using both pandas and pyspark\n",
    "* Display and return the dataframe containing the month in the format YYYYMM, the borough, and the taxi counts for that month and borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These should point to the paths where the data from the taxi and taxi-lookup ingests were written\n",
    "taxiPath = \"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/\"\n",
    "taxiLookupPath = \"hdfs:///tmp/data/nyc-taxi/zone-lookup/output/section2/json/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A note about joins\n",
    "# Try running this cell to join the boroughs for the pickup and  dropoff locations \n",
    "df_taxi = spark.read.json(taxiPath)\n",
    "df_taxi_lookup = spark.read.json(taxiLookupPath)\n",
    "\n",
    "taxi_filtered = (df_taxi\n",
    " .filter(df_taxi.pickup_datetime.isNotNull())\n",
    " .filter(df_taxi.dropoff_datetime.isNotNull()))\n",
    "taxi_pu = (taxi_filtered\n",
    ".join(df_taxi_lookup\n",
    "       .select(\"LocationID\", \"Borough\")\n",
    "       .withColumnRenamed(\"Borough\", \"PUBorough\"), \n",
    "       df_taxi_lookup.LocationID == df_taxi.PULocationID))\n",
    "taxi = (taxi_pu.join(df_taxi_lookup\n",
    "       .select(\"LocationID\", \"Borough\")\n",
    "       .withColumnRenamed(\"Borough\", \"DOBorough\"), \n",
    "       df_taxi_lookup.LocationID == taxi_pu.DOLocationID))\n",
    "taxi_pu.show()\n",
    "taxi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monthly_totals_pyspark(taxiPath, taxiLookupPath):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timer_method(\"get_monthly_totals_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monthly_totals_pandas(taxiPath, taxiLookupPath):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timer_method(\"get_monthly_totals_pandas(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.8 - Case Study 2: Month over month, get the borough with the most amount of pickups per month\n",
    "\n",
    "These functions should:\n",
    "* Utilize the functions from the previous lab to get the monthly taxi ride counts per borough\n",
    "* Utilizing both pandas and pyspark: display and return a dataframe containing the month, the borough with the most taxi rides for that month, and the count of taxi rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_pickups_per_month_pandas(taxiPath, taxiLookupPath):\n",
    "\n",
    "def get_most_pickups_per_month_pyspark(taxiPath, taxiLookupPath):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timer_method(\"get_most_pickups_per_month_pandas(taxiPath, taxiLookupPath)\"))\n",
    "print(timer_method(\"get_most_pickups_per_month_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab - 2.9 Run and time the overall pipeline\n",
    "\n",
    "This function should:\n",
    "* Ingest the taxi and taxi-lookup data from their source\n",
    "* Perform the aggregations from the previous labs and write the data to local ebs storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset notebook kernel\n",
    "def ingest_main():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timer_method(\"ingest_main\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
